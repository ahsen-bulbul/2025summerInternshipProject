{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72642fc7-59a4-46bf-bcb9-24e87298365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ CSV dosyasƒ± okunuyor: /home/yapayzeka/ahsen_bulbul/data/10data.csv\n",
      "‚úÖ 20 belge bulundu\n",
      "üîÑ ƒ∞≈ülenen: 0/20 (%0.0)\n",
      "‚úÖ Toplam 132 chunk olu≈üturuldu\n",
      "\n",
      "üìä ƒ∞statistikler:\n",
      "Ortalama chunk boyutu: 368.5 token\n",
      "Medyan chunk boyutu: 252.5 token\n",
      "Min chunk boyutu: 101 token\n",
      "Max chunk boyutu: 1224 token\n",
      "Tarih i√ßeren chunk'lar: 132\n",
      "Hukuki referans i√ßeren chunk'lar: 129\n",
      "\n",
      "üè∑Ô∏è B√∂l√ºm t√ºrleri:\n",
      "section_type\n",
      "DAVA             57\n",
      "OTHER            38\n",
      "TEMYIZ           10\n",
      "KARAR             8\n",
      "GEREK√áE           7\n",
      "MAHKEME           7\n",
      "BOZMA             4\n",
      "DEƒûERLENDIRME     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üíæ Sonu√ßlar kaydedildi: 2legal_chunks.csv\n",
      "üéâ ƒ∞≈ülem tamamlandƒ±!\n",
      "\n",
      "üìã √ñrnek chunk'lar:\n",
      "                     chunk_id section_type  tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text\n",
      "0  6750524485b6640290c37b06_0         DAVA     389                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          13. Hukuk Dairesi Taraflar arasƒ±ndaki r√ºcuen tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda, ƒ∞lk Derece Mahkemesince davanƒ±n kabul√ºne karar verilmi≈ütir. Kararƒ±n davacƒ± vekili tarafƒ±ndan istinaf edilmesi √ºzerine, B√∂lge Adliye Mahkemesince ba≈üvurunun esastan reddine karar verilmi≈ütir. B√∂lge Adliye Mahkemesi kararƒ± davalƒ± ... Grup Sosyal Hizmetleri ƒ∞n≈ü. Tem. Taah. Tic. A.≈û. vekili tarafƒ±ndan temyiz edilmekle; yapƒ±lan √∂n inceleme sonucunda gereƒüi d√º≈ü√ºn√ºld√º: ƒ∞lk Derece Mahkemesi kararƒ±nƒ± istinaf etmeyen tarafƒ±n, B√∂lge Adliye Mahkemesinin istinaf ba≈üvurusunun esastan reddine ili≈ükin kararƒ±nƒ± temyiz etme hakkƒ± bulunmadƒ±ƒüƒ±ndan adƒ± ge√ßen davalƒ± vekilinin temyiz dilek√ßesinin reddine karar verilmi≈ütir. KARAR A√ßƒ±klanan sebeple; Davalƒ± ... Grup Sosyal Hizmetleri ƒ∞n≈ü. Tem. Taah. Tic. A.≈û. vekilinin temyiz dilek√ßesinin REDDƒ∞NE, Dosyanƒ±n ƒ∞lk Derece Mahkemesine, kararƒ±n bir √∂rneƒüinin kararƒ± veren B√∂lge Adliye Mahkemesine g√∂nderilmesine 18.01.2024 tarihinde oy birliƒüiyle karar verildi.\n",
      "1  6750524485b6640290c37b07_0        OTHER     216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±ndaki tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda ƒ∞lk Derece Mahkemesince davanƒ±n reddine karar verilmi≈ütir. ƒ∞lk Derece Mahkemesi kararƒ± davacƒ±lar vekilince temyiz edilmekle; kesinlik, s√ºre, temyiz ≈üartƒ± ve diƒüer usul eksiklikleri y√∂n√ºnden yapƒ±lan √∂n inceleme sonucunda, temyiz dilek√ßesinin kabul√ºne karar verildikten ve Tetkik H√¢kimi tarafƒ±ndan hazƒ±rlanan rapor dinlendikten sonra dosyadaki belgeler incelenip gereƒüi d√º≈ü√ºn√ºld√º:\n",
      "2  6750524485b6640290c37b07_0      MAHKEME     325                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1. Asliye Hukuk Mahkemesinin 2009/139 D. ... dosyasƒ± √ºzerinden ihtiyati tedbir konularak in≈üaatƒ±n durdurulduƒüunu, HUMK'un 109. maddesi gereƒüince ihtiyati tedbir kararƒ±nƒ±n verildiƒüi tarihten itibaren 10 g√ºn i√ßerisinde esas hakkƒ±nda davanƒ±n a√ßƒ±lmasƒ± gerektiƒüini ve bu durumun dosyaya ibrazƒ± gerekirken davanƒ±n a√ßƒ±lmadƒ±ƒüƒ±nƒ±, Hukuk Usul√º Muhakemeleri Kanununun 110. maddesi, ihtiyati tedbir kararƒ±nƒ±n haksƒ±z olduƒüunun belirlenmesi halinde tedbir kararƒ± y√ºz√ºnden uƒüranƒ±lan zararƒ±n tazminini d√ºzenlediƒüini, ihtiyati tedbir kararƒ±nƒ± icra ettiren tarafƒ±n yasal s√ºrede dava a√ßmamasƒ± halinde ihtiyati tedbirin haksƒ±z konulduƒüunun kabul√º gerektiƒüi, kaldƒ± ki s√ºresinde dava a√ßsa da durumun deƒüi≈ümeyeceƒüini belirterek m√ºvekkillerinin in≈üaatƒ±nƒ±n ge√ß bitirilmesinden kaynaklƒ± 10.000,00 TL maddi tazminatƒ±n tahsiline karar verilmesini talep etmi≈ütir.\n",
      "3  6750524485b6640290c37b07_0      MAHKEME     760  1.Asliye Hukuk Mahkemesinin 2009/139 D.... sayƒ±lƒ± dosyasƒ± √ºzerinden 26.06.2009 tarihinde tedbir talep edildiƒüi, yapƒ±lan ke≈üif sonucu bilirki≈üilerin raporlarƒ±nƒ± ibraz ettikleri, 25.12.2009 tarihinde 2776 Parsel √ºzerinde yapƒ±mƒ±na devam edilen in≈üaatƒ±n tedbiren durdurulmasƒ±na karar verildiƒüi, kararƒ±n infazƒ± i√ßin 04.01.2010 tarihinde ... Belediye Ba≈ükanlƒ±ƒüƒ±'na ve N√∂bet√ßi ƒ∞cra M√ºd√ºrl√ºƒü√ºne ayrƒ± ayrƒ± m√ºzekkere yazƒ±ldƒ±ƒüƒ±, 05.01.2010 tarih saat 08:50 itibari ile in≈üaatƒ±n m√ºh√ºrlenerek tedbirin infaz edildiƒüi, dolayƒ±sƒ± ile tedbir kararƒ±n verildiƒüi tarihten 11 g√ºn ge√ßtikten sonra kararƒ±n infaz edildiƒüi, Mahkemenin infaz i√ßin yazdƒ±ƒüƒ± yazƒ±nƒ±n 10 g√ºnl√ºk s√ºre i√ßerisinde olduƒüu ancak infazƒ±n 11. g√ºn ger√ßekle≈ütiƒüi, 10 g√ºnl√ºk s√ºre i√ßerisinde herhangi bir dava a√ßƒ±lmadƒ±ƒüƒ±, dolayƒ±sƒ± ile HUMK'un 109. maddesi gereƒüince ihtiyati tedbirin infaz tarihi itibari ile kendiliƒüinden kalkmƒ±≈ü olduƒüu, buna raƒümen tedbirin infaz edildiƒüi, HUMK'un 109. maddesi \"Aksi takdirde ihtiyati tedbir bir g√ªna merasime hacet kalmaksƒ±zƒ±n kendiliƒüinden kalkar ve iktizasƒ±na g√∂re vazolunan tedbirin fiilen kaldƒ±rƒ±lmasƒ± ihtiyati tedbiri tatbik eden daire veya memurdan talep olunabilir\" h√ºkm√ºn√º d√ºzenlediƒüi, s√ºresinden bir g√ºn sonra tedbirin infazƒ± sƒ±rasƒ±nda davacƒ±larƒ±n yahut m√ºteahhidin infaz memurlarƒ±ndan tedbir kararƒ±nƒ± isteyip s√ºresinde dava a√ßƒ±lƒ±p a√ßƒ±lmadƒ±ƒüƒ±na dair de belgeyi istemeleri gerekirken bunu yapmadƒ±klarƒ±, kaldƒ± ki 05/01/2010 tarihinden hemen sonra aynƒ± g√ºn tedbirin verildiƒüi dosyanƒ±n incelenerek dava a√ßƒ±lƒ±p a√ßƒ±lmadƒ±ƒüƒ±nƒ±n tespit edilerek tedbirin kaldƒ±rƒ±lmasƒ±nƒ± talep etmeleri gerekirken infazdan 7 g√ºn sonra 12.01.2010 tarihinde tedbirin kaldƒ±rƒ±lmasƒ±nƒ± talep ettikleri, b√∂ylece bir zarar varsa zararƒ±n ortaya √ßƒ±kmasƒ±na ve artmasƒ±na bizzat m√ºlk sahiplerinin ve m√ºteahhidin sebep olduƒüu, zira in≈üaat durdurma zabtƒ±nƒ±n altƒ±nda... isimli ≈üahsƒ±n imzasƒ±nƒ±n olduƒüu, bu ≈üahsƒ±n da in≈üaatƒ± yapan m√ºteahhit...'ƒ±n akrabasƒ± olduƒüu g√∂r√ºlmekle a√ßƒ±lan davanƒ±n reddine karar verilmi≈ütir.\n",
      "4  6750524485b6640290c37b07_0        OTHER     304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              23. Hukuk Dairesinin 04.06.2013 tarihli ve 2013/2984 Esas, 2013/3773 Karar sayƒ±lƒ± kararƒ±yla dava tarihinde y√ºr√ºrl√ºkte olan 1086 sayƒ±lƒ± HUMK'un 109. maddesinin ''ƒ∞htiyati tedbir kararƒ± dava ikamesinden evvel verilmi≈ü ise tatbik edilmi≈ü olsun olmasƒ±n kararƒ±n verildiƒüi tarihten itibaren on g√ºn zarfƒ±nda esas hakkƒ±nda dava ikamesi lazƒ±mdƒ±r. Bu m√ºddette m√ºddei, davasƒ±nƒ± ikame eylediƒüini m√ºsbit evrakƒ±, kararƒ± tatbik eden memura ibrazla dosyaya vaz ve kaydettirerek mukabilinde ilmuhaber almaya mecburdur. Aksi takdirde ihtiyati tedbir bir g√ªna merasime hacet kalmaksƒ±zƒ±n kendiliƒüinden kalkar ve iktizasƒ±na g√∂re vazolunan tedbirin fiilen kaldƒ±rƒ±lmasƒ±, ihtiyati tedbiri tatbik eden daire veya memurdan talep olunabilir'' h√ºkm√ºn√º i√ßerdiƒüi, somut olayda, davalƒ±nƒ±n talebi √ºzerine ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Chunk metadata sƒ±nƒ±fƒ±\"\"\"\n",
    "    document_id: str\n",
    "    location: str\n",
    "    section_type: str\n",
    "    section_title: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    tokens: int\n",
    "    characters: int\n",
    "    has_dates: bool\n",
    "    has_legal_refs: bool\n",
    "    case_numbers: List[str]\n",
    "    dates: List[str]\n",
    "\n",
    "class TurkishLegalChunker:\n",
    "    \"\"\"T√ºrk√ße hukuki metinler i√ßin √∂zelle≈ütirilmi≈ü chunker\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 target_tokens: int = 500, \n",
    "                 max_tokens: int = 800, \n",
    "                 min_tokens: int = 100,\n",
    "                 overlap_ratio: float = 0.1):\n",
    "        \n",
    "        self.target_tokens = target_tokens\n",
    "        self.max_tokens = max_tokens\n",
    "        self.min_tokens = min_tokens\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Regex patterns\n",
    "        self.section_patterns = [\n",
    "            r'([IVX]+\\.\\s+[A-Z√úƒû≈û√á√ñI√ú][A-Z√úƒû≈û√á√ñI√úa-z√ºƒü≈ü√ß√∂ƒ±i\\s]+)',  # Roma rakamlarƒ±\n",
    "            r'([A-Z]\\.\\s+[A-Z√úƒû≈û√á√ñI√ú][A-Z√úƒû≈û√á√ñI√úa-z√ºƒü≈ü√ß√∂ƒ±i\\s]*)',   # A. B. C. b√∂l√ºmler\n",
    "            r'(\\d+\\.\\s*[A-Z√úƒû≈û√á√ñI√ú][A-Z√úƒû≈û√á√ñI√úa-z√ºƒü≈ü√ß√∂ƒ±i\\s]*)',    # 1. 2. 3. b√∂l√ºmler\n",
    "        ]\n",
    "        \n",
    "        self.date_pattern = r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}'\n",
    "        self.case_number_pattern = r'\\d{4}/\\d+\\s+[EK]\\.|[EK]\\.\\s*,\\s*\\d{4}/\\d+\\s+[EK]\\.'\n",
    "        \n",
    "        # T√ºrk√ße karakter d√ºzeltme haritasƒ±\n",
    "        self.char_fixes = {\n",
    "            '√Ñ¬∞': 'ƒ∞', '√Ö≈æ': '≈û', '√Ñ≈∏': 'ƒü', '√É¬º': '√º', '√É¬ß': '√ß', \n",
    "            '√Ñ¬±': 'ƒ±', '√É¬∂': '√∂', '√É': 'ƒ∞', '√É‚Ä°': '√á', '√É≈ì√É‚Ä°': '√ú√á',\n",
    "            '√Ç': '', '√¢‚Ç¨': '', '√¢‚Ç¨≈ì': '\"', '√¢‚Ç¨': '\"'\n",
    "        }\n",
    "        \n",
    "        # Hukuki terimler (b√∂l√ºm tespiti i√ßin)\n",
    "        self.legal_sections = {\n",
    "            'DAVA', 'CEVAP', 'MAHKEME', 'KARAR', 'TEMYIZ', 'ISTINAF', \n",
    "            'BOZMA', 'GEREK√áE', 'DEƒûERLENDIRME', 'SONU√á'\n",
    "        }\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Token sayƒ±sƒ±nƒ± hesapla\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def fix_encoding(self, text: str) -> str:\n",
    "        \"\"\"T√ºrk√ße karakter encoding sorunlarƒ±nƒ± d√ºzelt\"\"\"\n",
    "        for wrong, correct in self.char_fixes.items():\n",
    "            text = text.replace(wrong, correct)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Metni √∂n i≈ülemden ge√ßir\"\"\"\n",
    "        # Encoding d√ºzelt\n",
    "        text = self.fix_encoding(text)\n",
    "        \n",
    "        # Gereksiz bo≈üluklarƒ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Satƒ±r ba≈ülarƒ±ndaki bo≈üluklarƒ± temizle\n",
    "        text = re.sub(r'^\\s+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_metadata_info(self, text: str) -> Dict:\n",
    "        \"\"\"Metinden metadata bilgilerini √ßƒ±kar\"\"\"\n",
    "        # Tarihler\n",
    "        dates = re.findall(self.date_pattern, text)\n",
    "        \n",
    "        # Dava numaralarƒ±\n",
    "        case_numbers = re.findall(self.case_number_pattern, text)\n",
    "        \n",
    "        # Hukuki referanslar\n",
    "        legal_refs = bool(re.search(r'\\d+\\s+sayƒ±lƒ±|HMK|HUMK|ƒ∞ƒ∞K|TCK', text))\n",
    "        \n",
    "        return {\n",
    "            'dates': list(set(dates)),\n",
    "            'case_numbers': list(set(case_numbers)),\n",
    "            'has_legal_refs': legal_refs,\n",
    "            'has_dates': len(dates) > 0\n",
    "        }\n",
    "    \n",
    "    def detect_sections(self, text: str) -> List[Tuple[str, int, int]]:\n",
    "        \"\"\"Metindeki b√∂l√ºmleri tespit et\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        for pattern in self.section_patterns:\n",
    "            matches = list(re.finditer(pattern, text))\n",
    "            for match in matches:\n",
    "                section_title = match.group(1).strip()\n",
    "                start = match.start()\n",
    "                # Bir sonraki b√∂l√ºm√º bul\n",
    "                end = len(text)\n",
    "                \n",
    "                # Sonraki e≈üle≈ümeyi bul\n",
    "                next_match = None\n",
    "                for next_pattern in self.section_patterns:\n",
    "                    next_matches = list(re.finditer(next_pattern, text[match.end():]))\n",
    "                    if next_matches:\n",
    "                        if next_match is None or next_matches[0].start() < next_match:\n",
    "                            next_match = next_matches[0].start() + match.end()\n",
    "                \n",
    "                if next_match:\n",
    "                    end = next_match\n",
    "                \n",
    "                sections.append((section_title, start, end))\n",
    "        \n",
    "        # Sƒ±rala ve √ßakƒ±≈ümalarƒ± temizle\n",
    "        sections = sorted(sections, key=lambda x: x[1])\n",
    "        cleaned_sections = []\n",
    "        \n",
    "        for i, (title, start, end) in enumerate(sections):\n",
    "            if i == 0:\n",
    "                cleaned_sections.append((title, start, end))\n",
    "            else:\n",
    "                prev_end = cleaned_sections[-1][2]\n",
    "                if start >= prev_end:\n",
    "                    cleaned_sections.append((title, start, end))\n",
    "                else:\n",
    "                    # √áakƒ±≈üma varsa √∂nceki b√∂l√ºm√ºn sonunu g√ºncelle\n",
    "                    cleaned_sections[-1] = (cleaned_sections[-1][0], cleaned_sections[-1][1], start)\n",
    "                    cleaned_sections.append((title, start, end))\n",
    "        \n",
    "        return cleaned_sections\n",
    "    \n",
    "    def split_by_sentences(self, text: str, max_tokens: int) -> List[str]:\n",
    "        \"\"\"C√ºmleler bazƒ±nda metni b√∂l\"\"\"\n",
    "        # T√ºrk√ße i√ßin c√ºmle sonu i≈üaretleri\n",
    "        sentence_endings = r'[.!?]+(?=\\s+[A-Z√úƒû≈û√á√ñI√ú]|\\s*$)'\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = f\"{current_chunk} {sentence}\".strip()\n",
    "            \n",
    "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def create_overlapping_chunks(self, chunks: List[str]) -> List[str]:\n",
    "        \"\"\"Chunk'lar arasƒ± √∂rt√º≈üme olu≈ütur\"\"\"\n",
    "        if len(chunks) <= 1:\n",
    "            return chunks\n",
    "        \n",
    "        overlapped_chunks = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                # ƒ∞lk chunk\n",
    "                overlapped_chunks.append(chunk)\n",
    "            else:\n",
    "                # √ñnceki chunk'ƒ±n sonundan bir kƒ±smƒ±nƒ± al\n",
    "                prev_chunk = chunks[i-1]\n",
    "                overlap_tokens = int(self.count_tokens(prev_chunk) * self.overlap_ratio)\n",
    "                \n",
    "                # Son c√ºmleleri al (yakla≈üƒ±k)\n",
    "                prev_words = prev_chunk.split()\n",
    "                overlap_words = prev_words[-overlap_tokens*2:] if len(prev_words) > overlap_tokens*2 else prev_words\n",
    "                overlap_text = \" \".join(overlap_words)\n",
    "                \n",
    "                overlapped_chunk = f\"{overlap_text} {chunk}\"\n",
    "                overlapped_chunks.append(overlapped_chunk)\n",
    "        \n",
    "        return overlapped_chunks\n",
    "    \n",
    "    def chunk_document(self, document_id: str, location: str, raw_text: str) -> List[Dict]:\n",
    "        \"\"\"Ana chunking fonksiyonu\"\"\"\n",
    "        # √ñn i≈ülem\n",
    "        text = self.preprocess_text(raw_text)\n",
    "        \n",
    "        # Metadata bilgilerini √ßƒ±kar\n",
    "        metadata_info = self.extract_metadata_info(text)\n",
    "        \n",
    "        # B√∂l√ºmleri tespit et\n",
    "        sections = self.detect_sections(text)\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        if not sections:\n",
    "            # B√∂l√ºm bulunamazsa t√ºm metni chunk'la\n",
    "            chunks = self.split_by_sentences(text, self.max_tokens)\n",
    "            chunks = self.create_overlapping_chunks(chunks)\n",
    "            \n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if self.count_tokens(chunk_text) >= self.min_tokens:\n",
    "                    chunk_data = {\n",
    "                        'text': chunk_text,\n",
    "                        'metadata': ChunkMetadata(\n",
    "                            document_id=document_id,\n",
    "                            location=location,\n",
    "                            section_type=\"FULL_DOCUMENT\",\n",
    "                            section_title=\"Tam Metin\",\n",
    "                            chunk_index=i,\n",
    "                            total_chunks=len(chunks),\n",
    "                            tokens=self.count_tokens(chunk_text),\n",
    "                            characters=len(chunk_text),\n",
    "                            **metadata_info\n",
    "                        )\n",
    "                    }\n",
    "                    all_chunks.append(chunk_data)\n",
    "        \n",
    "        else:\n",
    "            # B√∂l√ºm bazƒ±nda chunk'la\n",
    "            for section_title, start, end in sections:\n",
    "                section_text = text[start:end].strip()\n",
    "                \n",
    "                if not section_text:\n",
    "                    continue\n",
    "                \n",
    "                # B√∂l√ºm tipini belirle\n",
    "                section_type = \"OTHER\"\n",
    "                for legal_term in self.legal_sections:\n",
    "                    if legal_term in section_title.upper():\n",
    "                        section_type = legal_term\n",
    "                        break\n",
    "                \n",
    "                if self.count_tokens(section_text) <= self.max_tokens:\n",
    "                    # K√º√ß√ºk b√∂l√ºm, aynen kullan\n",
    "                    if self.count_tokens(section_text) >= self.min_tokens:\n",
    "                        chunk_data = {\n",
    "                            'text': section_text,\n",
    "                            'metadata': ChunkMetadata(\n",
    "                                document_id=document_id,\n",
    "                                location=location,\n",
    "                                section_type=section_type,\n",
    "                                section_title=section_title,\n",
    "                                chunk_index=0,\n",
    "                                total_chunks=1,\n",
    "                                tokens=self.count_tokens(section_text),\n",
    "                                characters=len(section_text),\n",
    "                                **metadata_info\n",
    "                            )\n",
    "                        }\n",
    "                        all_chunks.append(chunk_data)\n",
    "                else:\n",
    "                    # B√ºy√ºk b√∂l√ºm, alt chunk'lara b√∂l\n",
    "                    section_chunks = self.split_by_sentences(section_text, self.max_tokens)\n",
    "                    section_chunks = self.create_overlapping_chunks(section_chunks)\n",
    "                    \n",
    "                    for i, chunk_text in enumerate(section_chunks):\n",
    "                        if self.count_tokens(chunk_text) >= self.min_tokens:\n",
    "                            chunk_data = {\n",
    "                                'text': chunk_text,\n",
    "                                'metadata': ChunkMetadata(\n",
    "                                    document_id=document_id,\n",
    "                                    location=location,\n",
    "                                    section_type=section_type,\n",
    "                                    section_title=section_title,\n",
    "                                    chunk_index=i,\n",
    "                                    total_chunks=len(section_chunks),\n",
    "                                    tokens=self.count_tokens(chunk_text),\n",
    "                                    characters=len(chunk_text),\n",
    "                                    **metadata_info\n",
    "                                )\n",
    "                            }\n",
    "                            all_chunks.append(chunk_data)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "def process_legal_csv(csv_path: str, output_path: str, \n",
    "                     target_tokens: int = 500, \n",
    "                     max_tokens: int = 800) -> None:\n",
    "    \"\"\"CSV dosyasƒ±nƒ± i≈üle ve chunk'larƒ± kaydet\"\"\"\n",
    "    \n",
    "    print(f\"üìñ CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚úÖ {len(df)} belge bulundu\")\n",
    "    \n",
    "    # Chunker'ƒ± ba≈ülat\n",
    "    chunker = TurkishLegalChunker(target_tokens=target_tokens, max_tokens=max_tokens)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    total_docs = len(df)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"üîÑ ƒ∞≈ülenen: {idx}/{total_docs} (%{idx/total_docs*100:.1f})\")\n",
    "        \n",
    "        try:\n",
    "            chunks = chunker.chunk_document(\n",
    "                document_id=row['_id'],\n",
    "                location=row['location'],\n",
    "                raw_text=row['rawText']\n",
    "            )\n",
    "            \n",
    "            # DataFrame i√ßin d√ºzle≈ütir\n",
    "            for chunk in chunks:\n",
    "                chunk_row = {\n",
    "                    'chunk_id': f\"{chunk['metadata'].document_id}_{chunk['metadata'].chunk_index}\",\n",
    "                    'document_id': chunk['metadata'].document_id,\n",
    "                    'location': chunk['metadata'].location,\n",
    "                    'section_type': chunk['metadata'].section_type,\n",
    "                    'section_title': chunk['metadata'].section_title,\n",
    "                    'chunk_index': chunk['metadata'].chunk_index,\n",
    "                    'total_chunks': chunk['metadata'].total_chunks,\n",
    "                    'text': chunk['text'],\n",
    "                    'tokens': chunk['metadata'].tokens,\n",
    "                    'characters': chunk['metadata'].characters,\n",
    "                    'has_dates': chunk['metadata'].has_dates,\n",
    "                    'has_legal_refs': chunk['metadata'].has_legal_refs,\n",
    "                    'dates': json.dumps(chunk['metadata'].dates),\n",
    "                    'case_numbers': json.dumps(chunk['metadata'].case_numbers)\n",
    "                }\n",
    "                all_chunks.append(chunk_row)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Hata - Belge {row['_id']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "    \n",
    "    # Sonu√ßlarƒ± kaydet\n",
    "    chunks_df = pd.DataFrame(all_chunks)\n",
    "    chunks_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    # ƒ∞statistikler\n",
    "    print(\"\\nüìä ƒ∞statistikler:\")\n",
    "    print(f\"Ortalama chunk boyutu: {chunks_df['tokens'].mean():.1f} token\")\n",
    "    print(f\"Medyan chunk boyutu: {chunks_df['tokens'].median():.1f} token\")\n",
    "    print(f\"Min chunk boyutu: {chunks_df['tokens'].min()} token\")\n",
    "    print(f\"Max chunk boyutu: {chunks_df['tokens'].max()} token\")\n",
    "    print(f\"Tarih i√ßeren chunk'lar: {chunks_df['has_dates'].sum()}\")\n",
    "    print(f\"Hukuki referans i√ßeren chunk'lar: {chunks_df['has_legal_refs'].sum()}\")\n",
    "    \n",
    "    # B√∂l√ºm t√ºrleri\n",
    "    print(f\"\\nüè∑Ô∏è B√∂l√ºm t√ºrleri:\")\n",
    "    print(chunks_df['section_type'].value_counts())\n",
    "    \n",
    "    print(f\"\\nüíæ Sonu√ßlar kaydedildi: {output_path}\")\n",
    "\n",
    "# √ñrnek kullanƒ±m\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV'yi i≈üle\n",
    "    process_legal_csv(\n",
    "        csv_path='/home/yapayzeka/ahsen_bulbul/data/10data.csv',\n",
    "        output_path='2legal_chunks.csv',\n",
    "        target_tokens=500,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    print(\"üéâ ƒ∞≈ülem tamamlandƒ±!\")\n",
    "    \n",
    "    # Sonu√ßlarƒ± kontrol et\n",
    "    df = pd.read_csv('2legal_chunks.csv')\n",
    "    print(f\"\\nüìã √ñrnek chunk'lar:\")\n",
    "    print(df[['chunk_id', 'section_type', 'tokens', 'text']].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a5324",
   "metadata": {},
   "source": [
    "\"qdrant_api_key\": \"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\"\n",
    "\"qdrant_client\": \"https://qdrant.adalet.gov.tr:443\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
