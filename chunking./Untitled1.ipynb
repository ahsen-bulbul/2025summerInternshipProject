{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72642fc7-59a4-46bf-bcb9-24e87298365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 CSV dosyası okunuyor: /home/yapayzeka/ahsen_bulbul/data/10data.csv\n",
      "✅ 20 belge bulundu\n",
      "🔄 İşlenen: 0/20 (%0.0)\n",
      "✅ Toplam 132 chunk oluşturuldu\n",
      "\n",
      "📊 İstatistikler:\n",
      "Ortalama chunk boyutu: 368.5 token\n",
      "Medyan chunk boyutu: 252.5 token\n",
      "Min chunk boyutu: 101 token\n",
      "Max chunk boyutu: 1224 token\n",
      "Tarih içeren chunk'lar: 132\n",
      "Hukuki referans içeren chunk'lar: 129\n",
      "\n",
      "🏷️ Bölüm türleri:\n",
      "section_type\n",
      "DAVA             57\n",
      "OTHER            38\n",
      "TEMYIZ           10\n",
      "KARAR             8\n",
      "GEREKÇE           7\n",
      "MAHKEME           7\n",
      "BOZMA             4\n",
      "DEĞERLENDIRME     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "💾 Sonuçlar kaydedildi: 2legal_chunks.csv\n",
      "🎉 İşlem tamamlandı!\n",
      "\n",
      "📋 Örnek chunk'lar:\n",
      "                     chunk_id section_type  tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text\n",
      "0  6750524485b6640290c37b06_0         DAVA     389                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          13. Hukuk Dairesi Taraflar arasındaki rücuen tazminat davasından dolayı yapılan yargılama sonunda, İlk Derece Mahkemesince davanın kabulüne karar verilmiştir. Kararın davacı vekili tarafından istinaf edilmesi üzerine, Bölge Adliye Mahkemesince başvurunun esastan reddine karar verilmiştir. Bölge Adliye Mahkemesi kararı davalı ... Grup Sosyal Hizmetleri İnş. Tem. Taah. Tic. A.Ş. vekili tarafından temyiz edilmekle; yapılan ön inceleme sonucunda gereği düşünüldü: İlk Derece Mahkemesi kararını istinaf etmeyen tarafın, Bölge Adliye Mahkemesinin istinaf başvurusunun esastan reddine ilişkin kararını temyiz etme hakkı bulunmadığından adı geçen davalı vekilinin temyiz dilekçesinin reddine karar verilmiştir. KARAR Açıklanan sebeple; Davalı ... Grup Sosyal Hizmetleri İnş. Tem. Taah. Tic. A.Ş. vekilinin temyiz dilekçesinin REDDİNE, Dosyanın İlk Derece Mahkemesine, kararın bir örneğinin kararı veren Bölge Adliye Mahkemesine gönderilmesine 18.01.2024 tarihinde oy birliğiyle karar verildi.\n",
      "1  6750524485b6640290c37b07_0        OTHER     216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasındaki tazminat davasından dolayı yapılan yargılama sonunda İlk Derece Mahkemesince davanın reddine karar verilmiştir. İlk Derece Mahkemesi kararı davacılar vekilince temyiz edilmekle; kesinlik, süre, temyiz şartı ve diğer usul eksiklikleri yönünden yapılan ön inceleme sonucunda, temyiz dilekçesinin kabulüne karar verildikten ve Tetkik Hâkimi tarafından hazırlanan rapor dinlendikten sonra dosyadaki belgeler incelenip gereği düşünüldü:\n",
      "2  6750524485b6640290c37b07_0      MAHKEME     325                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1. Asliye Hukuk Mahkemesinin 2009/139 D. ... dosyası üzerinden ihtiyati tedbir konularak inşaatın durdurulduğunu, HUMK'un 109. maddesi gereğince ihtiyati tedbir kararının verildiği tarihten itibaren 10 gün içerisinde esas hakkında davanın açılması gerektiğini ve bu durumun dosyaya ibrazı gerekirken davanın açılmadığını, Hukuk Usulü Muhakemeleri Kanununun 110. maddesi, ihtiyati tedbir kararının haksız olduğunun belirlenmesi halinde tedbir kararı yüzünden uğranılan zararın tazminini düzenlediğini, ihtiyati tedbir kararını icra ettiren tarafın yasal sürede dava açmaması halinde ihtiyati tedbirin haksız konulduğunun kabulü gerektiği, kaldı ki süresinde dava açsa da durumun değişmeyeceğini belirterek müvekkillerinin inşaatının geç bitirilmesinden kaynaklı 10.000,00 TL maddi tazminatın tahsiline karar verilmesini talep etmiştir.\n",
      "3  6750524485b6640290c37b07_0      MAHKEME     760  1.Asliye Hukuk Mahkemesinin 2009/139 D.... sayılı dosyası üzerinden 26.06.2009 tarihinde tedbir talep edildiği, yapılan keşif sonucu bilirkişilerin raporlarını ibraz ettikleri, 25.12.2009 tarihinde 2776 Parsel üzerinde yapımına devam edilen inşaatın tedbiren durdurulmasına karar verildiği, kararın infazı için 04.01.2010 tarihinde ... Belediye Başkanlığı'na ve Nöbetçi İcra Müdürlüğüne ayrı ayrı müzekkere yazıldığı, 05.01.2010 tarih saat 08:50 itibari ile inşaatın mühürlenerek tedbirin infaz edildiği, dolayısı ile tedbir kararın verildiği tarihten 11 gün geçtikten sonra kararın infaz edildiği, Mahkemenin infaz için yazdığı yazının 10 günlük süre içerisinde olduğu ancak infazın 11. gün gerçekleştiği, 10 günlük süre içerisinde herhangi bir dava açılmadığı, dolayısı ile HUMK'un 109. maddesi gereğince ihtiyati tedbirin infaz tarihi itibari ile kendiliğinden kalkmış olduğu, buna rağmen tedbirin infaz edildiği, HUMK'un 109. maddesi \"Aksi takdirde ihtiyati tedbir bir gûna merasime hacet kalmaksızın kendiliğinden kalkar ve iktizasına göre vazolunan tedbirin fiilen kaldırılması ihtiyati tedbiri tatbik eden daire veya memurdan talep olunabilir\" hükmünü düzenlediği, süresinden bir gün sonra tedbirin infazı sırasında davacıların yahut müteahhidin infaz memurlarından tedbir kararını isteyip süresinde dava açılıp açılmadığına dair de belgeyi istemeleri gerekirken bunu yapmadıkları, kaldı ki 05/01/2010 tarihinden hemen sonra aynı gün tedbirin verildiği dosyanın incelenerek dava açılıp açılmadığının tespit edilerek tedbirin kaldırılmasını talep etmeleri gerekirken infazdan 7 gün sonra 12.01.2010 tarihinde tedbirin kaldırılmasını talep ettikleri, böylece bir zarar varsa zararın ortaya çıkmasına ve artmasına bizzat mülk sahiplerinin ve müteahhidin sebep olduğu, zira inşaat durdurma zabtının altında... isimli şahsın imzasının olduğu, bu şahsın da inşaatı yapan müteahhit...'ın akrabası olduğu görülmekle açılan davanın reddine karar verilmiştir.\n",
      "4  6750524485b6640290c37b07_0        OTHER     304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              23. Hukuk Dairesinin 04.06.2013 tarihli ve 2013/2984 Esas, 2013/3773 Karar sayılı kararıyla dava tarihinde yürürlükte olan 1086 sayılı HUMK'un 109. maddesinin ''İhtiyati tedbir kararı dava ikamesinden evvel verilmiş ise tatbik edilmiş olsun olmasın kararın verildiği tarihten itibaren on gün zarfında esas hakkında dava ikamesi lazımdır. Bu müddette müddei, davasını ikame eylediğini müsbit evrakı, kararı tatbik eden memura ibrazla dosyaya vaz ve kaydettirerek mukabilinde ilmuhaber almaya mecburdur. Aksi takdirde ihtiyati tedbir bir gûna merasime hacet kalmaksızın kendiliğinden kalkar ve iktizasına göre vazolunan tedbirin fiilen kaldırılması, ihtiyati tedbiri tatbik eden daire veya memurdan talep olunabilir'' hükmünü içerdiği, somut olayda, davalının talebi üzerine ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Chunk metadata sınıfı\"\"\"\n",
    "    document_id: str\n",
    "    location: str\n",
    "    section_type: str\n",
    "    section_title: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    tokens: int\n",
    "    characters: int\n",
    "    has_dates: bool\n",
    "    has_legal_refs: bool\n",
    "    case_numbers: List[str]\n",
    "    dates: List[str]\n",
    "\n",
    "class TurkishLegalChunker:\n",
    "    \"\"\"Türkçe hukuki metinler için özelleştirilmiş chunker\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 target_tokens: int = 500, \n",
    "                 max_tokens: int = 800, \n",
    "                 min_tokens: int = 100,\n",
    "                 overlap_ratio: float = 0.1):\n",
    "        \n",
    "        self.target_tokens = target_tokens\n",
    "        self.max_tokens = max_tokens\n",
    "        self.min_tokens = min_tokens\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Regex patterns\n",
    "        self.section_patterns = [\n",
    "            r'([IVX]+\\.\\s+[A-ZÜĞŞÇÖIÜ][A-ZÜĞŞÇÖIÜa-züğşçöıi\\s]+)',  # Roma rakamları\n",
    "            r'([A-Z]\\.\\s+[A-ZÜĞŞÇÖIÜ][A-ZÜĞŞÇÖIÜa-züğşçöıi\\s]*)',   # A. B. C. bölümler\n",
    "            r'(\\d+\\.\\s*[A-ZÜĞŞÇÖIÜ][A-ZÜĞŞÇÖIÜa-züğşçöıi\\s]*)',    # 1. 2. 3. bölümler\n",
    "        ]\n",
    "        \n",
    "        self.date_pattern = r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}'\n",
    "        self.case_number_pattern = r'\\d{4}/\\d+\\s+[EK]\\.|[EK]\\.\\s*,\\s*\\d{4}/\\d+\\s+[EK]\\.'\n",
    "        \n",
    "        # Türkçe karakter düzeltme haritası\n",
    "        self.char_fixes = {\n",
    "            'Ä°': 'İ', 'Åž': 'Ş', 'ÄŸ': 'ğ', 'Ã¼': 'ü', 'Ã§': 'ç', \n",
    "            'Ä±': 'ı', 'Ã¶': 'ö', 'Ã': 'İ', 'Ã‡': 'Ç', 'ÃœÃ‡': 'ÜÇ',\n",
    "            'Â': '', 'â€': '', 'â€œ': '\"', 'â€': '\"'\n",
    "        }\n",
    "        \n",
    "        # Hukuki terimler (bölüm tespiti için)\n",
    "        self.legal_sections = {\n",
    "            'DAVA', 'CEVAP', 'MAHKEME', 'KARAR', 'TEMYIZ', 'ISTINAF', \n",
    "            'BOZMA', 'GEREKÇE', 'DEĞERLENDIRME', 'SONUÇ'\n",
    "        }\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Token sayısını hesapla\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def fix_encoding(self, text: str) -> str:\n",
    "        \"\"\"Türkçe karakter encoding sorunlarını düzelt\"\"\"\n",
    "        for wrong, correct in self.char_fixes.items():\n",
    "            text = text.replace(wrong, correct)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Metni ön işlemden geçir\"\"\"\n",
    "        # Encoding düzelt\n",
    "        text = self.fix_encoding(text)\n",
    "        \n",
    "        # Gereksiz boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Satır başlarındaki boşlukları temizle\n",
    "        text = re.sub(r'^\\s+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_metadata_info(self, text: str) -> Dict:\n",
    "        \"\"\"Metinden metadata bilgilerini çıkar\"\"\"\n",
    "        # Tarihler\n",
    "        dates = re.findall(self.date_pattern, text)\n",
    "        \n",
    "        # Dava numaraları\n",
    "        case_numbers = re.findall(self.case_number_pattern, text)\n",
    "        \n",
    "        # Hukuki referanslar\n",
    "        legal_refs = bool(re.search(r'\\d+\\s+sayılı|HMK|HUMK|İİK|TCK', text))\n",
    "        \n",
    "        return {\n",
    "            'dates': list(set(dates)),\n",
    "            'case_numbers': list(set(case_numbers)),\n",
    "            'has_legal_refs': legal_refs,\n",
    "            'has_dates': len(dates) > 0\n",
    "        }\n",
    "    \n",
    "    def detect_sections(self, text: str) -> List[Tuple[str, int, int]]:\n",
    "        \"\"\"Metindeki bölümleri tespit et\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        for pattern in self.section_patterns:\n",
    "            matches = list(re.finditer(pattern, text))\n",
    "            for match in matches:\n",
    "                section_title = match.group(1).strip()\n",
    "                start = match.start()\n",
    "                # Bir sonraki bölümü bul\n",
    "                end = len(text)\n",
    "                \n",
    "                # Sonraki eşleşmeyi bul\n",
    "                next_match = None\n",
    "                for next_pattern in self.section_patterns:\n",
    "                    next_matches = list(re.finditer(next_pattern, text[match.end():]))\n",
    "                    if next_matches:\n",
    "                        if next_match is None or next_matches[0].start() < next_match:\n",
    "                            next_match = next_matches[0].start() + match.end()\n",
    "                \n",
    "                if next_match:\n",
    "                    end = next_match\n",
    "                \n",
    "                sections.append((section_title, start, end))\n",
    "        \n",
    "        # Sırala ve çakışmaları temizle\n",
    "        sections = sorted(sections, key=lambda x: x[1])\n",
    "        cleaned_sections = []\n",
    "        \n",
    "        for i, (title, start, end) in enumerate(sections):\n",
    "            if i == 0:\n",
    "                cleaned_sections.append((title, start, end))\n",
    "            else:\n",
    "                prev_end = cleaned_sections[-1][2]\n",
    "                if start >= prev_end:\n",
    "                    cleaned_sections.append((title, start, end))\n",
    "                else:\n",
    "                    # Çakışma varsa önceki bölümün sonunu güncelle\n",
    "                    cleaned_sections[-1] = (cleaned_sections[-1][0], cleaned_sections[-1][1], start)\n",
    "                    cleaned_sections.append((title, start, end))\n",
    "        \n",
    "        return cleaned_sections\n",
    "    \n",
    "    def split_by_sentences(self, text: str, max_tokens: int) -> List[str]:\n",
    "        \"\"\"Cümleler bazında metni böl\"\"\"\n",
    "        # Türkçe için cümle sonu işaretleri\n",
    "        sentence_endings = r'[.!?]+(?=\\s+[A-ZÜĞŞÇÖIÜ]|\\s*$)'\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_chunk = f\"{current_chunk} {sentence}\".strip()\n",
    "            \n",
    "            if self.count_tokens(test_chunk) > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "            else:\n",
    "                current_chunk = test_chunk\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def create_overlapping_chunks(self, chunks: List[str]) -> List[str]:\n",
    "        \"\"\"Chunk'lar arası örtüşme oluştur\"\"\"\n",
    "        if len(chunks) <= 1:\n",
    "            return chunks\n",
    "        \n",
    "        overlapped_chunks = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                # İlk chunk\n",
    "                overlapped_chunks.append(chunk)\n",
    "            else:\n",
    "                # Önceki chunk'ın sonundan bir kısmını al\n",
    "                prev_chunk = chunks[i-1]\n",
    "                overlap_tokens = int(self.count_tokens(prev_chunk) * self.overlap_ratio)\n",
    "                \n",
    "                # Son cümleleri al (yaklaşık)\n",
    "                prev_words = prev_chunk.split()\n",
    "                overlap_words = prev_words[-overlap_tokens*2:] if len(prev_words) > overlap_tokens*2 else prev_words\n",
    "                overlap_text = \" \".join(overlap_words)\n",
    "                \n",
    "                overlapped_chunk = f\"{overlap_text} {chunk}\"\n",
    "                overlapped_chunks.append(overlapped_chunk)\n",
    "        \n",
    "        return overlapped_chunks\n",
    "    \n",
    "    def chunk_document(self, document_id: str, location: str, raw_text: str) -> List[Dict]:\n",
    "        \"\"\"Ana chunking fonksiyonu\"\"\"\n",
    "        # Ön işlem\n",
    "        text = self.preprocess_text(raw_text)\n",
    "        \n",
    "        # Metadata bilgilerini çıkar\n",
    "        metadata_info = self.extract_metadata_info(text)\n",
    "        \n",
    "        # Bölümleri tespit et\n",
    "        sections = self.detect_sections(text)\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        if not sections:\n",
    "            # Bölüm bulunamazsa tüm metni chunk'la\n",
    "            chunks = self.split_by_sentences(text, self.max_tokens)\n",
    "            chunks = self.create_overlapping_chunks(chunks)\n",
    "            \n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if self.count_tokens(chunk_text) >= self.min_tokens:\n",
    "                    chunk_data = {\n",
    "                        'text': chunk_text,\n",
    "                        'metadata': ChunkMetadata(\n",
    "                            document_id=document_id,\n",
    "                            location=location,\n",
    "                            section_type=\"FULL_DOCUMENT\",\n",
    "                            section_title=\"Tam Metin\",\n",
    "                            chunk_index=i,\n",
    "                            total_chunks=len(chunks),\n",
    "                            tokens=self.count_tokens(chunk_text),\n",
    "                            characters=len(chunk_text),\n",
    "                            **metadata_info\n",
    "                        )\n",
    "                    }\n",
    "                    all_chunks.append(chunk_data)\n",
    "        \n",
    "        else:\n",
    "            # Bölüm bazında chunk'la\n",
    "            for section_title, start, end in sections:\n",
    "                section_text = text[start:end].strip()\n",
    "                \n",
    "                if not section_text:\n",
    "                    continue\n",
    "                \n",
    "                # Bölüm tipini belirle\n",
    "                section_type = \"OTHER\"\n",
    "                for legal_term in self.legal_sections:\n",
    "                    if legal_term in section_title.upper():\n",
    "                        section_type = legal_term\n",
    "                        break\n",
    "                \n",
    "                if self.count_tokens(section_text) <= self.max_tokens:\n",
    "                    # Küçük bölüm, aynen kullan\n",
    "                    if self.count_tokens(section_text) >= self.min_tokens:\n",
    "                        chunk_data = {\n",
    "                            'text': section_text,\n",
    "                            'metadata': ChunkMetadata(\n",
    "                                document_id=document_id,\n",
    "                                location=location,\n",
    "                                section_type=section_type,\n",
    "                                section_title=section_title,\n",
    "                                chunk_index=0,\n",
    "                                total_chunks=1,\n",
    "                                tokens=self.count_tokens(section_text),\n",
    "                                characters=len(section_text),\n",
    "                                **metadata_info\n",
    "                            )\n",
    "                        }\n",
    "                        all_chunks.append(chunk_data)\n",
    "                else:\n",
    "                    # Büyük bölüm, alt chunk'lara böl\n",
    "                    section_chunks = self.split_by_sentences(section_text, self.max_tokens)\n",
    "                    section_chunks = self.create_overlapping_chunks(section_chunks)\n",
    "                    \n",
    "                    for i, chunk_text in enumerate(section_chunks):\n",
    "                        if self.count_tokens(chunk_text) >= self.min_tokens:\n",
    "                            chunk_data = {\n",
    "                                'text': chunk_text,\n",
    "                                'metadata': ChunkMetadata(\n",
    "                                    document_id=document_id,\n",
    "                                    location=location,\n",
    "                                    section_type=section_type,\n",
    "                                    section_title=section_title,\n",
    "                                    chunk_index=i,\n",
    "                                    total_chunks=len(section_chunks),\n",
    "                                    tokens=self.count_tokens(chunk_text),\n",
    "                                    characters=len(chunk_text),\n",
    "                                    **metadata_info\n",
    "                                )\n",
    "                            }\n",
    "                            all_chunks.append(chunk_data)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "def process_legal_csv(csv_path: str, output_path: str, \n",
    "                     target_tokens: int = 500, \n",
    "                     max_tokens: int = 800) -> None:\n",
    "    \"\"\"CSV dosyasını işle ve chunk'ları kaydet\"\"\"\n",
    "    \n",
    "    print(f\"📖 CSV dosyası okunuyor: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"✅ {len(df)} belge bulundu\")\n",
    "    \n",
    "    # Chunker'ı başlat\n",
    "    chunker = TurkishLegalChunker(target_tokens=target_tokens, max_tokens=max_tokens)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    total_docs = len(df)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"🔄 İşlenen: {idx}/{total_docs} (%{idx/total_docs*100:.1f})\")\n",
    "        \n",
    "        try:\n",
    "            chunks = chunker.chunk_document(\n",
    "                document_id=row['_id'],\n",
    "                location=row['location'],\n",
    "                raw_text=row['rawText']\n",
    "            )\n",
    "            \n",
    "            # DataFrame için düzleştir\n",
    "            for chunk in chunks:\n",
    "                chunk_row = {\n",
    "                    'chunk_id': f\"{chunk['metadata'].document_id}_{chunk['metadata'].chunk_index}\",\n",
    "                    'document_id': chunk['metadata'].document_id,\n",
    "                    'location': chunk['metadata'].location,\n",
    "                    'section_type': chunk['metadata'].section_type,\n",
    "                    'section_title': chunk['metadata'].section_title,\n",
    "                    'chunk_index': chunk['metadata'].chunk_index,\n",
    "                    'total_chunks': chunk['metadata'].total_chunks,\n",
    "                    'text': chunk['text'],\n",
    "                    'tokens': chunk['metadata'].tokens,\n",
    "                    'characters': chunk['metadata'].characters,\n",
    "                    'has_dates': chunk['metadata'].has_dates,\n",
    "                    'has_legal_refs': chunk['metadata'].has_legal_refs,\n",
    "                    'dates': json.dumps(chunk['metadata'].dates),\n",
    "                    'case_numbers': json.dumps(chunk['metadata'].case_numbers)\n",
    "                }\n",
    "                all_chunks.append(chunk_row)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Hata - Belge {row['_id']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"✅ Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "    \n",
    "    # Sonuçları kaydet\n",
    "    chunks_df = pd.DataFrame(all_chunks)\n",
    "    chunks_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    # İstatistikler\n",
    "    print(\"\\n📊 İstatistikler:\")\n",
    "    print(f\"Ortalama chunk boyutu: {chunks_df['tokens'].mean():.1f} token\")\n",
    "    print(f\"Medyan chunk boyutu: {chunks_df['tokens'].median():.1f} token\")\n",
    "    print(f\"Min chunk boyutu: {chunks_df['tokens'].min()} token\")\n",
    "    print(f\"Max chunk boyutu: {chunks_df['tokens'].max()} token\")\n",
    "    print(f\"Tarih içeren chunk'lar: {chunks_df['has_dates'].sum()}\")\n",
    "    print(f\"Hukuki referans içeren chunk'lar: {chunks_df['has_legal_refs'].sum()}\")\n",
    "    \n",
    "    # Bölüm türleri\n",
    "    print(f\"\\n🏷️ Bölüm türleri:\")\n",
    "    print(chunks_df['section_type'].value_counts())\n",
    "    \n",
    "    print(f\"\\n💾 Sonuçlar kaydedildi: {output_path}\")\n",
    "\n",
    "# Örnek kullanım\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV'yi işle\n",
    "    process_legal_csv(\n",
    "        csv_path='/home/yapayzeka/ahsen_bulbul/data/10data.csv',\n",
    "        output_path='2legal_chunks.csv',\n",
    "        target_tokens=500,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    print(\"🎉 İşlem tamamlandı!\")\n",
    "    \n",
    "    # Sonuçları kontrol et\n",
    "    df = pd.read_csv('2legal_chunks.csv')\n",
    "    print(f\"\\n📋 Örnek chunk'lar:\")\n",
    "    print(df[['chunk_id', 'section_type', 'tokens', 'text']].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a5324",
   "metadata": {},
   "source": [
    "\"qdrant_api_key\": \"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\"\n",
    "\"qdrant_client\": \"https://qdrant.adalet.gov.tr:443\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
