import pandas as pd
import re
import json
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import tiktoken

@dataclass
class ChunkMetadata:
    """Chunk metadata sÄ±nÄ±fÄ±"""
    document_id: str
    location: str
    section_type: str
    section_title: str
    chunk_index: int
    total_chunks: int
    tokens: int
    characters: int
    has_dates: bool
    has_legal_refs: bool
    case_numbers: List[str]
    dates: List[str]

class TurkishLegalChunker:
    """TÃ¼rkÃ§e hukuki metinler iÃ§in Ã¶zelleÅŸtirilmiÅŸ chunker"""
    
    def __init__(self, 
                 target_tokens: int = 500, 
                 max_tokens: int = 800, 
                 min_tokens: int = 100,
                 overlap_ratio: float = 0.1):
        
        self.target_tokens = target_tokens
        self.max_tokens = max_tokens
        self.min_tokens = min_tokens
        self.overlap_ratio = overlap_ratio
        
        # Tokenizer
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # Regex patterns
        self.section_patterns = [
            r'([IVX]+\.\s+[A-ZÃœÄÅÃ‡Ã–IÃœ][A-ZÃœÄÅÃ‡Ã–IÃœa-zÃ¼ÄŸÅŸÃ§Ã¶Ä±i\s]+)',  # Roma rakamlarÄ±
            r'([A-Z]\.\s+[A-ZÃœÄÅÃ‡Ã–IÃœ][A-ZÃœÄÅÃ‡Ã–IÃœa-zÃ¼ÄŸÅŸÃ§Ã¶Ä±i\s]*)',   # A. B. C. bÃ¶lÃ¼mler
            r'(\d+\.\s*[A-ZÃœÄÅÃ‡Ã–IÃœ][A-ZÃœÄÅÃ‡Ã–IÃœa-zÃ¼ÄŸÅŸÃ§Ã¶Ä±i\s]*)',    # 1. 2. 3. bÃ¶lÃ¼mler
        ]
        
        self.date_pattern = r'\d{1,2}\.\d{1,2}\.\d{4}'
        self.case_number_pattern = r'\d{4}/\d+\s+[EK]\.|[EK]\.\s*,\s*\d{4}/\d+\s+[EK]\.'
        
        # TÃ¼rkÃ§e karakter dÃ¼zeltme haritasÄ±
        self.char_fixes = {
            'Ã„Â°': 'Ä°', 'Ã…Å¾': 'Å', 'Ã„Å¸': 'ÄŸ', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÂ§': 'Ã§', 
            'Ã„Â±': 'Ä±', 'ÃƒÂ¶': 'Ã¶', 'Ãƒ': 'Ä°', 'Ãƒâ€¡': 'Ã‡', 'ÃƒÅ“Ãƒâ€¡': 'ÃœÃ‡',
            'Ã‚': '', 'Ã¢â‚¬': '', 'Ã¢â‚¬Å“': '"', 'Ã¢â‚¬': '"'
        }
        
        # Hukuki terimler (bÃ¶lÃ¼m tespiti iÃ§in)
        self.legal_sections = {
            'DAVA', 'CEVAP', 'MAHKEME', 'KARAR', 'TEMYIZ', 'ISTINAF', 
            'BOZMA', 'GEREKÃ‡E', 'DEÄERLENDIRME', 'SONUÃ‡'
        }

    def count_tokens(self, text: str) -> int:
        """Token sayÄ±sÄ±nÄ± hesapla"""
        return len(self.encoding.encode(text))
    
    def fix_encoding(self, text: str) -> str:
        """TÃ¼rkÃ§e karakter encoding sorunlarÄ±nÄ± dÃ¼zelt"""
        for wrong, correct in self.char_fixes.items():
            text = text.replace(wrong, correct)
        return text
    
    def preprocess_text(self, text: str) -> str:
        """Metni Ã¶n iÅŸlemden geÃ§ir"""
        # Encoding dÃ¼zelt
        text = self.fix_encoding(text)
        
        # Gereksiz boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text)
        
        # SatÄ±r baÅŸlarÄ±ndaki boÅŸluklarÄ± temizle
        text = re.sub(r'^\s+', '', text, flags=re.MULTILINE)
        
        return text.strip()
    
    def extract_metadata_info(self, text: str) -> Dict:
        """Metinden metadata bilgilerini Ã§Ä±kar"""
        # Tarihler
        dates = re.findall(self.date_pattern, text)
        
        # Dava numaralarÄ±
        case_numbers = re.findall(self.case_number_pattern, text)
        
        # Hukuki referanslar
        legal_refs = bool(re.search(r'\d+\s+sayÄ±lÄ±|HMK|HUMK|Ä°Ä°K|TCK', text))
        
        return {
            'dates': list(set(dates)),
            'case_numbers': list(set(case_numbers)),
            'has_legal_refs': legal_refs,
            'has_dates': len(dates) > 0
        }
    
    def detect_sections(self, text: str) -> List[Tuple[str, int, int]]:
        """Metindeki bÃ¶lÃ¼mleri tespit et"""
        sections = []
        
        for pattern in self.section_patterns:
            matches = list(re.finditer(pattern, text))
            for match in matches:
                section_title = match.group(1).strip()
                start = match.start()
                # Bir sonraki bÃ¶lÃ¼mÃ¼ bul
                end = len(text)
                
                # Sonraki eÅŸleÅŸmeyi bul
                next_match = None
                for next_pattern in self.section_patterns:
                    next_matches = list(re.finditer(next_pattern, text[match.end():]))
                    if next_matches:
                        if next_match is None or next_matches[0].start() < next_match:
                            next_match = next_matches[0].start() + match.end()
                
                if next_match:
                    end = next_match
                
                sections.append((section_title, start, end))
        
        # SÄ±rala ve Ã§akÄ±ÅŸmalarÄ± temizle
        sections = sorted(sections, key=lambda x: x[1])
        cleaned_sections = []
        
        for i, (title, start, end) in enumerate(sections):
            if i == 0:
                cleaned_sections.append((title, start, end))
            else:
                prev_end = cleaned_sections[-1][2]
                if start >= prev_end:
                    cleaned_sections.append((title, start, end))
                else:
                    # Ã‡akÄ±ÅŸma varsa Ã¶nceki bÃ¶lÃ¼mÃ¼n sonunu gÃ¼ncelle
                    cleaned_sections[-1] = (cleaned_sections[-1][0], cleaned_sections[-1][1], start)
                    cleaned_sections.append((title, start, end))
        
        return cleaned_sections
    
    def split_by_sentences(self, text: str, max_tokens: int) -> List[str]:
        """CÃ¼mleler bazÄ±nda metni bÃ¶l"""
        # TÃ¼rkÃ§e iÃ§in cÃ¼mle sonu iÅŸaretleri
        sentence_endings = r'[.!?]+(?=\s+[A-ZÃœÄÅÃ‡Ã–IÃœ]|\s*$)'
        sentences = re.split(sentence_endings, text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            test_chunk = f"{current_chunk} {sentence}".strip()
            
            if self.count_tokens(test_chunk) > max_tokens and current_chunk:
                chunks.append(current_chunk)
                current_chunk = sentence
            else:
                current_chunk = test_chunk
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def create_overlapping_chunks(self, chunks: List[str]) -> List[str]:
        """Chunk'lar arasÄ± Ã¶rtÃ¼ÅŸme oluÅŸtur"""
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = []
        
        for i, chunk in enumerate(chunks):
            if i == 0:
                # Ä°lk chunk
                overlapped_chunks.append(chunk)
            else:
                # Ã–nceki chunk'Ä±n sonundan bir kÄ±smÄ±nÄ± al
                prev_chunk = chunks[i-1]
                overlap_tokens = int(self.count_tokens(prev_chunk) * self.overlap_ratio)
                
                # Son cÃ¼mleleri al (yaklaÅŸÄ±k)
                prev_words = prev_chunk.split()
                overlap_words = prev_words[-overlap_tokens*2:] if len(prev_words) > overlap_tokens*2 else prev_words
                overlap_text = " ".join(overlap_words)
                
                overlapped_chunk = f"{overlap_text} {chunk}"
                overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks
    
    def chunk_document(self, document_id: str, location: str, raw_text: str) -> List[Dict]:
        """Ana chunking fonksiyonu"""
        # Ã–n iÅŸlem
        text = self.preprocess_text(raw_text)
        
        # Metadata bilgilerini Ã§Ä±kar
        metadata_info = self.extract_metadata_info(text)
        
        # BÃ¶lÃ¼mleri tespit et
        sections = self.detect_sections(text)
        
        all_chunks = []
        
        if not sections:
            # BÃ¶lÃ¼m bulunamazsa tÃ¼m metni chunk'la
            chunks = self.split_by_sentences(text, self.max_tokens)
            chunks = self.create_overlapping_chunks(chunks)
            
            for i, chunk_text in enumerate(chunks):
                if self.count_tokens(chunk_text) >= self.min_tokens:
                    chunk_data = {
                        'text': chunk_text,
                        'metadata': ChunkMetadata(
                            document_id=document_id,
                            location=location,
                            section_type="FULL_DOCUMENT",
                            section_title="Tam Metin",
                            chunk_index=i,
                            total_chunks=len(chunks),
                            tokens=self.count_tokens(chunk_text),
                            characters=len(chunk_text),
                            **metadata_info
                        )
                    }
                    all_chunks.append(chunk_data)
        
        else:
            # BÃ¶lÃ¼m bazÄ±nda chunk'la
            for section_title, start, end in sections:
                section_text = text[start:end].strip()
                
                if not section_text:
                    continue
                
                # BÃ¶lÃ¼m tipini belirle
                section_type = "OTHER"
                for legal_term in self.legal_sections:
                    if legal_term in section_title.upper():
                        section_type = legal_term
                        break
                
                if self.count_tokens(section_text) <= self.max_tokens:
                    # KÃ¼Ã§Ã¼k bÃ¶lÃ¼m, aynen kullan
                    if self.count_tokens(section_text) >= self.min_tokens:
                        chunk_data = {
                            'text': section_text,
                            'metadata': ChunkMetadata(
                                document_id=document_id,
                                location=location,
                                section_type=section_type,
                                section_title=section_title,
                                chunk_index=0,
                                total_chunks=1,
                                tokens=self.count_tokens(section_text),
                                characters=len(section_text),
                                **metadata_info
                            )
                        }
                        all_chunks.append(chunk_data)
                else:
                    # BÃ¼yÃ¼k bÃ¶lÃ¼m, alt chunk'lara bÃ¶l
                    section_chunks = self.split_by_sentences(section_text, self.max_tokens)
                    section_chunks = self.create_overlapping_chunks(section_chunks)
                    
                    for i, chunk_text in enumerate(section_chunks):
                        if self.count_tokens(chunk_text) >= self.min_tokens:
                            chunk_data = {
                                'text': chunk_text,
                                'metadata': ChunkMetadata(
                                    document_id=document_id,
                                    location=location,
                                    section_type=section_type,
                                    section_title=section_title,
                                    chunk_index=i,
                                    total_chunks=len(section_chunks),
                                    tokens=self.count_tokens(chunk_text),
                                    characters=len(chunk_text),
                                    **metadata_info
                                )
                            }
                            all_chunks.append(chunk_data)
        
        return all_chunks

def process_legal_csv(csv_path: str, output_path: str, 
                     target_tokens: int = 500, 
                     max_tokens: int = 800) -> None:
    """CSV dosyasÄ±nÄ± iÅŸle ve chunk'larÄ± kaydet"""
    
    print(f"ğŸ“– CSV dosyasÄ± okunuyor: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"âœ… {len(df)} belge bulundu")
    
    # Chunker'Ä± baÅŸlat
    chunker = TurkishLegalChunker(target_tokens=target_tokens, max_tokens=max_tokens)
    
    all_chunks = []
    
    # Progress tracking
    total_docs = len(df)
    
    for idx, row in df.iterrows():
        if idx % 100 == 0:
            print(f"ğŸ”„ Ä°ÅŸlenen: {idx}/{total_docs} (%{idx/total_docs*100:.1f})")
        
        try:
            chunks = chunker.chunk_document(
                document_id=row['_id'],
                location=row['location'],
                raw_text=row['rawText']
            )
            
            # DataFrame iÃ§in dÃ¼zleÅŸtir
            for chunk in chunks:
                chunk_row = {
                    'chunk_id': f"{chunk['metadata'].document_id}_{chunk['metadata'].chunk_index}",
                    'document_id': chunk['metadata'].document_id,
                    'location': chunk['metadata'].location,
                    'section_type': chunk['metadata'].section_type,
                    'section_title': chunk['metadata'].section_title,
                    'chunk_index': chunk['metadata'].chunk_index,
                    'total_chunks': chunk['metadata'].total_chunks,
                    'text': chunk['text'],
                    'tokens': chunk['metadata'].tokens,
                    'characters': chunk['metadata'].characters,
                    'has_dates': chunk['metadata'].has_dates,
                    'has_legal_refs': chunk['metadata'].has_legal_refs,
                    'dates': json.dumps(chunk['metadata'].dates),
                    'case_numbers': json.dumps(chunk['metadata'].case_numbers)
                }
                all_chunks.append(chunk_row)
                
        except Exception as e:
            print(f"âŒ Hata - Belge {row['_id']}: {str(e)}")
            continue
    
    print(f"âœ… Toplam {len(all_chunks)} chunk oluÅŸturuldu")
    
    # SonuÃ§larÄ± kaydet
    chunks_df = pd.DataFrame(all_chunks)
    chunks_df.to_csv(output_path, index=False, encoding='utf-8')
    
    # Ä°statistikler
    print("\nğŸ“Š Ä°statistikler:")
    print(f"Ortalama chunk boyutu: {chunks_df['tokens'].mean():.1f} token")
    print(f"Medyan chunk boyutu: {chunks_df['tokens'].median():.1f} token")
    print(f"Min chunk boyutu: {chunks_df['tokens'].min()} token")
    print(f"Max chunk boyutu: {chunks_df['tokens'].max()} token")
    print(f"Tarih iÃ§eren chunk'lar: {chunks_df['has_dates'].sum()}")
    print(f"Hukuki referans iÃ§eren chunk'lar: {chunks_df['has_legal_refs'].sum()}")
    
    # BÃ¶lÃ¼m tÃ¼rleri
    print(f"\nğŸ·ï¸ BÃ¶lÃ¼m tÃ¼rleri:")
    print(chunks_df['section_type'].value_counts())
    
    print(f"\nğŸ’¾ SonuÃ§lar kaydedildi: {output_path}")

# Ã–rnek kullanÄ±m
if __name__ == "__main__":
    # CSV'yi iÅŸle
    process_legal_csv(
        csv_path='yargitay.csv',
        output_path='legal_chunks.csv',
        target_tokens=500,
        max_tokens=800
    )
    
    print("ğŸ‰ Ä°ÅŸlem tamamlandÄ±!")
    
    # SonuÃ§larÄ± kontrol et
    df = pd.read_csv('legal_chunks.csv')
    print(f"\nğŸ“‹ Ã–rnek chunk'lar:")
    print(df[['chunk_id', 'section_type', 'tokens', 'text']].head().to_string())