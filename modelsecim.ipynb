{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "🏛️ YARGITAY DİNAMİK MODEL SİSTEMİ\n",
      "\n",
      "============================================================\n",
      "🤖 MODEL SEÇİMİ\n",
      "============================================================\n",
      "\n",
      "🌍 Çok Dilli Modeller:\n",
      "  bge-m3: BGE-M3 - Çok dilli, dense+sparse embedding destekli\n",
      "    └─ Boyut: 1024, Sparse: ✅(NONE), Max Token: 8192\n",
      "  multilingual-e5: E5 Multilingual Large - Çok dilli dense embedding\n",
      "    └─ Boyut: 1024, Sparse: ❌, Max Token: 512\n",
      "\n",
      "🇹🇷 Türkçe Özel:\n",
      "  turkish-bert: Turkish BERT - Türkçe özelleştirilmiş\n",
      "    └─ Boyut: 768, Sparse: ❌, Max Token: 512\n",
      "  distilbert-turkish: Hızlı Türkçe DistilBERT\n",
      "    └─ Boyut: 768, Sparse: ✅(TFIDF), Max Token: 512\n",
      "\n",
      "⚡ Hızlı & Genel:\n",
      "  bge-large: BGE Large - Sadece dense embedding\n",
      "    └─ Boyut: 1024, Sparse: ❌, Max Token: 512\n",
      "  all-mpnet: All-MiniLM - Genel amaçlı, hızlı\n",
      "    └─ Boyut: 768, Sparse: ❌, Max Token: 384\n",
      "\n",
      "💡 Öneri:\n",
      "  • Türkçe ağırlıklı: turkish-bert (TF-IDF sparse)\n",
      "  • En iyi performans: bge-m3 (Native sparse)\n",
      "  • Hızlı Türkçe: distilbert-turkish (TF-IDF sparse)\n",
      "  • Çok dilli: multilingual-e5 (TF-IDF sparse)\n",
      "\n",
      "🔍 Sparse Embedding Türleri:\n",
      "  • NATIVE: Model'in kendi sparse sistemi (sadece BGE-M3)\n",
      "  • TFIDF: TF-IDF tabanlı sparse embedding (tüm diğer modeller)\n",
      "bge-m3: BGE-M3 - Çok dilli, dense+sparse embedding destekli\n",
      "  └─ Boyut: 1024, Sparse: True\n",
      "bge-large: BGE Large - Sadece dense embedding\n",
      "  └─ Boyut: 1024, Sparse: False\n",
      "multilingual-e5: E5 Multilingual Large - Çok dilli dense embedding\n",
      "  └─ Boyut: 1024, Sparse: False\n",
      "turkish-bert: Turkish BERT - Türkçe özelleştirilmiş\n",
      "  └─ Boyut: 768, Sparse: False\n",
      "distilbert-turkish: Hızlı Türkçe DistilBERT\n",
      "  └─ Boyut: 768, Sparse: True\n",
      "all-mpnet: All-MiniLM - Genel amaçlı, hızlı\n",
      "  └─ Boyut: 768, Sparse: False\n",
      "\n",
      "Mevcut modeller: bge-m3, bge-large, multilingual-e5, turkish-bert, distilbert-turkish, all-mpnet\n",
      "✅ Seçilen model: dbmdz/distilbert-base-turkish-cased\n",
      "🔮 Model yükleniyor: dbmdz/distilbert-base-turkish-cased (tip: sentence_transformer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name dbmdz/distilbert-base-turkish-cased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model yüklendi: dbmdz/distilbert-base-turkish-cased\n",
      "✅ Hazır - Model: dbmdz/distilbert-base-turkish-cased | Cihaz: NVIDIA RTX A6000\n",
      "\n",
      "============================================================\n",
      "🏛️ YARGITAY SEMANTİK SİSTEM - Model: dbmdz/distilbert-base-turkish-cased\n",
      "============================================================\n",
      "1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\n",
      "2) İnteraktif arama\n",
      "3) Koleksiyon bilgilerini göster\n",
      "4) Model değiştir\n",
      "5) Çıkış\n",
      "🚀 Full pipeline başlıyor\n",
      "✅ Dense embedding boyutu: 512\n",
      "🔍 Sparse embedding: 7 terim (TFIDF)\n",
      "❌ Model bağlantı hatası: 'ModelManager' object has no attribute 'fitted_tfidf'\n",
      "❌ Hata çıktı\n",
      "\n",
      "============================================================\n",
      "🏛️ YARGITAY SEMANTİK SİSTEM - Model: dbmdz/distilbert-base-turkish-cased\n",
      "============================================================\n",
      "1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\n",
      "2) İnteraktif arama\n",
      "3) Koleksiyon bilgilerini göster\n",
      "4) Model değiştir\n",
      "5) Çıkış\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# Dynamic Model Selection + SemChunk + Qdrant Entegrasyon\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Model yapılandırmaları\n",
    "MODEL_CONFIGS = {\n",
    "    \"bge-m3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"model_type\": \"bge\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 8192,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE-M3 - Çok dilli, dense+sparse embedding destekli\"\n",
    "    },\n",
    "    \"bge-large\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE Large - Sadece dense embedding\"\n",
    "    },\n",
    "    \"multilingual-e5\": {\n",
    "        \"model_name\": \"intfloat/multilingual-e5-large\",\n",
    "        \"model_type\": \"sentence_transformer\", \n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"E5 Multilingual Large - Çok dilli dense embedding\"\n",
    "    },\n",
    "    \"turkish-bert\": {\n",
    "        \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"Turkish BERT - Türkçe özelleştirilmiş\"\n",
    "    },\n",
    "    \"distilbert-turkish\": {   # 👈 EKLENMELİ\n",
    "        \"model_name\": \"dbmdz/distilbert-base-turkish-cased\",\n",
    "        \"model_type\":\"sentence_transformer\",\n",
    "        \"description\": \"Hızlı Türkçe DistilBERT\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"sparse_type\": \"tfidf\"\n",
    "    },\n",
    "    \"all-mpnet\": {\n",
    "        \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 384,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"All-MiniLM - Genel amaçlı, hızlı\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    MODEL_TYPE: str = \"bge\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"dynamic_model_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "    SUPPORTS_SPARSE: bool = True\n",
    "    SUPPORTS_DENSE: bool = True\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Dinamik model yönetimi için sınıf\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, config: Config):\n",
    "        self.model_key = model_key\n",
    "        self.config = config\n",
    "        self.model_config = MODEL_CONFIGS.get(model_key)\n",
    "        \n",
    "        if not self.model_config:\n",
    "            raise ValueError(f\"Desteklenmeyen model: {model_key}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.vectorizer = None  # Sparse embedding için\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Seçilen modeli yükle\"\"\"\n",
    "        try:\n",
    "            model_name = self.model_config[\"model_name\"]\n",
    "            model_type = self.model_config[\"model_type\"]\n",
    "            \n",
    "            print(f\"🔮 Model yükleniyor: {model_name} (tip: {model_type})\")\n",
    "            \n",
    "            if model_type == \"bge\":\n",
    "                self.model = BGEM3FlagModel(\n",
    "                    model_name, \n",
    "                    use_fp16=self.config.USE_FP16, \n",
    "                    device=self.config.DEVICE\n",
    "                )\n",
    "            elif model_type == \"sentence_transformer\":\n",
    "                self.model = SentenceTransformer(model_name, device=self.config.DEVICE)\n",
    "                # Sparse embedding için TF-IDF hazırla\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "            else:\n",
    "                raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "                \n",
    "            print(f\"✅ Model yüklendi: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Model yükleme hatası: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Metinleri encode et - model tipine göre\"\"\"\n",
    "        dense_embeddings = []\n",
    "        sparse_embeddings = []\n",
    "        \n",
    "        try:\n",
    "            if self.model_config[\"model_type\"] == \"bge\" and hasattr(self.model, 'encode'):\n",
    "                # BGE-M3 için\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    result = self.model.encode(\n",
    "                        texts, \n",
    "                        return_dense=True, \n",
    "                        return_sparse=True\n",
    "                    )\n",
    "                    dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "                    sparse_raw = result.get(\"sparse_vecs\", [])\n",
    "                    sparse_embeddings = [\n",
    "                        {\"indices\": s.get(\"indices\", []), \"values\": s.get(\"values\", [])} \n",
    "                        for s in sparse_raw\n",
    "                    ] if sparse_raw else [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                else:\n",
    "                    dense_embeddings = self.model.encode(texts, return_dense=True)\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                    \n",
    "            else:\n",
    "                # Sentence Transformer için\n",
    "                dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "                \n",
    "                # Sparse embedding TF-IDF ile\n",
    "                if self.vectorizer:\n",
    "                    X_sparse = self.vectorizer.fit_transform(texts)\n",
    "                    sparse_embeddings = []\n",
    "                    for i in range(X_sparse.shape[0]):\n",
    "                        row = X_sparse[i].tocoo()\n",
    "                        sparse_embeddings.append({\n",
    "                            \"indices\": row.col.tolist(),\n",
    "                            \"values\": row.data.tolist()\n",
    "                        })\n",
    "                else:\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            \n",
    "            # Embedding boyutunu ayarla\n",
    "            target_dim = self.config.EMBEDDING_DIM\n",
    "            dense_clean = []\n",
    "            for vec in dense_embeddings:\n",
    "                if vec is None:\n",
    "                    dense_clean.append([0.0] * target_dim)\n",
    "                elif len(vec) < target_dim:\n",
    "                    dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "                else:\n",
    "                    dense_clean.append(vec[:target_dim])\n",
    "            \n",
    "            return dense_clean, sparse_embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Encoding hatası: {e}\")\n",
    "            # Fallback\n",
    "            return (\n",
    "                [[0.0] * self.config.EMBEDDING_DIM for _ in texts],\n",
    "                [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            )\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Model bilgilerini döndür\"\"\"\n",
    "        info = self.model_config.copy()\n",
    "        info[\"loaded\"] = self.model is not None\n",
    "        info[\"current_embedding_dim\"] = self.config.EMBEDDING_DIM\n",
    "        return info\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.config = config\n",
    "        self.model_manager = ModelManager(model_key, config)\n",
    "        \n",
    "        # Model bilgilerini config'e aktar\n",
    "        model_config = self.model_manager.model_config\n",
    "        self.config.BGE_MODEL_NAME = model_config[\"model_name\"]\n",
    "        self.config.MODEL_TYPE = model_config[\"model_type\"]\n",
    "        self.config.SUPPORTS_SPARSE = model_config[\"supports_sparse\"]\n",
    "        self.config.SUPPORTS_DENSE = model_config[\"supports_dense\"]\n",
    "        \n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Model yükle\n",
    "        if not self.model_manager.load_model():\n",
    "            raise RuntimeError(\"Model yüklenemedi!\")\n",
    "        \n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"✅ Hazır - Model: {model_config['model_name']} | Cihaz: {device_name}\")\n",
    "\n",
    "    def test_model_connection(self):\n",
    "        \"\"\"Model bağlantısını test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts(test_text) # fit_tfidf=True)\n",
    "            \n",
    "            sparse_method = self.model_manager.model_config.get(\"sparse_type\", \"none\")\n",
    "            sparse_count = len(sparse_emb[0]['indices']) if sparse_emb[0]['indices'] else 0\n",
    "            \n",
    "            print(f\"✅ Dense embedding boyutu: {len(dense_emb[0])}\")\n",
    "            print(f\"🔍 Sparse embedding: {sparse_count} terim ({sparse_method.upper()})\")\n",
    "            \n",
    "            # if self.model_manager.fitted_tfidf:\n",
    "            #     vocab_size = len(self.model_manager.tfidf_vectorizer.vocabulary_)\n",
    "            #     print(f\"📚 TF-IDF vocabulary: {vocab_size:,} terim\")\n",
    "            \n",
    "            return len(dense_emb[0])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Model bağlantı hatası: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense vector config\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM, \n",
    "                        distance=models.Distance.COSINE\n",
    "                    ),\n",
    "                }\n",
    "                \n",
    "                # Sparse config (eğer destekleniyorsa)\n",
    "                sparse_config = {}\n",
    "                if self.config.SUPPORTS_SPARSE:\n",
    "                    sparse_config = {\n",
    "                        \"sparse_vec\": models.SparseVectorParams(\n",
    "                            index=models.SparseIndexParams(on_disk=False)\n",
    "                        )\n",
    "                    }\n",
    "                \n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config=sparse_config if sparse_config else None\n",
    "                )\n",
    "                \n",
    "                support_info = \"Dense+Sparse\" if sparse_config else \"Dense only\"\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} ({support_info})\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding oluştur\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"🔮 {total} metin işleniyor (model: {self.config.BGE_MODEL_NAME})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  📊 Batch işlendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Embedding hatası (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"📄 CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"❌ Ana metin sütunu bulunamadı\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "                'model_used': self.config.BGE_MODEL_NAME  # Hangi model kullanıldığını kaydet\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ✅ İşlenen satır: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings(texts)\n",
    "\n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vector_dict = {\"dense_vec\": d}\n",
    "            \n",
    "            # Sparse vector sadece destekleniyorsa ekle\n",
    "            if self.config.SUPPORTS_SPARSE and s[\"indices\"]:\n",
    "                vector_dict[\"sparse_vec\"] = SparseVector(\n",
    "                    indices=s[\"indices\"],\n",
    "                    values=s[\"values\"]\n",
    "                )\n",
    "            \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector_dict,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "        try:\n",
    "            dense_emb, _ = self.model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0]\n",
    "            \n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=NamedVector(name=\"dense_vec\", vector=query_vector),\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"📊 {len(results)} sonuç bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Semantic search hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Hybrid search (sadece destekleniyorsa)\"\"\"\n",
    "        if not self.config.SUPPORTS_SPARSE:\n",
    "            print(\"⚠️ Bu model sparse embedding desteklemiyor, dense search yapılıyor...\")\n",
    "            return self.search_semantic(query, limit, score_threshold)\n",
    "        \n",
    "        try:\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts([query])\n",
    "            \n",
    "            requests = [\n",
    "                SearchRequest(\n",
    "                    vector=NamedVector(name=\"dense_vec\", vector=dense_emb[0]),\n",
    "                    limit=limit,\n",
    "                    with_payload=True,\n",
    "                    score_threshold=score_threshold\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Sparse varsa ekle\n",
    "            if sparse_emb[0][\"indices\"]:\n",
    "                requests.append(\n",
    "                    SearchRequest(\n",
    "                        vector=NamedSparseVector(\n",
    "                            name=\"sparse_vec\",\n",
    "                            vector=SparseVector(\n",
    "                                indices=sparse_emb[0][\"indices\"],\n",
    "                                values=sparse_emb[0][\"values\"]\n",
    "                            )\n",
    "                        ),\n",
    "                        limit=limit,\n",
    "                        score_threshold=score_threshold\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            results = []\n",
    "            for request_result in qr:\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"📊 {len(results)} sonuç bulundu (Hybrid)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Hybrid search hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            model_info = self.model_manager.get_model_info()\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"model_info\": model_info,\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM,\n",
    "                \"supports_sparse\": self.config.SUPPORTS_SPARSE\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.processor = YargitaySemanticProcessor(config, model_key)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Full pipeline başlıyor\")\n",
    "        emb_dim = self.processor.test_model_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ Chunk bulunamadı\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\n🔎 İnteraktif arama başlatıldı\")\n",
    "        model_info = self.processor.model_manager.get_model_info()\n",
    "        print(f\"📱 Aktif model: {model_info['model_name']}\")\n",
    "        print(f\"🔧 Özellikler: Dense✅, Sparse{'✅' if model_info['supports_sparse'] else '❌'}\")\n",
    "        \n",
    "        while True:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"🔍 ARAMA SEÇENEKLERİ\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(\"1) Dense arama (Semantic)\")\n",
    "            if model_info['supports_sparse']:\n",
    "                print(\"2) Hybrid arama (Dense + Sparse)\")\n",
    "                print(\"3) Karşılaştırmalı arama (Her iki yöntem)\")\n",
    "            else:\n",
    "                print(\"2) ❌ Hybrid arama (Bu model desteklemiyor)\")\n",
    "                print(\"3) ❌ Karşılaştırma (Sparse desteklenmiyor)\")\n",
    "            print(\"4) Ana menü\")\n",
    "            \n",
    "            ch = input(\"Seçiminiz (1-4): \").strip()\n",
    "            if ch==\"4\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\",\"3\"}:\n",
    "                print(\"❌ Geçersiz seçim\")\n",
    "                continue\n",
    "                \n",
    "            q = input(\"🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                limit = int(input(\"Kaç sonuç? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                print(\"\\n🎯 Dense Semantic Search...\")\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(results, \"Dense\")\n",
    "                \n",
    "            elif ch==\"2\" and model_info['supports_sparse']:\n",
    "                print(\"\\n🔀 Hybrid Search...\")\n",
    "                results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(results, \"Hybrid\")\n",
    "                \n",
    "            elif ch==\"3\" and model_info['supports_sparse']:\n",
    "                print(\"\\n📊 Karşılaştırmalı Arama...\")\n",
    "                print(\"🎯 Dense sonuçlar:\")\n",
    "                dense_results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(dense_results, \"Dense\", show_comparison=True)\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"🔀 Hybrid sonuçlar:\")\n",
    "                hybrid_results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(hybrid_results, \"Hybrid\", show_comparison=True)\n",
    "                \n",
    "            else:\n",
    "                print(\"⚠️ Bu özellik seçilen model tarafından desteklenmiyor.\")\n",
    "\n",
    "    def _display_results(self, results: List[Dict], search_type: str, show_comparison: bool = False):\n",
    "        \"\"\"Sonuçları görüntüle\"\"\"\n",
    "        if not results:\n",
    "            print(\"❌ Sonuç bulunamadı\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n📋 {len(results)} {search_type} sonuç:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            p = r.get(\"payload\") or {}\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            \n",
    "            # Skor rengine göre emoji\n",
    "            if score > 0.8:\n",
    "                score_icon = \"🟢\"\n",
    "            elif score > 0.6:\n",
    "                score_icon = \"🟡\"\n",
    "            else:\n",
    "                score_icon = \"🔴\"\n",
    "                \n",
    "            print(f\"\\n{i}. {score_icon} Skor: {score:.4f}\")\n",
    "            print(f\"   📄 Model: {p.get('model_used','N/A')}\")\n",
    "            print(f\"   🏛️ Daire: {p.get('daire','N/A')} | 📅 Tarih: {p.get('tarih','N/A')}\")\n",
    "            print(f\"   📋 Esas: {p.get('esas_no','N/A')} | 🔢 Karar: {p.get('karar_no','N/A')}\")\n",
    "            \n",
    "            text = p.get('text', '')\n",
    "            if len(text) > 200:\n",
    "                text_preview = text[:200] + \"...\"\n",
    "            else:\n",
    "                text_preview = text\n",
    "            print(f\"   📝 Metin: {text_preview}\")\n",
    "            \n",
    "            if show_comparison:\n",
    "                print(f\"   🏷️ Tip: {search_type}\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "def select_model() -> str:\n",
    "    \"\"\"Kullanıcıdan model seçimi al\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🤖 MODEL SEÇİMİ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Modelleri kategorilere ayır\n",
    "    categories = {\n",
    "        \"🌍 Çok Dilli Modeller\": [\"bge-m3\", \"multilingual-e5\"],\n",
    "        \"🇹🇷 Türkçe Özel\": [\"turkish-bert\", \"distilbert-turkish\"],\n",
    "        \"⚡ Hızlı & Genel\": [\"bge-large\", \"all-mpnet\"]\n",
    "    }\n",
    "    \n",
    "    for category, models in categories.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for model_key in models:\n",
    "            config = MODEL_CONFIGS[model_key]\n",
    "            sparse_type = config.get('sparse_type', 'none')\n",
    "            sparse_icon = f\"✅({sparse_type.upper()})\" if config['supports_sparse'] else \"❌\"\n",
    "            print(f\"  {model_key}: {config['description']}\")\n",
    "            print(f\"    └─ Boyut: {config['embedding_dim']}, Sparse: {sparse_icon}, Max Token: {config['max_seq_length']}\")\n",
    "    \n",
    "    print(f\"\\n💡 Öneri:\")\n",
    "    print(\"  • Türkçe ağırlıklı: turkish-bert (TF-IDF sparse)\")\n",
    "    print(\"  • En iyi performans: bge-m3 (Native sparse)\")\n",
    "    print(\"  • Hızlı Türkçe: distilbert-turkish (TF-IDF sparse)\")\n",
    "    print(\"  • Çok dilli: multilingual-e5 (TF-IDF sparse)\")\n",
    "    \n",
    "    print(f\"\\n🔍 Sparse Embedding Türleri:\")\n",
    "    print(\"  • NATIVE: Model'in kendi sparse sistemi (sadece BGE-M3)\")\n",
    "    print(\"  • TFIDF: TF-IDF tabanlı sparse embedding (tüm diğer modeller)\")\n",
    "    \n",
    "    # while True:\n",
    "    #     choice = input(\"\\nModel seçin (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "    #     if choice in MODEL_CONFIGS:\n",
    "    #         selected_config = MODEL_CONFIGS[choice]\n",
    "    #         sparse_method = selected_config.get('sparse_type', 'none')\n",
    "    #         print(f\"✅ Seçilen model: {selected_config['model_name']}\")\n",
    "    #         print(f\"📊 Sparse method: {sparse_method.upper()}\")\n",
    "    #         return choice\n",
    "    #     print(\"❌ Geçersiz model! Mevcut:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    \n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"{key}: {config['description']}\")\n",
    "        print(f\"  └─ Boyut: {config['embedding_dim']}, Sparse: {config['supports_sparse']}\")\n",
    "\n",
    "    print(\"\\nMevcut modeller:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nModel seçin (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            print(f\"✅ Seçilen model: {MODEL_CONFIGS[choice]['model_name']}\")\n",
    "            return choice\n",
    "        print(\"❌ Geçersiz model! Tekrar deneyin.\")\n",
    "\n",
    "def main():\n",
    "    print(\"🏛️ YARGITAY DİNAMİK MODEL SİSTEMİ\")\n",
    "    \n",
    "    # Model seçimi\n",
    "    selected_model = select_model()\n",
    "    selected_config = MODEL_CONFIGS[selected_model]\n",
    "    \n",
    "    # Config oluştur\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=f\"{selected_model}_chunks\",  # Model adına göre koleksiyon\n",
    "        EMBEDDING_DIM=min(512, selected_config[\"embedding_dim\"]),  # Boyutu ayarla\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pipeline = YargitayPipeline(config, selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline oluşturma hatası: {e}\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"🏛️ YARGITAY SEMANTİK SİSTEM - Model: {selected_config['model_name']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) İnteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini göster\")\n",
    "        print(\"4) Model değiştir\")\n",
    "        print(\"5) Çıkış\")\n",
    "        choice = input(\"Seçiminiz (1-5): \").strip()\n",
    "        \n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"✅ Tamamlandı\" if ok else \"❌ Hata çıktı\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            # Model değiştir ve sistemi yeniden başlat\n",
    "            selected_model = select_model()\n",
    "            selected_config = MODEL_CONFIGS[selected_model]\n",
    "            config.COLLECTION_NAME = f\"{selected_model}_chunks\"\n",
    "            config.EMBEDDING_DIM = min(512, selected_config[\"embedding_dim\"])\n",
    "            try:\n",
    "                pipeline = YargitayPipeline(config, selected_model)\n",
    "                print(\"✅ Model değiştirildi!\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Model değiştirme hatası: {e}\")\n",
    "        elif choice==\"5\":\n",
    "            print(\"👋 Görüşürüz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
