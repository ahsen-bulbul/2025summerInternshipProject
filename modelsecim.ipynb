{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "üèõÔ∏è YARGITAY Dƒ∞NAMƒ∞K MODEL Sƒ∞STEMƒ∞\n",
      "\n",
      "============================================================\n",
      "ü§ñ MODEL SE√áƒ∞Mƒ∞\n",
      "============================================================\n",
      "\n",
      "üåç √áok Dilli Modeller:\n",
      "  bge-m3: BGE-M3 - √áok dilli, dense+sparse embedding destekli\n",
      "    ‚îî‚îÄ Boyut: 1024, Sparse: ‚úÖ(NONE), Max Token: 8192\n",
      "  multilingual-e5: E5 Multilingual Large - √áok dilli dense embedding\n",
      "    ‚îî‚îÄ Boyut: 1024, Sparse: ‚ùå, Max Token: 512\n",
      "\n",
      "üáπüá∑ T√ºrk√ße √ñzel:\n",
      "  turkish-bert: Turkish BERT - T√ºrk√ße √∂zelle≈ütirilmi≈ü\n",
      "    ‚îî‚îÄ Boyut: 768, Sparse: ‚ùå, Max Token: 512\n",
      "  distilbert-turkish: Hƒ±zlƒ± T√ºrk√ße DistilBERT\n",
      "    ‚îî‚îÄ Boyut: 768, Sparse: ‚úÖ(TFIDF), Max Token: 512\n",
      "\n",
      "‚ö° Hƒ±zlƒ± & Genel:\n",
      "  bge-large: BGE Large - Sadece dense embedding\n",
      "    ‚îî‚îÄ Boyut: 1024, Sparse: ‚ùå, Max Token: 512\n",
      "  all-mpnet: All-MiniLM - Genel ama√ßlƒ±, hƒ±zlƒ±\n",
      "    ‚îî‚îÄ Boyut: 768, Sparse: ‚ùå, Max Token: 384\n",
      "\n",
      "üí° √ñneri:\n",
      "  ‚Ä¢ T√ºrk√ße aƒüƒ±rlƒ±klƒ±: turkish-bert (TF-IDF sparse)\n",
      "  ‚Ä¢ En iyi performans: bge-m3 (Native sparse)\n",
      "  ‚Ä¢ Hƒ±zlƒ± T√ºrk√ße: distilbert-turkish (TF-IDF sparse)\n",
      "  ‚Ä¢ √áok dilli: multilingual-e5 (TF-IDF sparse)\n",
      "\n",
      "üîç Sparse Embedding T√ºrleri:\n",
      "  ‚Ä¢ NATIVE: Model'in kendi sparse sistemi (sadece BGE-M3)\n",
      "  ‚Ä¢ TFIDF: TF-IDF tabanlƒ± sparse embedding (t√ºm diƒüer modeller)\n",
      "bge-m3: BGE-M3 - √áok dilli, dense+sparse embedding destekli\n",
      "  ‚îî‚îÄ Boyut: 1024, Sparse: True\n",
      "bge-large: BGE Large - Sadece dense embedding\n",
      "  ‚îî‚îÄ Boyut: 1024, Sparse: False\n",
      "multilingual-e5: E5 Multilingual Large - √áok dilli dense embedding\n",
      "  ‚îî‚îÄ Boyut: 1024, Sparse: False\n",
      "turkish-bert: Turkish BERT - T√ºrk√ße √∂zelle≈ütirilmi≈ü\n",
      "  ‚îî‚îÄ Boyut: 768, Sparse: False\n",
      "distilbert-turkish: Hƒ±zlƒ± T√ºrk√ße DistilBERT\n",
      "  ‚îî‚îÄ Boyut: 768, Sparse: True\n",
      "all-mpnet: All-MiniLM - Genel ama√ßlƒ±, hƒ±zlƒ±\n",
      "  ‚îî‚îÄ Boyut: 768, Sparse: False\n",
      "\n",
      "Mevcut modeller: bge-m3, bge-large, multilingual-e5, turkish-bert, distilbert-turkish, all-mpnet\n",
      "‚úÖ Se√ßilen model: dbmdz/distilbert-base-turkish-cased\n",
      "üîÆ Model y√ºkleniyor: dbmdz/distilbert-base-turkish-cased (tip: sentence_transformer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name dbmdz/distilbert-base-turkish-cased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model y√ºklendi: dbmdz/distilbert-base-turkish-cased\n",
      "‚úÖ Hazƒ±r - Model: dbmdz/distilbert-base-turkish-cased | Cihaz: NVIDIA RTX A6000\n",
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY SEMANTƒ∞K Sƒ∞STEM - Model: dbmdz/distilbert-base-turkish-cased\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) Model deƒüi≈ütir\n",
      "5) √áƒ±kƒ±≈ü\n",
      "üöÄ Full pipeline ba≈ülƒ±yor\n",
      "‚úÖ Dense embedding boyutu: 512\n",
      "üîç Sparse embedding: 7 terim (TFIDF)\n",
      "‚ùå Model baƒülantƒ± hatasƒ±: 'ModelManager' object has no attribute 'fitted_tfidf'\n",
      "‚ùå Hata √ßƒ±ktƒ±\n",
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY SEMANTƒ∞K Sƒ∞STEM - Model: dbmdz/distilbert-base-turkish-cased\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) Model deƒüi≈ütir\n",
      "5) √áƒ±kƒ±≈ü\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# Dynamic Model Selection + SemChunk + Qdrant Entegrasyon\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Model yapƒ±landƒ±rmalarƒ±\n",
    "MODEL_CONFIGS = {\n",
    "    \"bge-m3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"model_type\": \"bge\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 8192,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE-M3 - √áok dilli, dense+sparse embedding destekli\"\n",
    "    },\n",
    "    \"bge-large\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE Large - Sadece dense embedding\"\n",
    "    },\n",
    "    \"multilingual-e5\": {\n",
    "        \"model_name\": \"intfloat/multilingual-e5-large\",\n",
    "        \"model_type\": \"sentence_transformer\", \n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"E5 Multilingual Large - √áok dilli dense embedding\"\n",
    "    },\n",
    "    \"turkish-bert\": {\n",
    "        \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"Turkish BERT - T√ºrk√ße √∂zelle≈ütirilmi≈ü\"\n",
    "    },\n",
    "    \"distilbert-turkish\": {   # üëà EKLENMELƒ∞\n",
    "        \"model_name\": \"dbmdz/distilbert-base-turkish-cased\",\n",
    "        \"model_type\":\"sentence_transformer\",\n",
    "        \"description\": \"Hƒ±zlƒ± T√ºrk√ße DistilBERT\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"sparse_type\": \"tfidf\"\n",
    "    },\n",
    "    \"all-mpnet\": {\n",
    "        \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 384,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"All-MiniLM - Genel ama√ßlƒ±, hƒ±zlƒ±\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    MODEL_TYPE: str = \"bge\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"dynamic_model_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "    SUPPORTS_SPARSE: bool = True\n",
    "    SUPPORTS_DENSE: bool = True\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Dinamik model y√∂netimi i√ßin sƒ±nƒ±f\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, config: Config):\n",
    "        self.model_key = model_key\n",
    "        self.config = config\n",
    "        self.model_config = MODEL_CONFIGS.get(model_key)\n",
    "        \n",
    "        if not self.model_config:\n",
    "            raise ValueError(f\"Desteklenmeyen model: {model_key}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.vectorizer = None  # Sparse embedding i√ßin\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Se√ßilen modeli y√ºkle\"\"\"\n",
    "        try:\n",
    "            model_name = self.model_config[\"model_name\"]\n",
    "            model_type = self.model_config[\"model_type\"]\n",
    "            \n",
    "            print(f\"üîÆ Model y√ºkleniyor: {model_name} (tip: {model_type})\")\n",
    "            \n",
    "            if model_type == \"bge\":\n",
    "                self.model = BGEM3FlagModel(\n",
    "                    model_name, \n",
    "                    use_fp16=self.config.USE_FP16, \n",
    "                    device=self.config.DEVICE\n",
    "                )\n",
    "            elif model_type == \"sentence_transformer\":\n",
    "                self.model = SentenceTransformer(model_name, device=self.config.DEVICE)\n",
    "                # Sparse embedding i√ßin TF-IDF hazƒ±rla\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "            else:\n",
    "                raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "                \n",
    "            print(f\"‚úÖ Model y√ºklendi: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model y√ºkleme hatasƒ±: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Metinleri encode et - model tipine g√∂re\"\"\"\n",
    "        dense_embeddings = []\n",
    "        sparse_embeddings = []\n",
    "        \n",
    "        try:\n",
    "            if self.model_config[\"model_type\"] == \"bge\" and hasattr(self.model, 'encode'):\n",
    "                # BGE-M3 i√ßin\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    result = self.model.encode(\n",
    "                        texts, \n",
    "                        return_dense=True, \n",
    "                        return_sparse=True\n",
    "                    )\n",
    "                    dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "                    sparse_raw = result.get(\"sparse_vecs\", [])\n",
    "                    sparse_embeddings = [\n",
    "                        {\"indices\": s.get(\"indices\", []), \"values\": s.get(\"values\", [])} \n",
    "                        for s in sparse_raw\n",
    "                    ] if sparse_raw else [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                else:\n",
    "                    dense_embeddings = self.model.encode(texts, return_dense=True)\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                    \n",
    "            else:\n",
    "                # Sentence Transformer i√ßin\n",
    "                dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "                \n",
    "                # Sparse embedding TF-IDF ile\n",
    "                if self.vectorizer:\n",
    "                    X_sparse = self.vectorizer.fit_transform(texts)\n",
    "                    sparse_embeddings = []\n",
    "                    for i in range(X_sparse.shape[0]):\n",
    "                        row = X_sparse[i].tocoo()\n",
    "                        sparse_embeddings.append({\n",
    "                            \"indices\": row.col.tolist(),\n",
    "                            \"values\": row.data.tolist()\n",
    "                        })\n",
    "                else:\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            \n",
    "            # Embedding boyutunu ayarla\n",
    "            target_dim = self.config.EMBEDDING_DIM\n",
    "            dense_clean = []\n",
    "            for vec in dense_embeddings:\n",
    "                if vec is None:\n",
    "                    dense_clean.append([0.0] * target_dim)\n",
    "                elif len(vec) < target_dim:\n",
    "                    dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "                else:\n",
    "                    dense_clean.append(vec[:target_dim])\n",
    "            \n",
    "            return dense_clean, sparse_embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Encoding hatasƒ±: {e}\")\n",
    "            # Fallback\n",
    "            return (\n",
    "                [[0.0] * self.config.EMBEDDING_DIM for _ in texts],\n",
    "                [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            )\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Model bilgilerini d√∂nd√ºr\"\"\"\n",
    "        info = self.model_config.copy()\n",
    "        info[\"loaded\"] = self.model is not None\n",
    "        info[\"current_embedding_dim\"] = self.config.EMBEDDING_DIM\n",
    "        return info\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.config = config\n",
    "        self.model_manager = ModelManager(model_key, config)\n",
    "        \n",
    "        # Model bilgilerini config'e aktar\n",
    "        model_config = self.model_manager.model_config\n",
    "        self.config.BGE_MODEL_NAME = model_config[\"model_name\"]\n",
    "        self.config.MODEL_TYPE = model_config[\"model_type\"]\n",
    "        self.config.SUPPORTS_SPARSE = model_config[\"supports_sparse\"]\n",
    "        self.config.SUPPORTS_DENSE = model_config[\"supports_dense\"]\n",
    "        \n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Model y√ºkle\n",
    "        if not self.model_manager.load_model():\n",
    "            raise RuntimeError(\"Model y√ºklenemedi!\")\n",
    "        \n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Model: {model_config['model_name']} | Cihaz: {device_name}\")\n",
    "\n",
    "    def test_model_connection(self):\n",
    "        \"\"\"Model baƒülantƒ±sƒ±nƒ± test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts(test_text) # fit_tfidf=True)\n",
    "            \n",
    "            sparse_method = self.model_manager.model_config.get(\"sparse_type\", \"none\")\n",
    "            sparse_count = len(sparse_emb[0]['indices']) if sparse_emb[0]['indices'] else 0\n",
    "            \n",
    "            print(f\"‚úÖ Dense embedding boyutu: {len(dense_emb[0])}\")\n",
    "            print(f\"üîç Sparse embedding: {sparse_count} terim ({sparse_method.upper()})\")\n",
    "            \n",
    "            # if self.model_manager.fitted_tfidf:\n",
    "            #     vocab_size = len(self.model_manager.tfidf_vectorizer.vocabulary_)\n",
    "            #     print(f\"üìö TF-IDF vocabulary: {vocab_size:,} terim\")\n",
    "            \n",
    "            return len(dense_emb[0])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense vector config\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM, \n",
    "                        distance=models.Distance.COSINE\n",
    "                    ),\n",
    "                }\n",
    "                \n",
    "                # Sparse config (eƒüer destekleniyorsa)\n",
    "                sparse_config = {}\n",
    "                if self.config.SUPPORTS_SPARSE:\n",
    "                    sparse_config = {\n",
    "                        \"sparse_vec\": models.SparseVectorParams(\n",
    "                            index=models.SparseIndexParams(on_disk=False)\n",
    "                        )\n",
    "                    }\n",
    "                \n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config=sparse_config if sparse_config else None\n",
    "                )\n",
    "                \n",
    "                support_info = \"Dense+Sparse\" if sparse_config else \"Dense only\"\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} ({support_info})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding olu≈ütur\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (model: {self.config.BGE_MODEL_NAME})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "                'model_used': self.config.BGE_MODEL_NAME  # Hangi model kullanƒ±ldƒ±ƒüƒ±nƒ± kaydet\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings(texts)\n",
    "\n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vector_dict = {\"dense_vec\": d}\n",
    "            \n",
    "            # Sparse vector sadece destekleniyorsa ekle\n",
    "            if self.config.SUPPORTS_SPARSE and s[\"indices\"]:\n",
    "                vector_dict[\"sparse_vec\"] = SparseVector(\n",
    "                    indices=s[\"indices\"],\n",
    "                    values=s[\"values\"]\n",
    "                )\n",
    "            \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector_dict,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "        try:\n",
    "            dense_emb, _ = self.model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0]\n",
    "            \n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=NamedVector(name=\"dense_vec\", vector=query_vector),\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Semantic search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Hybrid search (sadece destekleniyorsa)\"\"\"\n",
    "        if not self.config.SUPPORTS_SPARSE:\n",
    "            print(\"‚ö†Ô∏è Bu model sparse embedding desteklemiyor, dense search yapƒ±lƒ±yor...\")\n",
    "            return self.search_semantic(query, limit, score_threshold)\n",
    "        \n",
    "        try:\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts([query])\n",
    "            \n",
    "            requests = [\n",
    "                SearchRequest(\n",
    "                    vector=NamedVector(name=\"dense_vec\", vector=dense_emb[0]),\n",
    "                    limit=limit,\n",
    "                    with_payload=True,\n",
    "                    score_threshold=score_threshold\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Sparse varsa ekle\n",
    "            if sparse_emb[0][\"indices\"]:\n",
    "                requests.append(\n",
    "                    SearchRequest(\n",
    "                        vector=NamedSparseVector(\n",
    "                            name=\"sparse_vec\",\n",
    "                            vector=SparseVector(\n",
    "                                indices=sparse_emb[0][\"indices\"],\n",
    "                                values=sparse_emb[0][\"values\"]\n",
    "                            )\n",
    "                        ),\n",
    "                        limit=limit,\n",
    "                        score_threshold=score_threshold\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            results = []\n",
    "            for request_result in qr:\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Hybrid)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Hybrid search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            model_info = self.model_manager.get_model_info()\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"model_info\": model_info,\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM,\n",
    "                \"supports_sparse\": self.config.SUPPORTS_SPARSE\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.processor = YargitaySemanticProcessor(config, model_key)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_model_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        model_info = self.processor.model_manager.get_model_info()\n",
    "        print(f\"üì± Aktif model: {model_info['model_name']}\")\n",
    "        print(f\"üîß √ñzellikler: Dense‚úÖ, Sparse{'‚úÖ' if model_info['supports_sparse'] else '‚ùå'}\")\n",
    "        \n",
    "        while True:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"üîç ARAMA SE√áENEKLERƒ∞\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(\"1) Dense arama (Semantic)\")\n",
    "            if model_info['supports_sparse']:\n",
    "                print(\"2) Hybrid arama (Dense + Sparse)\")\n",
    "                print(\"3) Kar≈üƒ±la≈ütƒ±rmalƒ± arama (Her iki y√∂ntem)\")\n",
    "            else:\n",
    "                print(\"2) ‚ùå Hybrid arama (Bu model desteklemiyor)\")\n",
    "                print(\"3) ‚ùå Kar≈üƒ±la≈ütƒ±rma (Sparse desteklenmiyor)\")\n",
    "            print(\"4) Ana men√º\")\n",
    "            \n",
    "            ch = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "            if ch==\"4\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\",\"3\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "                \n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                print(\"\\nüéØ Dense Semantic Search...\")\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(results, \"Dense\")\n",
    "                \n",
    "            elif ch==\"2\" and model_info['supports_sparse']:\n",
    "                print(\"\\nüîÄ Hybrid Search...\")\n",
    "                results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(results, \"Hybrid\")\n",
    "                \n",
    "            elif ch==\"3\" and model_info['supports_sparse']:\n",
    "                print(\"\\nüìä Kar≈üƒ±la≈ütƒ±rmalƒ± Arama...\")\n",
    "                print(\"üéØ Dense sonu√ßlar:\")\n",
    "                dense_results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(dense_results, \"Dense\", show_comparison=True)\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"üîÄ Hybrid sonu√ßlar:\")\n",
    "                hybrid_results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(hybrid_results, \"Hybrid\", show_comparison=True)\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Bu √∂zellik se√ßilen model tarafƒ±ndan desteklenmiyor.\")\n",
    "\n",
    "    def _display_results(self, results: List[Dict], search_type: str, show_comparison: bool = False):\n",
    "        \"\"\"Sonu√ßlarƒ± g√∂r√ºnt√ºle\"\"\"\n",
    "        if not results:\n",
    "            print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nüìã {len(results)} {search_type} sonu√ß:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            p = r.get(\"payload\") or {}\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            \n",
    "            # Skor rengine g√∂re emoji\n",
    "            if score > 0.8:\n",
    "                score_icon = \"üü¢\"\n",
    "            elif score > 0.6:\n",
    "                score_icon = \"üü°\"\n",
    "            else:\n",
    "                score_icon = \"üî¥\"\n",
    "                \n",
    "            print(f\"\\n{i}. {score_icon} Skor: {score:.4f}\")\n",
    "            print(f\"   üìÑ Model: {p.get('model_used','N/A')}\")\n",
    "            print(f\"   üèõÔ∏è Daire: {p.get('daire','N/A')} | üìÖ Tarih: {p.get('tarih','N/A')}\")\n",
    "            print(f\"   üìã Esas: {p.get('esas_no','N/A')} | üî¢ Karar: {p.get('karar_no','N/A')}\")\n",
    "            \n",
    "            text = p.get('text', '')\n",
    "            if len(text) > 200:\n",
    "                text_preview = text[:200] + \"...\"\n",
    "            else:\n",
    "                text_preview = text\n",
    "            print(f\"   üìù Metin: {text_preview}\")\n",
    "            \n",
    "            if show_comparison:\n",
    "                print(f\"   üè∑Ô∏è Tip: {search_type}\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "def select_model() -> str:\n",
    "    \"\"\"Kullanƒ±cƒ±dan model se√ßimi al\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ MODEL SE√áƒ∞Mƒ∞\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Modelleri kategorilere ayƒ±r\n",
    "    categories = {\n",
    "        \"üåç √áok Dilli Modeller\": [\"bge-m3\", \"multilingual-e5\"],\n",
    "        \"üáπüá∑ T√ºrk√ße √ñzel\": [\"turkish-bert\", \"distilbert-turkish\"],\n",
    "        \"‚ö° Hƒ±zlƒ± & Genel\": [\"bge-large\", \"all-mpnet\"]\n",
    "    }\n",
    "    \n",
    "    for category, models in categories.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for model_key in models:\n",
    "            config = MODEL_CONFIGS[model_key]\n",
    "            sparse_type = config.get('sparse_type', 'none')\n",
    "            sparse_icon = f\"‚úÖ({sparse_type.upper()})\" if config['supports_sparse'] else \"‚ùå\"\n",
    "            print(f\"  {model_key}: {config['description']}\")\n",
    "            print(f\"    ‚îî‚îÄ Boyut: {config['embedding_dim']}, Sparse: {sparse_icon}, Max Token: {config['max_seq_length']}\")\n",
    "    \n",
    "    print(f\"\\nüí° √ñneri:\")\n",
    "    print(\"  ‚Ä¢ T√ºrk√ße aƒüƒ±rlƒ±klƒ±: turkish-bert (TF-IDF sparse)\")\n",
    "    print(\"  ‚Ä¢ En iyi performans: bge-m3 (Native sparse)\")\n",
    "    print(\"  ‚Ä¢ Hƒ±zlƒ± T√ºrk√ße: distilbert-turkish (TF-IDF sparse)\")\n",
    "    print(\"  ‚Ä¢ √áok dilli: multilingual-e5 (TF-IDF sparse)\")\n",
    "    \n",
    "    print(f\"\\nüîç Sparse Embedding T√ºrleri:\")\n",
    "    print(\"  ‚Ä¢ NATIVE: Model'in kendi sparse sistemi (sadece BGE-M3)\")\n",
    "    print(\"  ‚Ä¢ TFIDF: TF-IDF tabanlƒ± sparse embedding (t√ºm diƒüer modeller)\")\n",
    "    \n",
    "    # while True:\n",
    "    #     choice = input(\"\\nModel se√ßin (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "    #     if choice in MODEL_CONFIGS:\n",
    "    #         selected_config = MODEL_CONFIGS[choice]\n",
    "    #         sparse_method = selected_config.get('sparse_type', 'none')\n",
    "    #         print(f\"‚úÖ Se√ßilen model: {selected_config['model_name']}\")\n",
    "    #         print(f\"üìä Sparse method: {sparse_method.upper()}\")\n",
    "    #         return choice\n",
    "    #     print(\"‚ùå Ge√ßersiz model! Mevcut:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    \n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"{key}: {config['description']}\")\n",
    "        print(f\"  ‚îî‚îÄ Boyut: {config['embedding_dim']}, Sparse: {config['supports_sparse']}\")\n",
    "\n",
    "    print(\"\\nMevcut modeller:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nModel se√ßin (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            print(f\"‚úÖ Se√ßilen model: {MODEL_CONFIGS[choice]['model_name']}\")\n",
    "            return choice\n",
    "        print(\"‚ùå Ge√ßersiz model! Tekrar deneyin.\")\n",
    "\n",
    "def main():\n",
    "    print(\"üèõÔ∏è YARGITAY Dƒ∞NAMƒ∞K MODEL Sƒ∞STEMƒ∞\")\n",
    "    \n",
    "    # Model se√ßimi\n",
    "    selected_model = select_model()\n",
    "    selected_config = MODEL_CONFIGS[selected_model]\n",
    "    \n",
    "    # Config olu≈ütur\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=f\"{selected_model}_chunks\",  # Model adƒ±na g√∂re koleksiyon\n",
    "        EMBEDDING_DIM=min(512, selected_config[\"embedding_dim\"]),  # Boyutu ayarla\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pipeline = YargitayPipeline(config, selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pipeline olu≈üturma hatasƒ±: {e}\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üèõÔ∏è YARGITAY SEMANTƒ∞K Sƒ∞STEM - Model: {selected_config['model_name']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) Model deƒüi≈ütir\")\n",
    "        print(\"5) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-5): \").strip()\n",
    "        \n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            # Model deƒüi≈ütir ve sistemi yeniden ba≈ülat\n",
    "            selected_model = select_model()\n",
    "            selected_config = MODEL_CONFIGS[selected_model]\n",
    "            config.COLLECTION_NAME = f\"{selected_model}_chunks\"\n",
    "            config.EMBEDDING_DIM = min(512, selected_config[\"embedding_dim\"])\n",
    "            try:\n",
    "                pipeline = YargitayPipeline(config, selected_model)\n",
    "                print(\"‚úÖ Model deƒüi≈ütirildi!\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Model deƒüi≈ütirme hatasƒ±: {e}\")\n",
    "        elif choice==\"5\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
