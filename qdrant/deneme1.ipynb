{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69283c71-5291-4569-af7c-f8ff5c761ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d969fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5b598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 214359.66it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hız için\n",
    "vec = model.encode([\"test\"], return_dense=True)[\"dense_vecs\"]\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c614124",
   "metadata": {},
   "source": [
    "### class langchain recursive+bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler için vector database sınıfı\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarı  \n",
    "            collection_name: Collection adı\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BGE-M3 modeli yükleniyor...\")\n",
    "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hız için\n",
    "        print(\"Model başarıyla yüklendi!\")\n",
    "\n",
    "        self.vector_size = self.model.encode(\n",
    "            [\"test\"], \n",
    "            return_dense=True, \n",
    "            return_sparse=False, \n",
    "            return_colbert_vecs=False\n",
    "        )[\"dense_vecs\"].shape[1]\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = True):\n",
    "        \"\"\"Collection oluştur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluşturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarını düzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'Ã¤': 'ä', 'Ã¶': 'ö', 'Ã¼': 'ü', 'ÃŸ': 'ß',\n",
    "            'Ã‡': 'Ç', 'Ä±': 'ı', 'Ä°': 'İ', 'ÅŸ': 'ş',\n",
    "            'Ä\\x9f': 'ğ', 'Ã§': 'ç', 'Ã¶': 'ö', 'Ã¼': 'ü'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasını işle\"\"\"\n",
    "        print(f\"CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlığını kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ HATA: Dosya bulunamadı: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"✅ UTF-8 encoding ile başarıyla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"✅ Latin-1 encoding ile başarıyla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satır sayısı: {len(df)}\")\n",
    "        print(f\"Sütunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Boş chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Boş metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e çevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluşturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)[\"dense_vecs\"]\n",
    "\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            #print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yükle\"\"\"\n",
    "        print(\"Qdrant'a yükleme başlıyor...\")\n",
    "        \n",
    "        # Embeddings oluştur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluştur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "            \"document_id\": str(row['_id']),\n",
    "            \"location\": str(row['location']),\n",
    "            \"dates\": str(row['extractedDates']),\n",
    "            \"esas_no\": str(row['esasNo']),\n",
    "            \"karar_no\": str(row['kararNo']),\n",
    "            \"esas_no_num\": str(row['esasNo_num']),\n",
    "            \"esas_no_tip\": str(row['esasNo_tip']),\n",
    "            \"karar_no_num\": str(row['kararNo_num']),\n",
    "            \"karar_no_tip\": str(row['kararNo_tip']),\n",
    "            \"chunk_id\": str(row['chunk_id']),\n",
    "            \"chunk_text\": str(row['chunk_text']),\n",
    "            \"token_count\": int(row['token_count']),\n",
    "            \"num_sentences\": int(row['num_sentences'])\n",
    "        }\n",
    "\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yükleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Yükleme hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} doküman başarıyla yüklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapılıyor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluştur\n",
    "        query_embedding = self.model.encode([query])[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Gelişmiş filtreleme ile arama\"\"\"\n",
    "        embedding_output = self.model.encode([query], convert_to_numpy=True)\n",
    "        query_embedding = embedding_output[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Filter oluştur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            # if 'daire' in filters:\n",
    "            #     conditions.append(models.FieldCondition(\n",
    "            #         key=\"daire\",\n",
    "            #         match=models.MatchValue(value=filters['daire'])\n",
    "            #     ))\n",
    "            \n",
    "            if 'kararNo_tip' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"kararNo_tip\", \n",
    "                    match=models.MatchValue(value=filters['kararNo_tip'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributeları kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e88b8",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349316d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfigürasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key  # Buraya gerçek API anahtarınızı yazın\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance oluştur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluştur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi işle ve yükle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini göster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Örnek aramalar\n",
    "    print(\"\\n=== Örnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold düşürüldü)\n",
    "    results = db.search(\"ihtiyati önlem tazminat\", limit=3, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati önlem tazminat' - {len(results)} sonuç\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        #print(f\"   Daire: {result['payload']['daire']}\")\n",
    "        # print(f\"   Esas No: {result['payload']['esasNo_num']}\")\n",
    "        # print(f\"   Karar No: {result['payload']['kararNo_num']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = { \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararı\", filters=filters, limit=2)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi + RED' - {len(results2)} sonuç\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        # print(f\"   Esas No: {result['payload']['esasNo_num']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanım için ayrı class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu başlıyor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandı!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKİ KARAR ARAMA SİSTEMİ\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediğiniz metni girin (çıkmak için 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Kaç sonuç gösterilsin? (varsayılan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"❌ Sonuç bulunamadı.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n🔍 '{query}' için {len(results)} sonuç bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\n📄 {i}. SONUÇ (Benzerlik: {result['score']:.3f})\")\n",
    "                    #print(f\"   📂 Daire: {payload['daire']}\")\n",
    "                    print(f\"   📋 Esas No: {payload['esasNo_num']}\")\n",
    "                    print(f\"   ⚖️ Karar No: {payload['kararNo_num']}\")\n",
    "                    # print(f\"   🏛️ Mahkeme: {payload['mahkeme']}\")\n",
    "                    print(f\"   📅 Tarihler: {payload['dates']}\")\n",
    "                    # print(f\"   📝 Chunk: {payload['chunk_index']}/{payload['total_chunks']}\")\n",
    "                    print(f\"   📄 Metin Önizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    \n",
    "    # Ana çalıştırma\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b7d37",
   "metadata": {},
   "source": [
    "### chonkie semantic+bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a117680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler için vector database sınıfı\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarı  \n",
    "            collection_name: Collection adı\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BGE-M3 modeli yükleniyor...\")\n",
    "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hız için\n",
    "        vec = self.model.encode([\"test\"], return_dense=True)[\"dense_vecs\"]\n",
    "        print(vec.shape)\n",
    "        print(\"Model başarıyla yüklendi!\")\n",
    "\n",
    "        self.vector_size = self.model.encode(\n",
    "            [\"test\"], \n",
    "            return_dense=True, \n",
    "            return_sparse=False, \n",
    "            return_colbert_vecs=False\n",
    "        )[\"dense_vecs\"].shape[1]\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = True):\n",
    "        \"\"\"Collection oluştur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    #distance=Distance.DOT\n",
    "                    #distance=Distance.EUCLID\n",
    "                    distance=Distance.COSINE\n",
    "                    \n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluşturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarını düzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'Ã¤': 'ä', 'Ã¶': 'ö', 'Ã¼': 'ü', 'ÃŸ': 'ß',\n",
    "            'Ã‡': 'Ç', 'Ä±': 'ı', 'Ä°': 'İ', 'ÅŸ': 'ş',\n",
    "            'Ä\\x9f': 'ğ', 'Ã§': 'ç', 'Ã¶': 'ö', 'Ã¼': 'ü'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasını işle\"\"\"\n",
    "        print(f\"CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlığını kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ HATA: Dosya bulunamadı: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"✅ UTF-8 encoding ile başarıyla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"✅ Latin-1 encoding ile başarıyla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satır sayısı: {len(df)}\")\n",
    "        print(f\"Sütunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        \n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Boş metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e çevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluşturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)[\"dense_vecs\"]\n",
    "\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            #print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yükle\"\"\"\n",
    "        print(\"Qdrant'a yükleme başlıyor...\")\n",
    "        \n",
    "        # Embeddings oluştur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluştur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \n",
    "                \"token_count\": int(row['token_count']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"num_sentences\": int(row['num_sentences'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yükleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Yükleme hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} doküman başarıyla yüklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapılıyor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluştur\n",
    "        query_embedding = self.model.encode([query])[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Gelişmiş filtreleme ile arama\"\"\"\n",
    "        embedding_output = self.model.encode([query], convert_to_numpy=True)\n",
    "        query_embedding = embedding_output[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Filter oluştur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'location' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"location\",\n",
    "                    match=models.MatchValue(value=filters['location'])\n",
    "                ))\n",
    "            \n",
    "            if 'kararNo_num' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"kararNo_num\", \n",
    "                    match=models.MatchValue(value=filters['kararNo_num'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributeları kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "        \n",
    "    from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "    def answer(self, query: str, limit: int = 5) -> str:\n",
    "        \"\"\"Soruya chunk'lardan faydalanarak cevap üret\"\"\"\n",
    "        # 1. Chunkları getir\n",
    "        results = self.search(query, limit=limit)\n",
    "\n",
    "        if not results:\n",
    "            return \"❌ Uygun bir sonuç bulunamadı.\"\n",
    "\n",
    "        # 2. Chunkları birleştir\n",
    "        context_chunks = \"\\n\\n\".join(\n",
    "            [f\"- {r['payload']['chunk_text']}\" for r in results]\n",
    "        )\n",
    "\n",
    "        # 3. LLM'e gönderilecek prompt\n",
    "        prompt = f\"\"\"\n",
    "        Aşağıda hukuki karar metinlerinden alınmış bölümler (chunk) verilmiştir. \n",
    "        Soruya bu metinler ışığında, mümkünse ilgili karar numaralarını belirterek \n",
    "        net ve öz bir yanıt ver.\n",
    "\n",
    "        ❓ Soru: {query}\n",
    "\n",
    "        📚 İlgili Metinler:\n",
    "        {context_chunks}\n",
    "\n",
    "        ✍️ Cevap:\n",
    "        \"\"\"\n",
    "\n",
    "        # 4. OpenAI istemcisi\n",
    "        client = OpenAI(api_key=\"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\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",   # hızlı ve ekonomik\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Sen deneyimli bir hukuk asistanısın. Yalnızca verilen metinlerden faydalanarak cevap ver.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2  # daha tutarlı cevaplar\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a03d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282411/3782857632.py:11: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.12.4. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 modeli yükleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 252162.57it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n",
      "Model başarıyla yüklendi!\n",
      "Vector boyutu: 1024\n",
      "\n",
      "==================================================\n",
      "HUKUKİ KARAR ARAMA SİSTEMİ\n",
      "==================================================\n",
      "\n",
      "🔎 LLM destekli cevap oluşturuluyor...\n",
      "Arama yapılıyor: 'tazminat davası var mı'\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Kullanım\u001b[39;00m\n\u001b[1;32m     69\u001b[0m searcher \u001b[38;5;241m=\u001b[39m InteractiveLegalSearch(\n\u001b[1;32m     70\u001b[0m     qdrant_url\u001b[38;5;241m=\u001b[39mqdrant_url,\n\u001b[1;32m     71\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m     72\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhukuki_kararlar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteractive_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m, in \u001b[0;36mInteractiveLegalSearch.interactive_search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# LLM destekli cevap\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔎 LLM destekli cevap oluşturuluyor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     cevap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📢 Cevap:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cevap)\n",
      "Cell \u001b[0;32mIn[6], line 305\u001b[0m, in \u001b[0;36mLegalDocumentVectorDB.answer\u001b[0;34m(self, query, limit)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# 4. OpenAI istemcisi\u001b[39;00m\n\u001b[1;32m    303\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-proj-OIWprDAGysGeOjas4-UgValoM7CXIuApTCBXjgNqgEjANFewQYkAxpcUG06hr1wJsuZZ8dyuwcT3BlbkFJeBx_bEsmm2iQf7LwpJJ-y5ys9buEL1WECTKwBm9E45f8haF1431GtgtufEgM8zeLLzwmPwTssA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# hızlı ve ekonomik\u001b[39;49;00m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSen deneyimli bir hukuk asistanısın. Yalnızca verilen metinlerden faydalanarak cevap ver.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# daha tutarlı cevaplar\u001b[39;49;00m\n\u001b[1;32m    312\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanım için ayrı class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key, collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu başlıyor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandı!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKİ KARAR ARAMA SİSTEMİ\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediğiniz metni girin (çıkmak için 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "\n",
    "                # Kullanıcıya seçenek sun\n",
    "                mode = input(\"1 = Chunk sonuçlarını göster, 2 = LLM ile cevap üret (varsayılan 1): \") or \"1\"\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Kaç sonuç gösterilsin? (varsayılan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                if mode == \"1\":\n",
    "                    # Normal chunk sonuçları\n",
    "                    results = self.db.search(query, limit=limit)\n",
    "                    \n",
    "                    if not results:\n",
    "                        print(\"❌ Sonuç bulunamadı.\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"\\n🔍 '{query}' için {len(results)} sonuç bulundu:\")\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "                    for i, result in enumerate(results, 1):\n",
    "                        payload = result['payload']\n",
    "                        print(f\"\\n📄 {i}. SONUÇ (Benzerlik: {result['score']:.3f})\")\n",
    "                        print(f\"   📋 Esas No: {payload['esas_no']}\")\n",
    "                        print(f\"   ⚖️ Karar No: {payload['karar_no']}\")\n",
    "                        print(f\"   📅 Tarihler: {payload['dates']}\")\n",
    "                        print(f\"   📝 Chunk: {payload['token_count']}\")\n",
    "                        print(f\"   📄 Metin Önizleme:\")\n",
    "                        print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                        print(\"-\" * 30)\n",
    "                \n",
    "                else:\n",
    "                    # LLM destekli cevap\n",
    "                    print(\"\\n🔎 LLM destekli cevap oluşturuluyor...\")\n",
    "                    cevap = self.db.answer(query, limit=limit)\n",
    "                    print(\"\\n📢 Cevap:\")\n",
    "                    print(cevap)\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "    # Kullanım\n",
    "    searcher = InteractiveLegalSearch(\n",
    "        qdrant_url=qdrant_url,\n",
    "        api_key=api_key,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    searcher.interactive_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2780843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282411/3782857632.py:11: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.12.4. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 modeli yükleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 24643.38it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n",
      "Model başarıyla yüklendi!\n",
      "Vector boyutu: 1024\n",
      "\n",
      "==================================================\n",
      "HUKUKİ KARAR ARAMA SİSTEMİ\n",
      "==================================================\n",
      "Arama yapılıyor: 'ihtiyati tedbir tazminatı hakkında davalar'\n",
      "\n",
      "🔍 'ihtiyati tedbir tazminatı hakkında davalar' için 5 sonuç bulundu:\n",
      "--------------------------------------------------\n",
      "\n",
      "📄 1. SONUÇ (Benzerlik: 0.670)\n",
      "   📋 Esas No: 2022/3281 E.\n",
      "   ⚖️ Karar No: 2024/117 K.\n",
      "   📅 Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   📝 Chunk: 271\n",
      "   📄 Metin Önizleme:\n",
      "      1. Asliye Hukuk Mahkemesinin 2009/139 D. ... dosyası üzerinden ihtiyati tedbir konularak inşaatın durdurulduğunu, HUMK'un 109. maddesi gereğince ihtiyati tedbir kararının verildiği tarihten itibaren 10 gün içerisinde esas hakkında davanın açılması gerektiğini ve bu durumun dosyaya ibrazı gerekirken ...\n",
      "------------------------------\n",
      "\n",
      "📄 2. SONUÇ (Benzerlik: 0.632)\n",
      "   📋 Esas No: 2022/3281 E.\n",
      "   ⚖️ Karar No: 2024/117 K.\n",
      "   📅 Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   📝 Chunk: 534\n",
      "   📄 Metin Önizleme:\n",
      "      ... sayılı dosyası üzerinden 25.12.2009 tarihinde inşaatın durdurulması yönünde ihtiyati tedbir kararı verildiği, teminatın 31.12.2009 tarihinde yatırıldığı, 04.01.2010 tarihinde mahkemece, İcra Müdürlüğüne ve Belediye Başkanlığına ihtiyati tedbir kararı gereğince işlem yapılması için müzekkereler y...\n",
      "------------------------------\n",
      "\n",
      "📄 3. SONUÇ (Benzerlik: 0.631)\n",
      "   📋 Esas No: 2022/3281 E.\n",
      "   ⚖️ Karar No: 2024/117 K.\n",
      "   📅 Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   📝 Chunk: 419\n",
      "   📄 Metin Önizleme:\n",
      "      2. İlgili Hukuk 6100 sayılı Hukuk Muhakemeleri Kanununun geçici 3 üncü maddesinin ikinci fıkrası atfıyla uygulanmasına devam olunan mülga 1086 sayılı Hukuk Usulü Muhakemeleri Kanununun 428 inci, 439 uncu maddesinin ikinci fıkrası ve 109. maddeleri, 6098 sayılı Türk Borçlar Kanununun 470 vd. maddeler...\n",
      "------------------------------\n",
      "\n",
      "📄 4. SONUÇ (Benzerlik: 0.603)\n",
      "   📋 Esas No: 2022/3281 E.\n",
      "   ⚖️ Karar No: 2024/117 K.\n",
      "   📅 Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   📝 Chunk: 262\n",
      "   📄 Metin Önizleme:\n",
      "      Bozma Kararı 1.Mahkemenin yukarıda belirtilen kararına karşı süresi içinde davacılar vekili temyiz isteminde bulunmuştur. 2.Yargıtay (Kapatılan) 23. Hukuk Dairesinin 04.06.2013 tarihli ve 2013/2984 Esas, 2013/3773 Karar sayılı kararıyla dava tarihinde yürürlükte olan 1086 sayılı HUMK'un 109. maddesi...\n",
      "------------------------------\n",
      "\n",
      "📄 5. SONUÇ (Benzerlik: 0.598)\n",
      "   📋 Esas No: 2023/596 E.\n",
      "   ⚖️ Karar No: 2024/257 K.\n",
      "   📅 Tarihler: 2024-01-18\n",
      "   📝 Chunk: 78\n",
      "   📄 Metin Önizleme:\n",
      "      6. Hukuk Dairesi 2023/596 E. , 2024/257 K. \\n \"İçtihat Metni\" MAHKEMESİ : ... Bölge Adliye Mahkemesi 13. Hukuk Dairesi Taraflar arasındaki rücuen tazminat davasından dolayı yapılan yargılama sonunda, İlk Derece Mahkemesince davanın kabulüne karar verilmiştir....\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "HUKUKİ KARAR ARAMA SİSTEMİ\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanım için ayrı class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name:str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu başlıyor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandı!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKİ KARAR ARAMA SİSTEMİ\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediğiniz metni girin (çıkmak için 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Kaç sonuç gösterilsin? (varsayılan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"❌ Sonuç bulunamadı.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n🔍 '{query}' için {len(results)} sonuç bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\n📄 {i}. SONUÇ (Benzerlik: {result['score']:.3f})\")\n",
    "                    \n",
    "                    print(f\"   📋 Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   ⚖️ Karar No: {payload['karar_no']}\")\n",
    "                \n",
    "                    print(f\"   📅 Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   📝 Chunk: {payload['token_count']}\")\n",
    "                    print(f\"   📄 Metin Önizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    searcher = InteractiveLegalSearch(\n",
    "    qdrant_url=qdrant_url,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    searcher.interactive_search()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116ee2a",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfigürasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key  # Buraya gerçek API anahtarınızı yazın\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance oluştur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluştur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi işle ve yükle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini göster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Örnek aramalar\n",
    "    print(\"\\n=== Örnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold düşürüldü)\n",
    "    results = db.search(\"ihtiyati önlem tazminat\", limit=10, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati önlem tazminat' - {len(results)} sonuç\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = {\"location\": \"6. Hukuk Dairesi\"}\n",
    "    # , \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararı\", filters=filters, limit=10)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi' - {len(results2)} sonuç\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "    # Önce ana fonksiyonu çalıştır (database setup)\n",
    "        main()\n",
    "\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name:str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu başlıyor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandı!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKİ KARAR ARAMA SİSTEMİ\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediğiniz metni girin (çıkmak için 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Kaç sonuç gösterilsin? (varsayılan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"❌ Sonuç bulunamadı.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n🔍 '{query}' için {len(results)} sonuç bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\n📄 {i}. SONUÇ (Benzerlik: {result['score']:.3f})\")\n",
    "                    \n",
    "                    print(f\"   📋 Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   ⚖️ Karar No: {payload['karar_no']}\")\n",
    "                \n",
    "                    print(f\"   📅 Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   📝 Chunk: {payload['token_count']}\")\n",
    "                    print(f\"   📄 Metin Önizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    searcher = InteractiveLegalSearch(\n",
    "    qdrant_url=qdrant_url,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    searcher.interactive_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817074e",
   "metadata": {},
   "source": [
    "#### berturk(facia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import os, re, uuid\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, FieldCondition, MatchValue, Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki_kararlar\"):\n",
    "        self.client = QdrantClient(url=qdrant_url, api_key=api_key, timeout=60)\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BERTurk modeli yükleniyor...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        self.model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        print(\"Model başarıyla yüklendi!\")\n",
    "        \n",
    "        # Embedding boyutu\n",
    "        self.vector_size = self.model.config.hidden_size\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'Ã¤':'ä', 'Ã¶':'ö', 'Ã¼':'ü', 'ÃŸ':'ß',\n",
    "            'Ã‡':'Ç', 'Ä±':'ı', 'Ä°':'İ', 'ÅŸ':'ş',\n",
    "            'Ä\\x9f':'ğ', 'Ã§':'ç'\n",
    "        }\n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        print(f\"CSV okunuyor: {csv_path}\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ Dosya bulunamadı: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "        print(f\"Sütunlar: {df.columns.tolist()} – Satır: {len(df)}\")\n",
    "        \n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        return df\n",
    "\n",
    "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        inputs = self.tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(\"Eski collection silindi.\")\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE)\n",
    "            )\n",
    "            print(\"Collection oluşturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(\"Collection zaten mevcut.\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.embed(texts)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload={\n",
    "                    \"chunk_id\": row['chunk_id'],\n",
    "                    \"chunk_text\": row['chunk_text_clean'],\n",
    "                    \"other\": row[['location','esasNo','kararNo']].to_dict()\n",
    "                }\n",
    "            ) for idx, (_, row) in enumerate(df.iterrows())\n",
    "        ]\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Yükleniyor\"):\n",
    "            self.client.upsert(collection_name=self.collection_name, points=points[i:i+batch_size])\n",
    "        print(f\"{len(points)} doküman yüklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        print(f\"Arama: {query}\")\n",
    "        query_vec = self.embed([query])[0]\n",
    "        search_results = self.client.search(collection_name=self.collection_name, query_vector=query_vec, limit=limit)\n",
    "        return [{\"score\": r.score, \"payload\": r.payload} for r in search_results]\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        info = self.client.get_collection(collection_name=self.collection_name)\n",
    "        return {\n",
    "            \"status\": str(info.status),\n",
    "            \"points_count\": getattr(info, 'points_count', None)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3be198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfigürasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"\n",
    "    \n",
    "    # Vector DB instance oluştur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluştur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi işle ve yükle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini göster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Örnek aramalar\n",
    "    print(\"\\n=== Örnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama\n",
    "    results = db.search(\"ihtiyati tedbir tazminat\", limit=5)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati edbir tazminat' - {len(results)} sonuç\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        payload = result['payload']\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {payload.get('esasNo')}\")\n",
    "        print(f\"   Karar No: {payload.get('kararNo')}\")\n",
    "        print(f\"   Metin: {payload['chunk_text'][:200]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5374fb",
   "metadata": {},
   "source": [
    "### chonkie semantic ve allminiLM12 deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler için vector database sınıfı\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarı  \n",
    "            collection_name: Collection adı\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Türkçe için optimize edilmiş multilingual model\n",
    "        print(\"Sentence Transformer modeli yükleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model başarıyla yüklendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = True):\n",
    "        \"\"\"Collection oluştur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluşturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarını düzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'Ã¤': 'ä', 'Ã¶': 'ö', 'Ã¼': 'ü', 'ÃŸ': 'ß',\n",
    "            'Ã‡': 'Ç', 'Ä±': 'ı', 'Ä°': 'İ', 'ÅŸ': 'ş',\n",
    "            'Ä\\x9f': 'ğ', 'Ã§': 'ç', 'Ã¶': 'ö', 'Ã¼': 'ü'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasını işle\"\"\"\n",
    "        print(f\"CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlığını kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ HATA: Dosya bulunamadı: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"✅ UTF-8 encoding ile başarıyla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"✅ Latin-1 encoding ile başarıyla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satır sayısı: {len(df)}\")\n",
    "        print(f\"Sütunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Boş chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Boş metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e çevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluşturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity için normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yükle\"\"\"\n",
    "        print(\"Qdrant'a yükleme başlıyor...\")\n",
    "        \n",
    "        # Embeddings oluştur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluştur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                #\"daire\": str(row['daire']),\n",
    "                #\"mahkeme\": str(row['mahkeme']),\n",
    "                \"kararNo_tip\": str(row['kararNo_tip']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                #\"chunk_index\": int(row['chunk_index']),\n",
    "                #\"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"token_count\": int(row['token_count'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yükleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Yükleme hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} doküman başarıyla yüklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapılıyor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluştur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Gelişmiş filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter oluştur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            # if 'daire' in filters:\n",
    "            #     conditions.append(models.FieldCondition(\n",
    "            #         key=\"daire\",\n",
    "            #         match=models.MatchValue(value=filters['daire'])\n",
    "            #     ))\n",
    "            \n",
    "            if 'kararNo_tip' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"kararNo_tip\", \n",
    "                    match=models.MatchValue(value=filters['kararNo_tip'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributeları kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fd870",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfigürasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key  \n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance oluştur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluştur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi işle ve yükle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini göster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Örnek aramalar\n",
    "    print(\"\\n=== Örnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold düşürüldü)\n",
    "    results = db.search(\"ihtiyati önlem tazminat\", limit=10, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati önlem tazminat' - {len(results)} sonuç\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = {\"location\": \"6. Hukuk Dairesi\"}\n",
    "    # , \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararı\", filters=filters, limit=10)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi' - {len(results2)} sonuç\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanım için ayrı class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name:str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu başlıyor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandı!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKİ KARAR ARAMA SİSTEMİ\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediğiniz metni girin (çıkmak için 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Kaç sonuç gösterilsin? (varsayılan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"❌ Sonuç bulunamadı.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n🔍 '{query}' için {len(results)} sonuç bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\n📄 {i}. SONUÇ (Benzerlik: {result['score']:.3f})\")\n",
    "                    \n",
    "                    print(f\"   📋 Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   ⚖️ Karar No: {payload['karar_no']}\")\n",
    "                \n",
    "                    print(f\"   📅 Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   📝 Chunk: {payload['token_count']}\")\n",
    "                    print(f\"   📄 Metin Önizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f18695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
