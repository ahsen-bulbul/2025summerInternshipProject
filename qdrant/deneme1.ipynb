{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69283c71-5291-4569-af7c-f8ff5c761ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from FlagEmbedding import BGEM3FlagModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c614124",
   "metadata": {},
   "source": [
    "### class langchain recursive+bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler iÃ§in vector database sÄ±nÄ±fÄ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarÄ±  \n",
    "            collection_name: Collection adÄ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BGE-M3 modeli yÃ¼kleniyor...\")\n",
    "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hÄ±z iÃ§in\n",
    "        print(\"Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "\n",
    "        self.vector_size = self.model.encode(\n",
    "            [\"test\"], \n",
    "            return_dense=True, \n",
    "            return_sparse=False, \n",
    "            return_colbert_vecs=False\n",
    "        )[\"dense_vecs\"].shape[1]\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluÅŸtur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluÅŸturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarÄ±nÄ± dÃ¼zelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'ÃƒÂ¤': 'Ã¤', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÅ¸': 'ÃŸ',\n",
    "            'Ãƒâ€¡': 'Ã‡', 'Ã„Â±': 'Ä±', 'Ã„Â°': 'Ä°', 'Ã…Å¸': 'ÅŸ',\n",
    "            'Ã„\\x9f': 'ÄŸ', 'ÃƒÂ§': 'Ã§', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasÄ±nÄ± iÅŸle\"\"\"\n",
    "        print(f\"CSV dosyasÄ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlÄ±ÄŸÄ±nÄ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âŒ HATA: Dosya bulunamadÄ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"âœ… UTF-8 encoding ile baÅŸarÄ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"âœ… Latin-1 encoding ile baÅŸarÄ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satÄ±r sayÄ±sÄ±: {len(df)}\")\n",
    "        print(f\"SÃ¼tunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # BoÅŸ chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"BoÅŸ metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e Ã§evir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluÅŸturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)[\"dense_vecs\"]\n",
    "\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            #print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yÃ¼kle\"\"\"\n",
    "        print(\"Qdrant'a yÃ¼kleme baÅŸlÄ±yor...\")\n",
    "        \n",
    "        # Embeddings oluÅŸtur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluÅŸtur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"daire\": str(row['daire']),\n",
    "                \"mahkeme\": str(row['mahkeme']),\n",
    "                \"karar_turu\": str(row['karar_turu']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \"chunk_index\": int(row['chunk_index']),\n",
    "                \"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"chunk_length\": int(row['chunk_length'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yÃ¼kle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yÃ¼kleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"YÃ¼kleme hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dokÃ¼man baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapÄ±lÄ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluÅŸtur\n",
    "        query_embedding = self.model.encode([query])[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"GeliÅŸmiÅŸ filtreleme ile arama\"\"\"\n",
    "        embedding_output = self.model.encode([query], convert_to_numpy=True)\n",
    "        query_embedding = embedding_output[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Filter oluÅŸtur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'daire' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"daire\",\n",
    "                    match=models.MatchValue(value=filters['daire'])\n",
    "                ))\n",
    "            \n",
    "            if 'karar_turu' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"karar_turu\", \n",
    "                    match=models.MatchValue(value=filters['karar_turu'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarÄ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e88b8",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349316d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # KonfigÃ¼rasyon\n",
    "    QDRANT_URL = \"https://qdrant.adalet.gov.tr:443\"\n",
    "    API_KEY = \"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\"  # Buraya gerÃ§ek API anahtarÄ±nÄ±zÄ± yazÄ±n\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance oluÅŸtur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluÅŸtur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi iÅŸle ve yÃ¼kle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini gÃ¶ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Ã–rnek aramalar\n",
    "    print(\"\\n=== Ã–rnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold dÃ¼ÅŸÃ¼rÃ¼ldÃ¼)\n",
    "    results = db.search(\"ihtiyati Ã¶nlem tazminat\", limit=3, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati Ã¶nlem tazminat' - {len(results)} sonuÃ§\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Daire: {result['payload']['daire']}\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = {\"daire\": \"6. Hukuk Dairesi\", \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararÄ±\", filters=filters, limit=2)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi + RED' - {len(results2)} sonuÃ§\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanÄ±m iÃ§in ayrÄ± class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu baÅŸlÄ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandÄ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKÄ° KARAR ARAMA SÄ°STEMÄ°\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediÄŸiniz metni girin (Ã§Ä±kmak iÃ§in 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"KaÃ§ sonuÃ§ gÃ¶sterilsin? (varsayÄ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"âŒ SonuÃ§ bulunamadÄ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nğŸ” '{query}' iÃ§in {len(results)} sonuÃ§ bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nğŸ“„ {i}. SONUÃ‡ (Benzerlik: {result['score']:.3f})\")\n",
    "                    print(f\"   ğŸ“‚ Daire: {payload['daire']}\")\n",
    "                    print(f\"   ğŸ“‹ Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   âš–ï¸ Karar No: {payload['karar_no']}\")\n",
    "                    print(f\"   ğŸ›ï¸ Mahkeme: {payload['mahkeme']}\")\n",
    "                    print(f\"   ğŸ“… Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   ğŸ“ Chunk: {payload['chunk_index']}/{payload['total_chunks']}\")\n",
    "                    print(f\"   ğŸ“„ Metin Ã–nizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    \n",
    "    # Ana Ã§alÄ±ÅŸtÄ±rma\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b7d37",
   "metadata": {},
   "source": [
    "### chonkie semantic+bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler iÃ§in vector database sÄ±nÄ±fÄ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarÄ±  \n",
    "            collection_name: Collection adÄ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BGE-M3 modeli yÃ¼kleniyor...\")\n",
    "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hÄ±z iÃ§in\n",
    "        print(\"Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "\n",
    "        self.vector_size = self.model.encode(\n",
    "            [\"test\"], \n",
    "            return_dense=True, \n",
    "            return_sparse=False, \n",
    "            return_colbert_vecs=False\n",
    "        )[\"dense_vecs\"].shape[1]\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluÅŸtur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluÅŸturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarÄ±nÄ± dÃ¼zelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'ÃƒÂ¤': 'Ã¤', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÅ¸': 'ÃŸ',\n",
    "            'Ãƒâ€¡': 'Ã‡', 'Ã„Â±': 'Ä±', 'Ã„Â°': 'Ä°', 'Ã…Å¸': 'ÅŸ',\n",
    "            'Ã„\\x9f': 'ÄŸ', 'ÃƒÂ§': 'Ã§', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasÄ±nÄ± iÅŸle\"\"\"\n",
    "        print(f\"CSV dosyasÄ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlÄ±ÄŸÄ±nÄ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âŒ HATA: Dosya bulunamadÄ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"âœ… UTF-8 encoding ile baÅŸarÄ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"âœ… Latin-1 encoding ile baÅŸarÄ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satÄ±r sayÄ±sÄ±: {len(df)}\")\n",
    "        print(f\"SÃ¼tunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # BoÅŸ chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"BoÅŸ metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e Ã§evir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluÅŸturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)[\"dense_vecs\"]\n",
    "\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            #print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yÃ¼kle\"\"\"\n",
    "        print(\"Qdrant'a yÃ¼kleme baÅŸlÄ±yor...\")\n",
    "        \n",
    "        # Embeddings oluÅŸtur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluÅŸtur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \n",
    "                \"token_count\": int(row['token_count']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"num_sentences\": int(row['num_sentences'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yÃ¼kle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yÃ¼kleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"YÃ¼kleme hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dokÃ¼man baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapÄ±lÄ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluÅŸtur\n",
    "        query_embedding = self.model.encode([query])[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"GeliÅŸmiÅŸ filtreleme ile arama\"\"\"\n",
    "        embedding_output = self.model.encode([query], convert_to_numpy=True)\n",
    "        query_embedding = embedding_output[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Filter oluÅŸtur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'location' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"location\",\n",
    "                    match=models.MatchValue(value=filters['location'])\n",
    "                ))\n",
    "            \n",
    "            if 'kararNo_num' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"kararNo_num\", \n",
    "                    match=models.MatchValue(value=filters['kararNo_num'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarÄ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116ee2a",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # KonfigÃ¼rasyon\n",
    "    QDRANT_URL = \"https://qdrant.adalet.gov.tr:443\"\n",
    "    API_KEY = \"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\"  # Buraya gerÃ§ek API anahtarÄ±nÄ±zÄ± yazÄ±n\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance oluÅŸtur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluÅŸtur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi iÅŸle ve yÃ¼kle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini gÃ¶ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Ã–rnek aramalar\n",
    "    print(\"\\n=== Ã–rnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold dÃ¼ÅŸÃ¼rÃ¼ldÃ¼)\n",
    "    results = db.search(\"ihtiyati Ã¶nlem tazminat\", limit=10, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati Ã¶nlem tazminat' - {len(results)} sonuÃ§\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = {\"location\": \"6. Hukuk Dairesi\"}\n",
    "    # , \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararÄ±\", filters=filters, limit=10)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi' - {len(results2)} sonuÃ§\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanÄ±m iÃ§in ayrÄ± class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name:str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu baÅŸlÄ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandÄ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKÄ° KARAR ARAMA SÄ°STEMÄ°\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediÄŸiniz metni girin (Ã§Ä±kmak iÃ§in 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"KaÃ§ sonuÃ§ gÃ¶sterilsin? (varsayÄ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"âŒ SonuÃ§ bulunamadÄ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nğŸ” '{query}' iÃ§in {len(results)} sonuÃ§ bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nğŸ“„ {i}. SONUÃ‡ (Benzerlik: {result['score']:.3f})\")\n",
    "                    \n",
    "                    print(f\"   ğŸ“‹ Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   âš–ï¸ Karar No: {payload['karar_no']}\")\n",
    "                \n",
    "                    print(f\"   ğŸ“… Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   ğŸ“ Chunk: {payload['token_count']}\")\n",
    "                    print(f\"   ğŸ“„ Metin Ã–nizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    searcher = InteractiveLegalSearch(\n",
    "    qdrant_url=\"https://qdrant.adalet.gov.tr:443\",\n",
    "    api_key=\"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\",\n",
    "    collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    searcher.interactive_search()\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817074e",
   "metadata": {},
   "source": [
    "#### berturk(facia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import os, re, uuid\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, FieldCondition, MatchValue, Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki_kararlar\"):\n",
    "        self.client = QdrantClient(url=qdrant_url, api_key=api_key, timeout=60)\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BERTurk modeli yÃ¼kleniyor...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        self.model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        print(\"Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "        \n",
    "        # Embedding boyutu\n",
    "        self.vector_size = self.model.config.hidden_size\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'ÃƒÂ¤':'Ã¤', 'ÃƒÂ¶':'Ã¶', 'ÃƒÂ¼':'Ã¼', 'ÃƒÅ¸':'ÃŸ',\n",
    "            'Ãƒâ€¡':'Ã‡', 'Ã„Â±':'Ä±', 'Ã„Â°':'Ä°', 'Ã…Å¸':'ÅŸ',\n",
    "            'Ã„\\x9f':'ÄŸ', 'ÃƒÂ§':'Ã§'\n",
    "        }\n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        print(f\"CSV okunuyor: {csv_path}\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âŒ Dosya bulunamadÄ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "        print(f\"SÃ¼tunlar: {df.columns.tolist()} â€“ SatÄ±r: {len(df)}\")\n",
    "        \n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        return df\n",
    "\n",
    "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        inputs = self.tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(\"Eski collection silindi.\")\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE)\n",
    "            )\n",
    "            print(\"Collection oluÅŸturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(\"Collection zaten mevcut.\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.embed(texts)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload={\n",
    "                    \"chunk_id\": row['chunk_id'],\n",
    "                    \"chunk_text\": row['chunk_text_clean'],\n",
    "                    \"other\": row[['location','esasNo','kararNo']].to_dict()\n",
    "                }\n",
    "            ) for idx, (_, row) in enumerate(df.iterrows())\n",
    "        ]\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"YÃ¼kleniyor\"):\n",
    "            self.client.upsert(collection_name=self.collection_name, points=points[i:i+batch_size])\n",
    "        print(f\"{len(points)} dokÃ¼man yÃ¼klendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        print(f\"Arama: {query}\")\n",
    "        query_vec = self.embed([query])[0]\n",
    "        search_results = self.client.search(collection_name=self.collection_name, query_vector=query_vec, limit=limit)\n",
    "        return [{\"score\": r.score, \"payload\": r.payload} for r in search_results]\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        info = self.client.get_collection(collection_name=self.collection_name)\n",
    "        return {\n",
    "            \"status\": str(info.status),\n",
    "            \"points_count\": getattr(info, 'points_count', None)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3be198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # KonfigÃ¼rasyon\n",
    "    QDRANT_URL = \"https://qdrant.adalet.gov.tr:443\"\n",
    "    API_KEY = \"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\"\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"\n",
    "    \n",
    "    # Vector DB instance oluÅŸtur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluÅŸtur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi iÅŸle ve yÃ¼kle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini gÃ¶ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Ã–rnek aramalar\n",
    "    print(\"\\n=== Ã–rnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama\n",
    "    results = db.search(\"ihtiyati tedbir tazminat\", limit=5)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati edbir tazminat' - {len(results)} sonuÃ§\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        payload = result['payload']\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {payload.get('esasNo')}\")\n",
    "        print(f\"   Karar No: {payload.get('kararNo')}\")\n",
    "        print(f\"   Metin: {payload['chunk_text'][:200]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5374fb",
   "metadata": {},
   "source": [
    "### chonkie semantic ve allminiLM12 deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d01190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler iÃ§in vector database sÄ±nÄ±fÄ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarÄ±  \n",
    "            collection_name: Collection adÄ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ multilingual model\n",
    "        print(\"Sentence Transformer modeli yÃ¼kleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluÅŸtur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluÅŸturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarÄ±nÄ± dÃ¼zelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'ÃƒÂ¤': 'Ã¤', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÅ¸': 'ÃŸ',\n",
    "            'Ãƒâ€¡': 'Ã‡', 'Ã„Â±': 'Ä±', 'Ã„Â°': 'Ä°', 'Ã…Å¸': 'ÅŸ',\n",
    "            'Ã„\\x9f': 'ÄŸ', 'ÃƒÂ§': 'Ã§', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasÄ±nÄ± iÅŸle\"\"\"\n",
    "        print(f\"CSV dosyasÄ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlÄ±ÄŸÄ±nÄ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âŒ HATA: Dosya bulunamadÄ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"âœ… UTF-8 encoding ile baÅŸarÄ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"âœ… Latin-1 encoding ile baÅŸarÄ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satÄ±r sayÄ±sÄ±: {len(df)}\")\n",
    "        print(f\"SÃ¼tunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # BoÅŸ chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"BoÅŸ metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e Ã§evir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluÅŸturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity iÃ§in normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yÃ¼kle\"\"\"\n",
    "        print(\"Qdrant'a yÃ¼kleme baÅŸlÄ±yor...\")\n",
    "        \n",
    "        # Embeddings oluÅŸtur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluÅŸtur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"daire\": str(row['daire']),\n",
    "                \"mahkeme\": str(row['mahkeme']),\n",
    "                \"karar_turu\": str(row['karar_turu']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \"chunk_index\": int(row['chunk_index']),\n",
    "                \"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"chunk_length\": int(row['chunk_length'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yÃ¼kle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yÃ¼kleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"YÃ¼kleme hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dokÃ¼man baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapÄ±lÄ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluÅŸtur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"GeliÅŸmiÅŸ filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter oluÅŸtur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'daire' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"daire\",\n",
    "                    match=models.MatchValue(value=filters['daire'])\n",
    "                ))\n",
    "            \n",
    "            if 'karar_turu' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"karar_turu\", \n",
    "                    match=models.MatchValue(value=filters['karar_turu'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarÄ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fd870",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # KonfigÃ¼rasyon\n",
    "    QDRANT_URL = \"https://qdrant.adalet.gov.tr:443\"\n",
    "    API_KEY = \"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\"  # Buraya gerÃ§ek API anahtarÄ±nÄ±zÄ± yazÄ±n\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance oluÅŸtur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection oluÅŸtur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi iÅŸle ve yÃ¼kle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini gÃ¶ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Ã–rnek aramalar\n",
    "    print(\"\\n=== Ã–rnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold dÃ¼ÅŸÃ¼rÃ¼ldÃ¼)\n",
    "    results = db.search(\"ihtiyati tedbir tazminat\", limit=3, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati tedbir tazminat' - {len(results)} sonuÃ§\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Daire: {result['payload']['daire']}\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = {\"daire\": \"6. Hukuk Dairesi\", \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararÄ±\", filters=filters, limit=2)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi + RED' - {len(results2)} sonuÃ§\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanÄ±m iÃ§in ayrÄ± class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu baÅŸlÄ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandÄ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKÄ° KARAR ARAMA SÄ°STEMÄ°\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediÄŸiniz metni girin (Ã§Ä±kmak iÃ§in 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"KaÃ§ sonuÃ§ gÃ¶sterilsin? (varsayÄ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"âŒ SonuÃ§ bulunamadÄ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nğŸ” '{query}' iÃ§in {len(results)} sonuÃ§ bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nğŸ“„ {i}. SONUÃ‡ (Benzerlik: {result['score']:.3f})\")\n",
    "                    print(f\"   ğŸ“‚ Daire: {payload['daire']}\")\n",
    "                    print(f\"   ğŸ“‹ Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   âš–ï¸ Karar No: {payload['karar_no']}\")\n",
    "                    print(f\"   ğŸ›ï¸ Mahkeme: {payload['mahkeme']}\")\n",
    "                    print(f\"   ğŸ“… Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   ğŸ“ Chunk: {payload['chunk_index']}/{payload['total_chunks']}\")\n",
    "                    print(f\"   ğŸ“„ Metin Ã–nizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    \n",
    "    # Ana Ã§alÄ±ÅŸtÄ±rma\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077bbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
