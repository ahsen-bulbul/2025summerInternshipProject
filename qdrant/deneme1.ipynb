{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69283c71-5291-4569-af7c-f8ff5c761ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d969fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5b598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 214359.66it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hƒ±z i√ßin\n",
    "vec = model.encode([\"test\"], return_dense=True)[\"dense_vecs\"]\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c614124",
   "metadata": {},
   "source": [
    "### class langchain recursive+bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler i√ßin vector database sƒ±nƒ±fƒ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarƒ±  \n",
    "            collection_name: Collection adƒ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BGE-M3 modeli y√ºkleniyor...\")\n",
    "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hƒ±z i√ßin\n",
    "        print(\"Model ba≈üarƒ±yla y√ºklendi!\")\n",
    "\n",
    "        self.vector_size = self.model.encode(\n",
    "            [\"test\"], \n",
    "            return_dense=True, \n",
    "            return_sparse=False, \n",
    "            return_colbert_vecs=False\n",
    "        )[\"dense_vecs\"].shape[1]\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = True):\n",
    "        \"\"\"Collection olu≈ütur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' olu≈üturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarƒ±nƒ± d√ºzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            '√É¬§': '√§', '√É¬∂': '√∂', '√É¬º': '√º', '√É≈∏': '√ü',\n",
    "            '√É‚Ä°': '√á', '√Ñ¬±': 'ƒ±', '√Ñ¬∞': 'ƒ∞', '√Ö≈∏': '≈ü',\n",
    "            '√Ñ\\x9f': 'ƒü', '√É¬ß': '√ß', '√É¬∂': '√∂', '√É¬º': '√º'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla bo≈üluklarƒ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasƒ±nƒ± i≈üle\"\"\"\n",
    "        print(f\"CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlƒ±ƒüƒ±nƒ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ùå HATA: Dosya bulunamadƒ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"‚úÖ UTF-8 encoding ile ba≈üarƒ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"‚úÖ Latin-1 encoding ile ba≈üarƒ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satƒ±r sayƒ±sƒ±: {len(df)}\")\n",
    "        print(f\"S√ºtunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Bo≈ü chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Bo≈ü metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e √ßevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding olu≈üturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)[\"dense_vecs\"]\n",
    "\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            #print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a y√ºkle\"\"\"\n",
    "        print(\"Qdrant'a y√ºkleme ba≈ülƒ±yor...\")\n",
    "        \n",
    "        # Embeddings olu≈ütur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points olu≈ütur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "            \"document_id\": str(row['_id']),\n",
    "            \"location\": str(row['location']),\n",
    "            \"dates\": str(row['extractedDates']),\n",
    "            \"esas_no\": str(row['esasNo']),\n",
    "            \"karar_no\": str(row['kararNo']),\n",
    "            \"esas_no_num\": str(row['esasNo_num']),\n",
    "            \"esas_no_tip\": str(row['esasNo_tip']),\n",
    "            \"karar_no_num\": str(row['kararNo_num']),\n",
    "            \"karar_no_tip\": str(row['kararNo_tip']),\n",
    "            \"chunk_id\": str(row['chunk_id']),\n",
    "            \"chunk_text\": str(row['chunk_text']),\n",
    "            \"token_count\": int(row['token_count']),\n",
    "            \"num_sentences\": int(row['num_sentences'])\n",
    "        }\n",
    "\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde y√ºkle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a y√ºkleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Y√ºkleme hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dok√ºman ba≈üarƒ±yla y√ºklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapƒ±lƒ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i olu≈ütur\n",
    "        query_embedding = self.model.encode([query])[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Geli≈ümi≈ü filtreleme ile arama\"\"\"\n",
    "        embedding_output = self.model.encode([query], convert_to_numpy=True)\n",
    "        query_embedding = embedding_output[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Filter olu≈ütur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            # if 'daire' in filters:\n",
    "            #     conditions.append(models.FieldCondition(\n",
    "            #         key=\"daire\",\n",
    "            #         match=models.MatchValue(value=filters['daire'])\n",
    "            #     ))\n",
    "            \n",
    "            if 'kararNo_tip' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"kararNo_tip\", \n",
    "                    match=models.MatchValue(value=filters['kararNo_tip'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarƒ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e88b8",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349316d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfig√ºrasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key  # Buraya ger√ßek API anahtarƒ±nƒ±zƒ± yazƒ±n\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance olu≈ütur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection olu≈ütur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi i≈üle ve y√ºkle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini g√∂ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # √ñrnek aramalar\n",
    "    print(\"\\n=== √ñrnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold d√º≈ü√ºr√ºld√º)\n",
    "    results = db.search(\"ihtiyati √∂nlem tazminat\", limit=3, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati √∂nlem tazminat' - {len(results)} sonu√ß\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonu√ß (Skor: {result['score']:.3f})\")\n",
    "        #print(f\"   Daire: {result['payload']['daire']}\")\n",
    "        # print(f\"   Esas No: {result['payload']['esasNo_num']}\")\n",
    "        # print(f\"   Karar No: {result['payload']['kararNo_num']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = { \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararƒ±\", filters=filters, limit=2)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi + RED' - {len(results2)} sonu√ß\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. Sonu√ß (Skor: {result['score']:.3f})\")\n",
    "        # print(f\"   Esas No: {result['payload']['esasNo_num']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanƒ±m i√ßin ayrƒ± class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu ba≈ülƒ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandƒ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediƒüiniz metni girin (√ßƒ±kmak i√ßin 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Ka√ß sonu√ß g√∂sterilsin? (varsayƒ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"‚ùå Sonu√ß bulunamadƒ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nüîç '{query}' i√ßin {len(results)} sonu√ß bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nüìÑ {i}. SONU√á (Benzerlik: {result['score']:.3f})\")\n",
    "                    #print(f\"   üìÇ Daire: {payload['daire']}\")\n",
    "                    print(f\"   üìã Esas No: {payload['esasNo_num']}\")\n",
    "                    print(f\"   ‚öñÔ∏è Karar No: {payload['kararNo_num']}\")\n",
    "                    # print(f\"   üèõÔ∏è Mahkeme: {payload['mahkeme']}\")\n",
    "                    print(f\"   üìÖ Tarihler: {payload['dates']}\")\n",
    "                    # print(f\"   üìù Chunk: {payload['chunk_index']}/{payload['total_chunks']}\")\n",
    "                    print(f\"   üìÑ Metin √ñnizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    \n",
    "    # Ana √ßalƒ±≈ütƒ±rma\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b7d37",
   "metadata": {},
   "source": [
    "### chonkie semantic+bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a117680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler i√ßin vector database sƒ±nƒ±fƒ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarƒ±  \n",
    "            collection_name: Collection adƒ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BGE-M3 modeli y√ºkleniyor...\")\n",
    "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # fp16 hƒ±z i√ßin\n",
    "        vec = self.model.encode([\"test\"], return_dense=True)[\"dense_vecs\"]\n",
    "        print(vec.shape)\n",
    "        print(\"Model ba≈üarƒ±yla y√ºklendi!\")\n",
    "\n",
    "        self.vector_size = self.model.encode(\n",
    "            [\"test\"], \n",
    "            return_dense=True, \n",
    "            return_sparse=False, \n",
    "            return_colbert_vecs=False\n",
    "        )[\"dense_vecs\"].shape[1]\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = True):\n",
    "        \"\"\"Collection olu≈ütur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    #distance=Distance.DOT\n",
    "                    #distance=Distance.EUCLID\n",
    "                    distance=Distance.COSINE\n",
    "                    \n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' olu≈üturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarƒ±nƒ± d√ºzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            '√É¬§': '√§', '√É¬∂': '√∂', '√É¬º': '√º', '√É≈∏': '√ü',\n",
    "            '√É‚Ä°': '√á', '√Ñ¬±': 'ƒ±', '√Ñ¬∞': 'ƒ∞', '√Ö≈∏': '≈ü',\n",
    "            '√Ñ\\x9f': 'ƒü', '√É¬ß': '√ß', '√É¬∂': '√∂', '√É¬º': '√º'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla bo≈üluklarƒ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasƒ±nƒ± i≈üle\"\"\"\n",
    "        print(f\"CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlƒ±ƒüƒ±nƒ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ùå HATA: Dosya bulunamadƒ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"‚úÖ UTF-8 encoding ile ba≈üarƒ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"‚úÖ Latin-1 encoding ile ba≈üarƒ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satƒ±r sayƒ±sƒ±: {len(df)}\")\n",
    "        print(f\"S√ºtunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        \n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Bo≈ü metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e √ßevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding olu≈üturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch)[\"dense_vecs\"]\n",
    "\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            #print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a y√ºkle\"\"\"\n",
    "        print(\"Qdrant'a y√ºkleme ba≈ülƒ±yor...\")\n",
    "        \n",
    "        # Embeddings olu≈ütur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points olu≈ütur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \n",
    "                \"token_count\": int(row['token_count']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"num_sentences\": int(row['num_sentences'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde y√ºkle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a y√ºkleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Y√ºkleme hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dok√ºman ba≈üarƒ±yla y√ºklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapƒ±lƒ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i olu≈ütur\n",
    "        query_embedding = self.model.encode([query])[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Geli≈ümi≈ü filtreleme ile arama\"\"\"\n",
    "        embedding_output = self.model.encode([query], convert_to_numpy=True)\n",
    "        query_embedding = embedding_output[\"dense_vecs\"][0].tolist()\n",
    "        \n",
    "        # Filter olu≈ütur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'location' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"location\",\n",
    "                    match=models.MatchValue(value=filters['location'])\n",
    "                ))\n",
    "            \n",
    "            if 'kararNo_num' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"kararNo_num\", \n",
    "                    match=models.MatchValue(value=filters['kararNo_num'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarƒ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "        \n",
    "    from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "    def answer(self, query: str, limit: int = 5) -> str:\n",
    "        \"\"\"Soruya chunk'lardan faydalanarak cevap √ºret\"\"\"\n",
    "        # 1. Chunklarƒ± getir\n",
    "        results = self.search(query, limit=limit)\n",
    "\n",
    "        if not results:\n",
    "            return \"‚ùå Uygun bir sonu√ß bulunamadƒ±.\"\n",
    "\n",
    "        # 2. Chunklarƒ± birle≈ütir\n",
    "        context_chunks = \"\\n\\n\".join(\n",
    "            [f\"- {r['payload']['chunk_text']}\" for r in results]\n",
    "        )\n",
    "\n",
    "        # 3. LLM'e g√∂nderilecek prompt\n",
    "        prompt = f\"\"\"\n",
    "        A≈üaƒüƒ±da hukuki karar metinlerinden alƒ±nmƒ±≈ü b√∂l√ºmler (chunk) verilmi≈ütir. \n",
    "        Soruya bu metinler ƒ±≈üƒ±ƒüƒ±nda, m√ºmk√ºnse ilgili karar numaralarƒ±nƒ± belirterek \n",
    "        net ve √∂z bir yanƒ±t ver.\n",
    "\n",
    "        ‚ùì Soru: {query}\n",
    "\n",
    "        üìö ƒ∞lgili Metinler:\n",
    "        {context_chunks}\n",
    "\n",
    "        ‚úçÔ∏è Cevap:\n",
    "        \"\"\"\n",
    "\n",
    "        # 4. OpenAI istemcisi\n",
    "        client = OpenAI(api_key=\"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\")\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",   # hƒ±zlƒ± ve ekonomik\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Sen deneyimli bir hukuk asistanƒ±sƒ±n. Yalnƒ±zca verilen metinlerden faydalanarak cevap ver.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2  # daha tutarlƒ± cevaplar\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a03d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282411/3782857632.py:11: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.12.4. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 modeli y√ºkleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 252162.57it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n",
      "Model ba≈üarƒ±yla y√ºklendi!\n",
      "Vector boyutu: 1024\n",
      "\n",
      "==================================================\n",
      "HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "\n",
      "üîé LLM destekli cevap olu≈üturuluyor...\n",
      "Arama yapƒ±lƒ±yor: 'tazminat davasƒ± var mƒ±'\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Kullanƒ±m\u001b[39;00m\n\u001b[1;32m     69\u001b[0m searcher \u001b[38;5;241m=\u001b[39m InteractiveLegalSearch(\n\u001b[1;32m     70\u001b[0m     qdrant_url\u001b[38;5;241m=\u001b[39mqdrant_url,\n\u001b[1;32m     71\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m     72\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhukuki_kararlar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteractive_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m, in \u001b[0;36mInteractiveLegalSearch.interactive_search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# LLM destekli cevap\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîé LLM destekli cevap olu≈üturuluyor...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     cevap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müì¢ Cevap:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cevap)\n",
      "Cell \u001b[0;32mIn[6], line 305\u001b[0m, in \u001b[0;36mLegalDocumentVectorDB.answer\u001b[0;34m(self, query, limit)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# 4. OpenAI istemcisi\u001b[39;00m\n\u001b[1;32m    303\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-proj-OIWprDAGysGeOjas4-UgValoM7CXIuApTCBXjgNqgEjANFewQYkAxpcUG06hr1wJsuZZ8dyuwcT3BlbkFJeBx_bEsmm2iQf7LwpJJ-y5ys9buEL1WECTKwBm9E45f8haF1431GtgtufEgM8zeLLzwmPwTssA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# hƒ±zlƒ± ve ekonomik\u001b[39;49;00m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSen deneyimli bir hukuk asistanƒ±sƒ±n. Yalnƒ±zca verilen metinlerden faydalanarak cevap ver.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# daha tutarlƒ± cevaplar\u001b[39;49;00m\n\u001b[1;32m    312\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/ahsen_bulbul/ahsen/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanƒ±m i√ßin ayrƒ± class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key, collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu ba≈ülƒ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandƒ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediƒüiniz metni girin (√ßƒ±kmak i√ßin 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "\n",
    "                # Kullanƒ±cƒ±ya se√ßenek sun\n",
    "                mode = input(\"1 = Chunk sonu√ßlarƒ±nƒ± g√∂ster, 2 = LLM ile cevap √ºret (varsayƒ±lan 1): \") or \"1\"\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Ka√ß sonu√ß g√∂sterilsin? (varsayƒ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                if mode == \"1\":\n",
    "                    # Normal chunk sonu√ßlarƒ±\n",
    "                    results = self.db.search(query, limit=limit)\n",
    "                    \n",
    "                    if not results:\n",
    "                        print(\"‚ùå Sonu√ß bulunamadƒ±.\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"\\nüîç '{query}' i√ßin {len(results)} sonu√ß bulundu:\")\n",
    "                    print(\"-\" * 50)\n",
    "                    \n",
    "                    for i, result in enumerate(results, 1):\n",
    "                        payload = result['payload']\n",
    "                        print(f\"\\nüìÑ {i}. SONU√á (Benzerlik: {result['score']:.3f})\")\n",
    "                        print(f\"   üìã Esas No: {payload['esas_no']}\")\n",
    "                        print(f\"   ‚öñÔ∏è Karar No: {payload['karar_no']}\")\n",
    "                        print(f\"   üìÖ Tarihler: {payload['dates']}\")\n",
    "                        print(f\"   üìù Chunk: {payload['token_count']}\")\n",
    "                        print(f\"   üìÑ Metin √ñnizleme:\")\n",
    "                        print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                        print(\"-\" * 30)\n",
    "                \n",
    "                else:\n",
    "                    # LLM destekli cevap\n",
    "                    print(\"\\nüîé LLM destekli cevap olu≈üturuluyor...\")\n",
    "                    cevap = self.db.answer(query, limit=limit)\n",
    "                    print(\"\\nüì¢ Cevap:\")\n",
    "                    print(cevap)\n",
    "                    print(\"-\" * 50)\n",
    "\n",
    "    # Kullanƒ±m\n",
    "    searcher = InteractiveLegalSearch(\n",
    "        qdrant_url=qdrant_url,\n",
    "        api_key=api_key,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    searcher.interactive_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2780843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_282411/3782857632.py:11: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.12.4. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 modeli y√ºkleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 24643.38it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n",
      "Model ba≈üarƒ±yla y√ºklendi!\n",
      "Vector boyutu: 1024\n",
      "\n",
      "==================================================\n",
      "HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "Arama yapƒ±lƒ±yor: 'ihtiyati tedbir tazminatƒ± hakkƒ±nda davalar'\n",
      "\n",
      "üîç 'ihtiyati tedbir tazminatƒ± hakkƒ±nda davalar' i√ßin 5 sonu√ß bulundu:\n",
      "--------------------------------------------------\n",
      "\n",
      "üìÑ 1. SONU√á (Benzerlik: 0.670)\n",
      "   üìã Esas No: 2022/3281 E.\n",
      "   ‚öñÔ∏è Karar No: 2024/117 K.\n",
      "   üìÖ Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   üìù Chunk: 271\n",
      "   üìÑ Metin √ñnizleme:\n",
      "      1. Asliye Hukuk Mahkemesinin 2009/139 D. ... dosyasƒ± √ºzerinden ihtiyati tedbir konularak in≈üaatƒ±n durdurulduƒüunu, HUMK'un 109. maddesi gereƒüince ihtiyati tedbir kararƒ±nƒ±n verildiƒüi tarihten itibaren 10 g√ºn i√ßerisinde esas hakkƒ±nda davanƒ±n a√ßƒ±lmasƒ± gerektiƒüini ve bu durumun dosyaya ibrazƒ± gerekirken ...\n",
      "------------------------------\n",
      "\n",
      "üìÑ 2. SONU√á (Benzerlik: 0.632)\n",
      "   üìã Esas No: 2022/3281 E.\n",
      "   ‚öñÔ∏è Karar No: 2024/117 K.\n",
      "   üìÖ Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   üìù Chunk: 534\n",
      "   üìÑ Metin √ñnizleme:\n",
      "      ... sayƒ±lƒ± dosyasƒ± √ºzerinden 25.12.2009 tarihinde in≈üaatƒ±n durdurulmasƒ± y√∂n√ºnde ihtiyati tedbir kararƒ± verildiƒüi, teminatƒ±n 31.12.2009 tarihinde yatƒ±rƒ±ldƒ±ƒüƒ±, 04.01.2010 tarihinde mahkemece, ƒ∞cra M√ºd√ºrl√ºƒü√ºne ve Belediye Ba≈ükanlƒ±ƒüƒ±na ihtiyati tedbir kararƒ± gereƒüince i≈ülem yapƒ±lmasƒ± i√ßin m√ºzekkereler y...\n",
      "------------------------------\n",
      "\n",
      "üìÑ 3. SONU√á (Benzerlik: 0.631)\n",
      "   üìã Esas No: 2022/3281 E.\n",
      "   ‚öñÔ∏è Karar No: 2024/117 K.\n",
      "   üìÖ Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   üìù Chunk: 419\n",
      "   üìÑ Metin √ñnizleme:\n",
      "      2. ƒ∞lgili Hukuk 6100 sayƒ±lƒ± Hukuk Muhakemeleri Kanununun ge√ßici 3 √ºnc√º maddesinin ikinci fƒ±krasƒ± atfƒ±yla uygulanmasƒ±na devam olunan m√ºlga 1086 sayƒ±lƒ± Hukuk Usul√º Muhakemeleri Kanununun 428 inci, 439 uncu maddesinin ikinci fƒ±krasƒ± ve 109. maddeleri, 6098 sayƒ±lƒ± T√ºrk Bor√ßlar Kanununun 470 vd. maddeler...\n",
      "------------------------------\n",
      "\n",
      "üìÑ 4. SONU√á (Benzerlik: 0.603)\n",
      "   üìã Esas No: 2022/3281 E.\n",
      "   ‚öñÔ∏è Karar No: 2024/117 K.\n",
      "   üìÖ Tarihler: 2009-06-26,2009-12-25,2009-12-31,2010-01-04,2010-01-05,2010-01-08,2010-01-12,2010-01-13,2010-01-14,2011-12-30,2013-06-04,2024-01-11\n",
      "   üìù Chunk: 262\n",
      "   üìÑ Metin √ñnizleme:\n",
      "      Bozma Kararƒ± 1.Mahkemenin yukarƒ±da belirtilen kararƒ±na kar≈üƒ± s√ºresi i√ßinde davacƒ±lar vekili temyiz isteminde bulunmu≈ütur. 2.Yargƒ±tay (Kapatƒ±lan) 23. Hukuk Dairesinin 04.06.2013 tarihli ve 2013/2984 Esas, 2013/3773 Karar sayƒ±lƒ± kararƒ±yla dava tarihinde y√ºr√ºrl√ºkte olan 1086 sayƒ±lƒ± HUMK'un 109. maddesi...\n",
      "------------------------------\n",
      "\n",
      "üìÑ 5. SONU√á (Benzerlik: 0.598)\n",
      "   üìã Esas No: 2023/596 E.\n",
      "   ‚öñÔ∏è Karar No: 2024/257 K.\n",
      "   üìÖ Tarihler: 2024-01-18\n",
      "   üìù Chunk: 78\n",
      "   üìÑ Metin √ñnizleme:\n",
      "      6. Hukuk Dairesi 2023/596 E. , 2024/257 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ : ... B√∂lge Adliye Mahkemesi 13. Hukuk Dairesi Taraflar arasƒ±ndaki r√ºcuen tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda, ƒ∞lk Derece Mahkemesince davanƒ±n kabul√ºne karar verilmi≈ütir....\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanƒ±m i√ßin ayrƒ± class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name:str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu ba≈ülƒ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandƒ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediƒüiniz metni girin (√ßƒ±kmak i√ßin 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Ka√ß sonu√ß g√∂sterilsin? (varsayƒ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"‚ùå Sonu√ß bulunamadƒ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nüîç '{query}' i√ßin {len(results)} sonu√ß bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nüìÑ {i}. SONU√á (Benzerlik: {result['score']:.3f})\")\n",
    "                    \n",
    "                    print(f\"   üìã Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   ‚öñÔ∏è Karar No: {payload['karar_no']}\")\n",
    "                \n",
    "                    print(f\"   üìÖ Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   üìù Chunk: {payload['token_count']}\")\n",
    "                    print(f\"   üìÑ Metin √ñnizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    searcher = InteractiveLegalSearch(\n",
    "    qdrant_url=qdrant_url,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    searcher.interactive_search()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116ee2a",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfig√ºrasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key  # Buraya ger√ßek API anahtarƒ±nƒ±zƒ± yazƒ±n\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance olu≈ütur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection olu≈ütur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi i≈üle ve y√ºkle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini g√∂ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # √ñrnek aramalar\n",
    "    print(\"\\n=== √ñrnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold d√º≈ü√ºr√ºld√º)\n",
    "    results = db.search(\"ihtiyati √∂nlem tazminat\", limit=10, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati √∂nlem tazminat' - {len(results)} sonu√ß\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonu√ß (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = {\"location\": \"6. Hukuk Dairesi\"}\n",
    "    # , \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararƒ±\", filters=filters, limit=10)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi' - {len(results2)} sonu√ß\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. Sonu√ß (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "    # √ñnce ana fonksiyonu √ßalƒ±≈ütƒ±r (database setup)\n",
    "        main()\n",
    "\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name:str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu ba≈ülƒ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandƒ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediƒüiniz metni girin (√ßƒ±kmak i√ßin 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Ka√ß sonu√ß g√∂sterilsin? (varsayƒ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"‚ùå Sonu√ß bulunamadƒ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nüîç '{query}' i√ßin {len(results)} sonu√ß bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nüìÑ {i}. SONU√á (Benzerlik: {result['score']:.3f})\")\n",
    "                    \n",
    "                    print(f\"   üìã Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   ‚öñÔ∏è Karar No: {payload['karar_no']}\")\n",
    "                \n",
    "                    print(f\"   üìÖ Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   üìù Chunk: {payload['token_count']}\")\n",
    "                    print(f\"   üìÑ Metin √ñnizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    searcher = InteractiveLegalSearch(\n",
    "    qdrant_url=qdrant_url,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    searcher.interactive_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817074e",
   "metadata": {},
   "source": [
    "#### berturk(facia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import os, re, uuid\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct, FieldCondition, MatchValue, Filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki_kararlar\"):\n",
    "        self.client = QdrantClient(url=qdrant_url, api_key=api_key, timeout=60)\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        print(\"BERTurk modeli y√ºkleniyor...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        self.model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        print(\"Model ba≈üarƒ±yla y√ºklendi!\")\n",
    "        \n",
    "        # Embedding boyutu\n",
    "        self.vector_size = self.model.config.hidden_size\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text): return \"\"\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            '√É¬§':'√§', '√É¬∂':'√∂', '√É¬º':'√º', '√É≈∏':'√ü',\n",
    "            '√É‚Ä°':'√á', '√Ñ¬±':'ƒ±', '√Ñ¬∞':'ƒ∞', '√Ö≈∏':'≈ü',\n",
    "            '√Ñ\\x9f':'ƒü', '√É¬ß':'√ß'\n",
    "        }\n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        print(f\"CSV okunuyor: {csv_path}\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ùå Dosya bulunamadƒ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "        print(f\"S√ºtunlar: {df.columns.tolist()} ‚Äì Satƒ±r: {len(df)}\")\n",
    "        \n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        return df\n",
    "\n",
    "    def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        inputs = self.tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(\"Eski collection silindi.\")\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE)\n",
    "            )\n",
    "            print(\"Collection olu≈üturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(\"Collection zaten mevcut.\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.embed(texts)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload={\n",
    "                    \"chunk_id\": row['chunk_id'],\n",
    "                    \"chunk_text\": row['chunk_text_clean'],\n",
    "                    \"other\": row[['location','esasNo','kararNo']].to_dict()\n",
    "                }\n",
    "            ) for idx, (_, row) in enumerate(df.iterrows())\n",
    "        ]\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Y√ºkleniyor\"):\n",
    "            self.client.upsert(collection_name=self.collection_name, points=points[i:i+batch_size])\n",
    "        print(f\"{len(points)} dok√ºman y√ºklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        print(f\"Arama: {query}\")\n",
    "        query_vec = self.embed([query])[0]\n",
    "        search_results = self.client.search(collection_name=self.collection_name, query_vector=query_vec, limit=limit)\n",
    "        return [{\"score\": r.score, \"payload\": r.payload} for r in search_results]\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        info = self.client.get_collection(collection_name=self.collection_name)\n",
    "        return {\n",
    "            \"status\": str(info.status),\n",
    "            \"points_count\": getattr(info, 'points_count', None)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3be198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfig√ºrasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"\n",
    "    \n",
    "    # Vector DB instance olu≈ütur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection olu≈ütur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi i≈üle ve y√ºkle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini g√∂ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # √ñrnek aramalar\n",
    "    print(\"\\n=== √ñrnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama\n",
    "    results = db.search(\"ihtiyati tedbir tazminat\", limit=5)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati edbir tazminat' - {len(results)} sonu√ß\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        payload = result['payload']\n",
    "        print(f\"\\n{i}. Sonu√ß (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {payload.get('esasNo')}\")\n",
    "        print(f\"   Karar No: {payload.get('kararNo')}\")\n",
    "        print(f\"   Metin: {payload['chunk_text'][:200]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5374fb",
   "metadata": {},
   "source": [
    "### chonkie semantic ve allminiLM12 deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler i√ßin vector database sƒ±nƒ±fƒ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarƒ±  \n",
    "            collection_name: Collection adƒ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # T√ºrk√ße i√ßin optimize edilmi≈ü multilingual model\n",
    "        print(\"Sentence Transformer modeli y√ºkleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model ba≈üarƒ±yla y√ºklendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = True):\n",
    "        \"\"\"Collection olu≈ütur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' olu≈üturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarƒ±nƒ± d√ºzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            '√É¬§': '√§', '√É¬∂': '√∂', '√É¬º': '√º', '√É≈∏': '√ü',\n",
    "            '√É‚Ä°': '√á', '√Ñ¬±': 'ƒ±', '√Ñ¬∞': 'ƒ∞', '√Ö≈∏': '≈ü',\n",
    "            '√Ñ\\x9f': 'ƒü', '√É¬ß': '√ß', '√É¬∂': '√∂', '√É¬º': '√º'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla bo≈üluklarƒ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasƒ±nƒ± i≈üle\"\"\"\n",
    "        print(f\"CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlƒ±ƒüƒ±nƒ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ùå HATA: Dosya bulunamadƒ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"‚úÖ UTF-8 encoding ile ba≈üarƒ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"‚úÖ Latin-1 encoding ile ba≈üarƒ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satƒ±r sayƒ±sƒ±: {len(df)}\")\n",
    "        print(f\"S√ºtunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Bo≈ü chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Bo≈ü metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e √ßevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding olu≈üturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity i√ßin normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a y√ºkle\"\"\"\n",
    "        print(\"Qdrant'a y√ºkleme ba≈ülƒ±yor...\")\n",
    "        \n",
    "        # Embeddings olu≈ütur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points olu≈ütur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                #\"daire\": str(row['daire']),\n",
    "                #\"mahkeme\": str(row['mahkeme']),\n",
    "                \"kararNo_tip\": str(row['kararNo_tip']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                #\"chunk_index\": int(row['chunk_index']),\n",
    "                #\"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"token_count\": int(row['token_count'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde y√ºkle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a y√ºkleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Y√ºkleme hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dok√ºman ba≈üarƒ±yla y√ºklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapƒ±lƒ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i olu≈ütur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Geli≈ümi≈ü filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter olu≈ütur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            # if 'daire' in filters:\n",
    "            #     conditions.append(models.FieldCondition(\n",
    "            #         key=\"daire\",\n",
    "            #         match=models.MatchValue(value=filters['daire'])\n",
    "            #     ))\n",
    "            \n",
    "            if 'kararNo_tip' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"kararNo_tip\", \n",
    "                    match=models.MatchValue(value=filters['kararNo_tip'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarƒ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fd870",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    # Konfig√ºrasyon\n",
    "    QDRANT_URL = qdrant_url\n",
    "    API_KEY = api_key  \n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/2semantic_with_metadata.csv\"  # CSV dosya yolu\n",
    "    \n",
    "    # Vector DB instance olu≈ütur\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=QDRANT_URL,\n",
    "        api_key=API_KEY,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    \n",
    "    # Collection olu≈ütur\n",
    "    db.create_collection(recreate=True)\n",
    "    \n",
    "    # CSV'yi i≈üle ve y√ºkle\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "    \n",
    "    # Collection bilgilerini g√∂ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # √ñrnek aramalar\n",
    "    print(\"\\n=== √ñrnek Aramalar ===\")\n",
    "    \n",
    "    # Basit arama (score threshold d√º≈ü√ºr√ºld√º)\n",
    "    results = db.search(\"ihtiyati √∂nlem tazminat\", limit=10, score_threshold=0.6)\n",
    "    print(f\"\\n1. Arama: 'ihtiyati √∂nlem tazminat' - {len(results)} sonu√ß\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonu√ß (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "    \n",
    "    # Filtreli arama\n",
    "    filters = {\"location\": \"6. Hukuk Dairesi\"}\n",
    "    # , \"karar_turu\": \"RED\"}\n",
    "    results2 = db.advanced_search(\"mahkeme kararƒ±\", filters=filters, limit=10)\n",
    "    print(f\"\\n2. Filtreli Arama: '6. Hukuk Dairesi' - {len(results2)} sonu√ß\")\n",
    "    for i, result in enumerate(results2, 1):\n",
    "        print(f\"\\n{i}. Sonu√ß (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:150]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Interaktif kullanƒ±m i√ßin ayrƒ± class\n",
    "    class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str, collection_name:str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu ba≈ülƒ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandƒ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKƒ∞ KARAR ARAMA Sƒ∞STEMƒ∞\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediƒüiniz metni girin (√ßƒ±kmak i√ßin 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Ka√ß sonu√ß g√∂sterilsin? (varsayƒ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"‚ùå Sonu√ß bulunamadƒ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nüîç '{query}' i√ßin {len(results)} sonu√ß bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nüìÑ {i}. SONU√á (Benzerlik: {result['score']:.3f})\")\n",
    "                    \n",
    "                    print(f\"   üìã Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   ‚öñÔ∏è Karar No: {payload['karar_no']}\")\n",
    "                \n",
    "                    print(f\"   üìÖ Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   üìù Chunk: {payload['token_count']}\")\n",
    "                    print(f\"   üìÑ Metin √ñnizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f18695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
