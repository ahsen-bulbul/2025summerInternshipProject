{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "collection_name = os.getenv(\"QDRANT_COLLECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler için vector database sınıfı\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarı  \n",
    "            collection_name: Collection adı\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Türkçe için optimize edilmiş multilingual model\n",
    "        print(\"Sentence Transformer modeli yükleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model başarıyla yüklendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluştur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluşturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarını düzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'Ã¤': 'ä', 'Ã¶': 'ö', 'Ã¼': 'ü', 'ÃŸ': 'ß',\n",
    "            'Ã‡': 'Ç', 'Ä±': 'ı', 'Ä°': 'İ', 'ÅŸ': 'ş',\n",
    "            'Ä\\x9f': 'ğ', 'Ã§': 'ç', 'Ã¶': 'ö', 'Ã¼': 'ü'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasını işle\"\"\"\n",
    "        print(f\"CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlığını kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ HATA: Dosya bulunamadı: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"✅ UTF-8 encoding ile başarıyla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"✅ Latin-1 encoding ile başarıyla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satır sayısı: {len(df)}\")\n",
    "        print(f\"Sütunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Boş chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Boş metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e çevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluşturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity için normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yükle\"\"\"\n",
    "        print(\"Qdrant'a yükleme başlıyor...\")\n",
    "        \n",
    "        # Embeddings oluştur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluştur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"daire\": str(row['daire']),\n",
    "                \"mahkeme\": str(row['mahkeme']),\n",
    "                \"karar_turu\": str(row['karar_turu']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \"chunk_index\": int(row['chunk_index']),\n",
    "                \"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"chunk_length\": int(row['chunk_length'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yükleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Yükleme hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} doküman başarıyla yüklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapılıyor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluştur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Gelişmiş filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter oluştur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'daire' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"daire\",\n",
    "                    match=models.MatchValue(value=filters['daire'])\n",
    "                ))\n",
    "            \n",
    "            if 'karar_turu' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"karar_turu\", \n",
    "                    match=models.MatchValue(value=filters['karar_turu'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributeları kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LegalDocumentVectorDB initialized for collection: hukuki_kararlar\n",
      "Collection recreated: hukuki_kararlar\n",
      "Processing CSV: /home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\n",
      "Uploading 137 records to Qdrant in batches of 50\n",
      "\n",
      "=== Qdrant Collection Bilgileri ===\n",
      "{\n",
      "  \"collection_name\": \"hukuki_kararlar\",\n",
      "  \"status\": \"active\",\n",
      "  \"vector_count\": 1000\n",
      "}\n",
      "\n",
      "=== Örnek Aramalar ===\n",
      "\n",
      "1. Sonuç (Skor: 0.850)\n",
      "   Daire: 4. Hukuk Dairesi\n",
      "   Esas No: 2023/1234\n",
      "   Karar No: 2023/5678\n",
      "   Metin: İhtiyati tedbir kararının tazminat yükümlülüğü hakkında......\n",
      "\n",
      "2. Sonuç (Skor: 0.850)\n",
      "   Daire: 4. Hukuk Dairesi\n",
      "   Esas No: 2023/1234\n",
      "   Karar No: 2023/5678\n",
      "   Metin: İhtiyati tedbir kararının tazminat yükümlülüğü hakkında......\n",
      "\n",
      "3. Sonuç (Skor: 0.850)\n",
      "   Daire: 4. Hukuk Dairesi\n",
      "   Esas No: 2023/1234\n",
      "   Karar No: 2023/5678\n",
      "   Metin: İhtiyati tedbir kararının tazminat yükümlülüğü hakkında......\n",
      "CSV okunuyor: /home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\n",
      "Toplam satır: 137\n",
      "Sample alındı: 5 satır\n",
      "Dataset hazır.\n",
      "RAG değerlendirmesi başlatılıyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]Exception raised in Job[5]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:   5%|▌         | 1/20 [00:49<15:46, 49.79s/it]Exception raised in Job[9]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  10%|█         | 2/20 [01:00<08:05, 26.97s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  15%|█▌        | 3/20 [01:52<10:48, 38.13s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  20%|██        | 4/20 [01:58<06:47, 25.46s/it]Exception raised in Job[6]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  25%|██▌       | 5/20 [02:01<04:20, 17.38s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  30%|███       | 6/20 [02:18<04:00, 17.19s/it]Exception raised in Job[4]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  35%|███▌      | 7/20 [02:21<02:43, 12.55s/it]Exception raised in Job[15]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  40%|████      | 8/20 [02:24<01:56,  9.73s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  45%|████▌     | 9/20 [02:29<01:28,  8.03s/it]Exception raised in Job[17]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  50%|█████     | 10/20 [02:30<00:58,  5.89s/it]Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  55%|█████▌    | 11/20 [02:30<00:37,  4.22s/it]Exception raised in Job[14]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  60%|██████    | 12/20 [02:41<00:49,  6.15s/it]Exception raised in Job[8]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  65%|██████▌   | 13/20 [02:42<00:32,  4.63s/it]Exception raised in Job[7]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  70%|███████   | 14/20 [02:44<00:23,  3.86s/it]Exception raised in Job[12]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  75%|███████▌  | 15/20 [02:45<00:15,  3.12s/it]Exception raised in Job[16]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  80%|████████  | 16/20 [02:56<00:22,  5.52s/it]Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Evaluating:  85%|████████▌ | 17/20 [03:00<00:14,  4.79s/it]Exception raised in Job[18]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Evaluating:  95%|█████████▌| 19/20 [04:47<00:27, 27.47s/it]Exception raised in Job[19]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 20/20 [04:58<00:00, 14.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG Değerlendirme Sonuçları ===\n",
      "faithfulness: nan\n",
      "answer_relevancy: nan\n",
      "context_precision: nan\n",
      "context_recall: nan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "\n",
    "# ENV dosyasını yükle\n",
    "dotenv_path = \"/home/yapayzeka/ahsen_bulbul/qdrant.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY bulunamadı. qdrant.env dosyasını kontrol et.\")\n",
    "\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, csv_path: str, sample_size: int = 5):\n",
    "        \"\"\"\n",
    "        CSV'den verileri yükleyen ve RAG değerlendirmesi yapan sınıf.\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.sample_size = sample_size\n",
    "        self.df = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def load_csv(self):\n",
    "        \"\"\"CSV dosyasını oku ve gerekli sütunları hazırla\"\"\"\n",
    "        print(f\"CSV okunuyor: {self.csv_path}\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        print(f\"Toplam satır: {len(self.df)}\")\n",
    "\n",
    "        # Context sütunu oluştur\n",
    "        self.df[\"contexts\"] = self.df.apply(\n",
    "            lambda row: [\n",
    "                f\"Daire: {row['daire']}\",\n",
    "                f\"Mahkeme: {row['mahkeme']}\",\n",
    "                f\"Karar Türü: {row['karar_turu']}\"\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # question ve ground_truth sütunları oluştur\n",
    "        self.df[\"question\"] = self.df[\"chunk_text\"]  # şimdilik chunk_text'i soru gibi kullandık\n",
    "        self.df[\"ground_truth\"] = self.df[\"esasNo\"].astype(str)  # veya kararNo\n",
    "\n",
    "        # sadece küçük bir sample al (API kotasını aşmamak için)\n",
    "        if self.sample_size and len(self.df) > self.sample_size:\n",
    "            self.df = self.df.sample(self.sample_size, random_state=42)\n",
    "            print(f\"Sample alındı: {len(self.df)} satır\")\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"Pandas DataFrame'i HuggingFace Dataset formatına çevir\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"CSV yüklenmemiş. Önce load_csv() çağır.\")\n",
    "        \n",
    "        self.dataset = Dataset.from_pandas(\n",
    "            self.df[[\"question\", \"chunk_text\", \"contexts\", \"ground_truth\"]].rename(\n",
    "                columns={\"chunk_text\": \"answer\"}\n",
    "            )\n",
    "        )\n",
    "        print(\"Dataset hazır.\")\n",
    "\n",
    "    def evaluate_rag(self):\n",
    "        \"\"\"RAG değerlendirmesini çalıştır\"\"\"\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset hazırlanmadı. Önce prepare_dataset() çağır.\")\n",
    "        \n",
    "        print(\"RAG değerlendirmesi başlatılıyor...\")\n",
    "        results = evaluate(\n",
    "            self.dataset,\n",
    "            metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def print_evaluation_results(self, results):\n",
    "        \"\"\"Evaluation sonuçlarını yazdır\"\"\"\n",
    "        print(\"\\n=== RAG Değerlendirme Sonuçları ===\")\n",
    "        \n",
    "        try:\n",
    "            # RAGAS EvaluationResult nesnesinden metrikleri al\n",
    "            if hasattr(results, 'to_pandas'):\n",
    "                # Pandas DataFrame'e çevir\n",
    "                df_results = results.to_pandas()\n",
    "                for column in df_results.columns:\n",
    "                    if df_results[column].dtype in ['float64', 'float32']:\n",
    "                        mean_score = df_results[column].mean()\n",
    "                        print(f\"{column}: {mean_score:.4f}\")\n",
    "            \n",
    "            elif hasattr(results, '__dict__'):\n",
    "                # Nesne özelliklerine direkt erişim\n",
    "                for metric, score in results.__dict__.items():\n",
    "                    if isinstance(score, (int, float)):\n",
    "                        print(f\"{metric}: {score:.4f}\")\n",
    "            \n",
    "            else:\n",
    "                # Manuel olarak bilinen metrikleri yazdır\n",
    "                metric_names = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "                for metric in metric_names:\n",
    "                    if hasattr(results, metric):\n",
    "                        score = getattr(results, metric)\n",
    "                        print(f\"{metric}: {score:.4f}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Sonuçları yazdırırken hata: {e}\")\n",
    "            print(\"Results objesi tipi:\", type(results))\n",
    "            print(\"Results objesi:\", results)\n",
    "\n",
    "\n",
    "# Basit LegalDocumentVectorDB mock sınıfı (gerçek implementasyon için ayrı dosyada olmalı)\n",
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url, api_key, collection_name):\n",
    "        self.qdrant_url = qdrant_url\n",
    "        self.api_key = api_key\n",
    "        self.collection_name = collection_name\n",
    "        print(f\"LegalDocumentVectorDB initialized for collection: {collection_name}\")\n",
    "\n",
    "    def create_collection(self, recreate=False):\n",
    "        print(f\"Collection {'recreated' if recreate else 'created'}: {self.collection_name}\")\n",
    "\n",
    "    def process_csv(self, csv_path):\n",
    "        print(f\"Processing CSV: {csv_path}\")\n",
    "        # Mock implementation - gerçekte CSV'yi işleyip DataFrame döndürür\n",
    "        return pd.read_csv(csv_path) if os.path.exists(csv_path) else None\n",
    "\n",
    "    def upload_to_qdrant(self, df, batch_size=50):\n",
    "        print(f\"Uploading {len(df)} records to Qdrant in batches of {batch_size}\")\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        return {\n",
    "            \"collection_name\": self.collection_name,\n",
    "            \"status\": \"active\",\n",
    "            \"vector_count\": 1000  # mock data\n",
    "        }\n",
    "\n",
    "    def search(self, query, limit=5, score_threshold=0.5):\n",
    "        # Mock search results\n",
    "        return [\n",
    "            {\n",
    "                \"score\": 0.85,\n",
    "                \"payload\": {\n",
    "                    \"daire\": \"4. Hukuk Dairesi\",\n",
    "                    \"esas_no\": \"2023/1234\",\n",
    "                    \"karar_no\": \"2023/5678\",\n",
    "                    \"chunk_text\": \"İhtiyati tedbir kararının tazminat yükümlülüğü hakkında...\"\n",
    "                }\n",
    "            }\n",
    "        ] * min(limit, 3)  # Mock olarak 3 sonuç döndür\n",
    "\n",
    "\n",
    "def main():\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"\n",
    "\n",
    "    # CSV dosyasının varlığını kontrol et\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        print(f\"UYARI: CSV dosyası bulunamadı: {CSV_FILE}\")\n",
    "        print(\"Mock data ile devam ediliyor...\")\n",
    "        # Mock CSV oluştur (test için)\n",
    "        mock_data = {\n",
    "            'daire': ['4. Hukuk Dairesi'] * 5,\n",
    "            'mahkeme': ['Yargıtay'] * 5,\n",
    "            'karar_turu': ['Temyiz'] * 5,\n",
    "            'chunk_text': [\n",
    "                'İhtiyati tedbir kararının tazminat yükümlülüğü konusunda...',\n",
    "                'Sözleşmeli fesih durumunda tazminat hesaplanması...',\n",
    "                'Manevi tazminat miktarının belirlenmesinde...',\n",
    "                'İş kazası sonucu maddi tazminat talebinin...',\n",
    "                'Tecavüz fiili nedeniyle manevi tazminat...'\n",
    "            ],\n",
    "            'esasNo': ['2023/1234', '2023/1235', '2023/1236', '2023/1237', '2023/1238'],\n",
    "            'kararNo': ['2023/5678', '2023/5679', '2023/5680', '2023/5681', '2023/5682']\n",
    "        }\n",
    "        \n",
    "        # Geçici CSV oluştur\n",
    "        temp_csv = \"/tmp/mock_yargitay_chunks.csv\"\n",
    "        pd.DataFrame(mock_data).to_csv(temp_csv, index=False)\n",
    "        CSV_FILE = temp_csv\n",
    "\n",
    "    # Qdrant tarafı\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=qdrant_url,\n",
    "        api_key=api_key,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    db.create_collection(recreate=True)\n",
    "    df = db.process_csv(CSV_FILE)\n",
    "    if df is not None:\n",
    "        db.upload_to_qdrant(df, batch_size=50)\n",
    "\n",
    "    # Collection bilgilerini göster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Qdrant Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "\n",
    "    # Örnek aramalar\n",
    "    print(\"\\n=== Örnek Aramalar ===\")\n",
    "    results = db.search(\"ihtiyati tedbir tazminat\", limit=3, score_threshold=0.6)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Daire: {result['payload']['daire']}\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "\n",
    "    # RAG Değerlendirmesi\n",
    "    try:\n",
    "        evaluator = RAGEvaluator(CSV_FILE, sample_size=5)\n",
    "        evaluator.load_csv()\n",
    "        evaluator.prepare_dataset()\n",
    "        rag_results = evaluator.evaluate_rag()\n",
    "        \n",
    "        # Sonuçları yazdır (düzeltilmiş versiyon)\n",
    "        evaluator.print_evaluation_results(rag_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"RAG değerlendirmesinde hata oluştu: {e}\")\n",
    "        print(\"Bu genellikle OPENAI_API_KEY eksikliği veya API limiti aşımından kaynaklanır.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
