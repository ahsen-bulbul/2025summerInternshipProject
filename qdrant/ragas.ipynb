{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Gerçek CSV ile RAG Değerlendirmesi\n",
      "============================================================\n",
      "📂 Gerçek CSV okunuyor: /home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\n",
      "✅ CSV yüklendi: 137 satır\n",
      "📋 CSV Sütunları: ['_id', 'location', 'esasNo', 'kararNo', 'extractedDates', 'esasNo_num', 'esasNo_tip', 'kararNo_num', 'kararNo_tip', 'daire', 'mahkeme', 'karar_turu', 'chunk_id', 'chunk_index', 'total_chunks', 'chunk_text', 'chunk_length']\n",
      "📊 İlk 3 satır örneği:\n",
      "                        _id        location        esasNo      kararNo  \\\n",
      "0  6750524485b6640290c37b06  6.HukukDairesi   2023/596 E.  2024/257 K.   \n",
      "1  6750524485b6640290c37b07  6.HukukDairesi  2022/3281 E.  2024/117 K.   \n",
      "2  6750524485b6640290c37b07  6.HukukDairesi  2022/3281 E.  2024/117 K.   \n",
      "\n",
      "                                      extractedDates esasNo_num esasNo_tip  \\\n",
      "0                                         2024-01-18   2023/596          E   \n",
      "1  2009-06-26,2009-12-25,2009-12-31,2010-01-04,20...  2022/3281          E   \n",
      "2  2009-06-26,2009-12-25,2009-12-31,2010-01-04,20...  2022/3281          E   \n",
      "\n",
      "  kararNo_num kararNo_tip             daire  \\\n",
      "0    2024/257           K  6. Hukuk Dairesi   \n",
      "1    2024/117           K  6. Hukuk Dairesi   \n",
      "2    2024/117           K  6. Hukuk Dairesi   \n",
      "\n",
      "                                             mahkeme karar_turu  \\\n",
      "0  [...] Bölge Adliye Mahkemesi 13. Hukuk Dairesi...        RED   \n",
      "1              Asliye Hukuk Mahkemesi Taraflar arası        RED   \n",
      "2              Asliye Hukuk Mahkemesi Taraflar arası        RED   \n",
      "\n",
      "                           chunk_id  chunk_index  total_chunks  \\\n",
      "0  6750524485b6640290c37b06_chunk_1            1             1   \n",
      "1  6750524485b6640290c37b07_chunk_1            1            13   \n",
      "2  6750524485b6640290c37b07_chunk_2            2            13   \n",
      "\n",
      "                                          chunk_text  chunk_length  \n",
      "0  6. Hukuk Dairesi 2023/596 E. , 2024/257 K. \\n ...          1095  \n",
      "1  6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n...          1071  \n",
      "2  . maddesi, ihtiyati tedbir kararının haksız ol...          1061  \n",
      "🔽 Sample alındı: 5 satır\n",
      "\n",
      "🔍 5 örnek değerlendiriliyor...\n",
      "⏳ İşleniyor: 106/5 - ID: 2023/4278 E.\n",
      "⏳ İşleniyor: 105/5 - ID: 2024/229 E.\n",
      "⏳ İşleniyor: 13/5 - ID: 2022/3281 E.\n",
      "⏳ İşleniyor: 27/5 - ID: 2023/4229 E.\n",
      "⏳ İşleniyor: 124/5 - ID: 2022/4331 E.\n",
      "\n",
      "============================================================\n",
      "📈 GERÇEK CSV DEĞERLENDİRME SONUÇLARI\n",
      "============================================================\n",
      "faithfulness        : 0.1073\n",
      "answer_relevancy    : 0.2651\n",
      "context_precision   : 0.0000\n",
      "context_recall      : 0.0000\n",
      "\n",
      "📊 En yüksek faithfulness skoru: 0.2188\n",
      "📊 En düşük faithfulness skoru: 0.0389\n",
      "\n",
      "💾 Detaylı sonuçlar kaydedildi: /tmp/real_csv_rag_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "class RealCSVRAGEvaluator:\n",
    "    def __init__(self, csv_path: str, sample_size: int = 5):\n",
    "        \"\"\"\n",
    "        Gerçek CSV dosyanızı okuyarak RAG değerlendirmesi yapar\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.sample_size = sample_size\n",
    "        self.df = None\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def load_real_csv(self):\n",
    "        \"\"\"Gerçek CSV dosyanızı okur ve işler\"\"\"\n",
    "        print(f\"📂 Gerçek CSV okunuyor: {self.csv_path}\")\n",
    "        \n",
    "        if not os.path.exists(self.csv_path):\n",
    "            raise FileNotFoundError(f\"CSV dosyası bulunamadı: {self.csv_path}\")\n",
    "        \n",
    "        # CSV'yi oku\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        print(f\"✅ CSV yüklendi: {len(self.df)} satır\")\n",
    "        \n",
    "        # Sütunları kontrol et ve yazdır\n",
    "        print(f\"📋 CSV Sütunları: {list(self.df.columns)}\")\n",
    "        print(f\"📊 İlk 3 satır örneği:\")\n",
    "        print(self.df.head(3))\n",
    "        \n",
    "        # Gerekli sütunları hazırla\n",
    "        self.prepare_evaluation_columns()\n",
    "        \n",
    "        # Sample al (büyük dosyalar için)\n",
    "        if self.sample_size and len(self.df) > self.sample_size:\n",
    "            self.df = self.df.sample(self.sample_size, random_state=42)\n",
    "            print(f\"🔽 Sample alındı: {len(self.df)} satır\")\n",
    "    \n",
    "    def prepare_evaluation_columns(self):\n",
    "        \"\"\"RAG değerlendirmesi için gerekli sütunları hazırlar\"\"\"\n",
    "        \n",
    "        # Context sütunu oluştur (mevcut bilgilerden)\n",
    "        if 'contexts' not in self.df.columns:\n",
    "            context_parts = []\n",
    "            if 'daire' in self.df.columns:\n",
    "                context_parts.append(f\"Daire: {self.df['daire']}\")\n",
    "            if 'mahkeme' in self.df.columns:\n",
    "                context_parts.append(f\"Mahkeme: {self.df['mahkeme']}\")\n",
    "            if 'karar_turu' in self.df.columns:\n",
    "                context_parts.append(f\"Karar Türü: {self.df['karar_turu']}\")\n",
    "            \n",
    "            self.df[\"contexts\"] = self.df.apply(\n",
    "                lambda row: [\n",
    "                    f\"Daire: {row.get('daire', 'Bilinmeyen')}\" if 'daire' in self.df.columns else \"\",\n",
    "                    f\"Mahkeme: {row.get('mahkeme', 'Bilinmeyen')}\" if 'mahkeme' in self.df.columns else \"\",\n",
    "                    f\"Karar Türü: {row.get('karar_turu', 'Bilinmeyen')}\" if 'karar_turu' in self.df.columns else \"\"\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        # Question sütunu oluştur\n",
    "        if 'question' not in self.df.columns:\n",
    "            if 'chunk_text' in self.df.columns:\n",
    "                # chunk_text'den soru oluştur\n",
    "                self.df['question'] = self.df['chunk_text'].apply(\n",
    "                    lambda text: f\"Bu hukuki karar hakkında açıklama yapınız: {text[:100]}...\"\n",
    "                )\n",
    "            else:\n",
    "                # Genel sorular oluştur\n",
    "                self.df['question'] = \"Bu hukuki karar hakkında bilgi veriniz.\"\n",
    "        \n",
    "        # Answer sütunu (chunk_text'i kullan)\n",
    "        if 'answer' not in self.df.columns:\n",
    "            if 'chunk_text' in self.df.columns:\n",
    "                self.df['answer'] = self.df['chunk_text']\n",
    "            else:\n",
    "                raise ValueError(\"'chunk_text' sütunu bulunamadı!\")\n",
    "        \n",
    "        # Ground truth sütunu\n",
    "        if 'ground_truth' not in self.df.columns:\n",
    "            if 'esasNo' in self.df.columns:\n",
    "                self.df['ground_truth'] = self.df['esasNo'].astype(str)\n",
    "            elif 'kararNo' in self.df.columns:\n",
    "                self.df['ground_truth'] = self.df['kararNo'].astype(str)\n",
    "            else:\n",
    "                # Fallback: index kullan\n",
    "                self.df['ground_truth'] = self.df.index.astype(str)\n",
    "    \n",
    "    def evaluate_faithfulness(self, answer: str, contexts: list) -> float:\n",
    "        \"\"\"Cevabın context'e ne kadar sadık olduğunu ölçer\"\"\"\n",
    "        if not contexts or not answer:\n",
    "            return 0.0\n",
    "        \n",
    "        # Context'leri birleştir ve temizle\n",
    "        combined_context = ' '.join([ctx for ctx in contexts if ctx.strip()])\n",
    "        \n",
    "        if not combined_context.strip():\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # TF-IDF ile benzerlik hesapla\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([answer, combined_context])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return min(similarity, 1.0)\n",
    "        except:\n",
    "            # Fallback: Kelime örtüşmesi\n",
    "            answer_words = set(answer.lower().split())\n",
    "            context_words = set(combined_context.lower().split())\n",
    "            if not answer_words:\n",
    "                return 0.0\n",
    "            overlap = len(answer_words.intersection(context_words))\n",
    "            return overlap / len(answer_words)\n",
    "    \n",
    "    def evaluate_relevancy(self, question: str, answer: str) -> float:\n",
    "        \"\"\"Cevabın soruyla ne kadar ilgili olduğunu ölçer\"\"\"\n",
    "        if not question or not answer:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([question, answer])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return min(similarity, 1.0)\n",
    "        except:\n",
    "            # Fallback: Kelime örtüşmesi\n",
    "            q_words = set(question.lower().split())\n",
    "            a_words = set(answer.lower().split())\n",
    "            if not q_words:\n",
    "                return 0.0\n",
    "            overlap = len(q_words.intersection(a_words))\n",
    "            return overlap / len(q_words)\n",
    "    \n",
    "    def evaluate_context_precision(self, contexts: list, ground_truth: str) -> float:\n",
    "        \"\"\"Context'in ne kadar kesin/ilgili olduğunu ölçer\"\"\"\n",
    "        if not contexts or not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_contexts = 0\n",
    "        valid_contexts = [ctx for ctx in contexts if ctx.strip()]\n",
    "        \n",
    "        if not valid_contexts:\n",
    "            return 0.0\n",
    "        \n",
    "        for context in valid_contexts:\n",
    "            if ground_truth.lower() in context.lower():\n",
    "                relevant_contexts += 1\n",
    "            elif any(word in context.lower() for word in ground_truth.lower().split() if len(word) > 2):\n",
    "                relevant_contexts += 0.5\n",
    "        \n",
    "        return min(relevant_contexts / len(valid_contexts), 1.0)\n",
    "    \n",
    "    def evaluate_context_recall(self, contexts: list, ground_truth: str) -> float:\n",
    "        \"\"\"Ground truth'un context'te ne kadar coverage'ı var\"\"\"\n",
    "        if not contexts or not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        combined_context = ' '.join([ctx for ctx in contexts if ctx.strip()]).lower()\n",
    "        gt_words = set(ground_truth.lower().split())\n",
    "        \n",
    "        if not gt_words or not combined_context:\n",
    "            return 0.0\n",
    "        \n",
    "        found_words = sum(1 for word in gt_words if len(word) > 2 and word in combined_context)\n",
    "        return found_words / len(gt_words) if gt_words else 0.0\n",
    "    \n",
    "    def evaluate_dataset(self) -> dict:\n",
    "        \"\"\"Dataset'teki tüm örnekleri değerlendirir\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"CSV yüklenmemiş! Önce load_real_csv() çağırın.\")\n",
    "        \n",
    "        results = {\n",
    "            'faithfulness': [],\n",
    "            'answer_relevancy': [],\n",
    "            'context_precision': [],\n",
    "            'context_recall': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🔍 {len(self.df)} örnek değerlendiriliyor...\")\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            print(f\"⏳ İşleniyor: {idx+1}/{len(self.df)} - ID: {row.get('esasNo', idx)}\")\n",
    "            \n",
    "            question = str(row['question'])\n",
    "            answer = str(row['answer'])\n",
    "            contexts = row['contexts'] if isinstance(row['contexts'], list) else [str(row['contexts'])]\n",
    "            ground_truth = str(row['ground_truth'])\n",
    "            \n",
    "            # Debug bilgileri\n",
    "            if idx == 0:\n",
    "                print(f\"📝 Örnek veri:\")\n",
    "                print(f\"   Soru: {question[:100]}...\")\n",
    "                print(f\"   Cevap: {answer[:100]}...\")\n",
    "                print(f\"   Context: {contexts[:2]}\")\n",
    "                print(f\"   Ground Truth: {ground_truth}\")\n",
    "            \n",
    "            # Metrikleri hesapla\n",
    "            faithfulness = self.evaluate_faithfulness(answer, contexts)\n",
    "            relevancy = self.evaluate_relevancy(question, answer)\n",
    "            precision = self.evaluate_context_precision(contexts, ground_truth)\n",
    "            recall = self.evaluate_context_recall(contexts, ground_truth)\n",
    "            \n",
    "            results['faithfulness'].append(faithfulness)\n",
    "            results['answer_relevancy'].append(relevancy)\n",
    "            results['context_precision'].append(precision)\n",
    "            results['context_recall'].append(recall)\n",
    "        \n",
    "        # Ortalama skorları hesapla\n",
    "        avg_results = {\n",
    "            metric: np.mean(scores) for metric, scores in results.items()\n",
    "        }\n",
    "        \n",
    "        return avg_results, results\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"🚀 Gerçek CSV ile RAG Değerlendirmesi\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Gerçek CSV dosya yolu (sizinkini kullanın)\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Evaluator'ü başlat\n",
    "        evaluator = RealCSVRAGEvaluator(CSV_FILE, sample_size=5)  # 5 örnek test\n",
    "        \n",
    "        # Gerçek CSV'yi yükle\n",
    "        evaluator.load_real_csv()\n",
    "        \n",
    "        # Değerlendirmeyi çalıştır\n",
    "        avg_results, detailed_results = evaluator.evaluate_dataset()\n",
    "        \n",
    "        # Sonuçları göster\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📈 GERÇEK CSV DEĞERLENDİRME SONUÇLARI\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for metric, score in avg_results.items():\n",
    "            print(f\"{metric:20}: {score:.4f}\")\n",
    "        \n",
    "        # En iyi ve en kötü örnekleri göster\n",
    "        print(f\"\\n📊 En yüksek faithfulness skoru: {max(detailed_results['faithfulness']):.4f}\")\n",
    "        print(f\"📊 En düşük faithfulness skoru: {min(detailed_results['faithfulness']):.4f}\")\n",
    "        \n",
    "        # Sonuçları kaydet\n",
    "        results_df = pd.DataFrame(detailed_results)\n",
    "        output_file = \"/tmp/real_csv_rag_evaluation.csv\"\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n💾 Detaylı sonuçlar kaydedildi: {output_file}\")\n",
    "        \n",
    "        return avg_results\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Dosya hatası: {e}\")\n",
    "        print(\"CSV dosya yolunu kontrol edin!\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Değerlendirme hatası: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam satır: 137\n",
      "Sample alındı: 50 satır\n",
      "Q&A dataset oluşturuldu ve kaydedildi: /home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_qa_dataset.csv\n",
      "                                              question  \\\n",
      "105  6. Hukuk Dairesi tarafından verilen RED kararı...   \n",
      "104  6. Hukuk Dairesi tarafından verilen RED kararı...   \n",
      "12   Asliye Hukuk Mahkemesi Taraflar arası kararına...   \n",
      "26   Esas No 2023/4229 E. ve Karar No 2024/136 K. i...   \n",
      "123  [...] Bölge Adliye Mahkemesi 46. Hukuk Dairesi...   \n",
      "\n",
      "                                                answer  \\\n",
      "105  6. Hukuk Dairesi 2023/4278 E. , 2024/689 K. \\n...   \n",
      "104  . Davalı vekili 19.12.2023 tarihli dilekçesi i...   \n",
      "12   . 2. İlgili Hukuk 6100 sayılı Hukuk Muhakemele...   \n",
      "26   . B. İstinaf Sebepleri Alacaklı [...] [...] Tı...   \n",
      "123  . III. İLK DERECE MAHKEMESİ KARARI İlk Derece ...   \n",
      "\n",
      "                                               context        esasNo  \\\n",
      "105  6. Hukuk Dairesi 2023/4278 E. , 2024/689 K. \\n...  2023/4278 E.   \n",
      "104  . Davalı vekili 19.12.2023 tarihli dilekçesi i...   2024/229 E.   \n",
      "12   . 2. İlgili Hukuk 6100 sayılı Hukuk Muhakemele...  2022/3281 E.   \n",
      "26   . B. İstinaf Sebepleri Alacaklı [...] [...] Tı...  2023/4229 E.   \n",
      "123  . III. İLK DERECE MAHKEMESİ KARARI İlk Derece ...  2022/4331 E.   \n",
      "\n",
      "         kararNo             daire  \\\n",
      "105  2024/689 K.  6. Hukuk Dairesi   \n",
      "104  2024/489 K.  6. Hukuk Dairesi   \n",
      "12   2024/117 K.  6. Hukuk Dairesi   \n",
      "26   2024/136 K.  6. Hukuk Dairesi   \n",
      "123  2024/516 K.  6. Hukuk Dairesi   \n",
      "\n",
      "                                               mahkeme karar_turu  \n",
      "105  Asliye Hukuk Mahkemesi - K A R A R - Davacı ve...        RED  \n",
      "104  Asliye Hukuk Mahkemesi Davacılar vekili dava d...        RED  \n",
      "12               Asliye Hukuk Mahkemesi Taraflar arası        RED  \n",
      "26   [...] Bölge Adliye Mahkemesi 17. Hukuk Dairesi...        RED  \n",
      "123  [...] Bölge Adliye Mahkemesi 46. Hukuk Dairesi...        RED  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_qa_dataset(csv_path, output_path, sample_size=None, seed=42):\n",
    "    \"\"\"\n",
    "    CSV'den otomatik Q&A dataset üretir.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Chunk CSV dosyası\n",
    "        output_path (str): Oluşturulacak CSV dosyası\n",
    "        sample_size (int, optional): Kaç satır örnek alınacak. Default tüm veri.\n",
    "        seed (int): Random seed\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Toplam satır: {len(df)}\")\n",
    "\n",
    "    # Sample al\n",
    "    if sample_size and len(df) > sample_size:\n",
    "        df = df.sample(sample_size, random_state=seed)\n",
    "        print(f\"Sample alındı: {len(df)} satır\")\n",
    "\n",
    "    # Soru oluşturma (basit template)\n",
    "    def create_question(row):\n",
    "        # Örnek soru şablonları\n",
    "        templates = [\n",
    "            f\"{row['daire']} tarafından verilen {row['karar_turu']} kararı hakkında bilgi veriniz.\",\n",
    "            f\"Esas No {row['esasNo']} ve Karar No {row['kararNo']} ile ilgili kararda ne söyleniyor?\",\n",
    "            f\"{row['mahkeme']} kararına göre durum nedir?\",\n",
    "            f\"{row['daire']} {row['karar_turu']} kararında ne karar verilmiş?\"\n",
    "        ]\n",
    "        return random.choice(templates)\n",
    "\n",
    "    df['question'] = df.apply(create_question, axis=1)\n",
    "    df['answer'] = df['chunk_text']\n",
    "    df['context'] = df['chunk_text']  # RAG için context olarak chunk'ı kullanabiliriz\n",
    "\n",
    "    # Sadece gerekli sütunları seç\n",
    "    qa_df = df[['question', 'answer', 'context', 'esasNo', 'kararNo', 'daire', 'mahkeme', 'karar_turu']]\n",
    "\n",
    "    # Kaydet\n",
    "    qa_df.to_csv(output_path, index=False)\n",
    "    print(f\"Q&A dataset oluşturuldu ve kaydedildi: {output_path}\")\n",
    "\n",
    "    return qa_df\n",
    "\n",
    "\n",
    "# Kullanım örneği\n",
    "csv_file = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"\n",
    "output_file = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_qa_dataset.csv\"\n",
    "qa_df = generate_qa_dataset(csv_file, output_file, sample_size=50)\n",
    "print(qa_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLegalDocumentVectorDB\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, qdrant_url: \u001b[38;5;28mstr\u001b[39m, api_key: \u001b[38;5;28mstr\u001b[39m, collection_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhukuki kararlar\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Hukuki belgeler için vector database sınıfı\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m            collection_name: Collection adı\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mLegalDocumentVectorDB\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_csv\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"CSV dosyasını işle\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV dosyası okunuyor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler için vector database sınıfı\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarı  \n",
    "            collection_name: Collection adı\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Türkçe için optimize edilmiş multilingual model\n",
    "        print(\"Sentence Transformer modeli yükleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model başarıyla yüklendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluştur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluşturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarını düzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'Ã¤': 'ä', 'Ã¶': 'ö', 'Ã¼': 'ü', 'ÃŸ': 'ß',\n",
    "            'Ã‡': 'Ç', 'Ä±': 'ı', 'Ä°': 'İ', 'ÅŸ': 'ş',\n",
    "            'Ä\\x9f': 'ğ', 'Ã§': 'ç', 'Ã¶': 'ö', 'Ã¼': 'ü'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasını işle\"\"\"\n",
    "        print(f\"CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlığını kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ HATA: Dosya bulunamadı: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"✅ UTF-8 encoding ile başarıyla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"✅ Latin-1 encoding ile başarıyla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satır sayısı: {len(df)}\")\n",
    "        print(f\"Sütunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Boş chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Boş metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e çevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluşturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity için normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yükle\"\"\"\n",
    "        print(\"Qdrant'a yükleme başlıyor...\")\n",
    "        \n",
    "        # Embeddings oluştur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluştur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"daire\": str(row['daire']),\n",
    "                \"mahkeme\": str(row['mahkeme']),\n",
    "                \"karar_turu\": str(row['karar_turu']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \"chunk_index\": int(row['chunk_index']),\n",
    "                \"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"chunk_length\": int(row['chunk_length'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yükleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Yükleme hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} doküman başarıyla yüklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapılıyor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluştur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Gelişmiş filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter oluştur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'daire' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"daire\",\n",
    "                    match=models.MatchValue(value=filters['daire'])\n",
    "                ))\n",
    "            \n",
    "            if 'karar_turu' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"karar_turu\", \n",
    "                    match=models.MatchValue(value=filters['karar_turu'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributeları kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LegalDocumentVectorDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m     evaluator\u001b[38;5;241m.\u001b[39mevaluate_rag(output_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/real_csv_rag_evaluation.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Qdrant veri tabanı\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mLegalDocumentVectorDB\u001b[49m(\n\u001b[1;32m     82\u001b[0m         qdrant_url\u001b[38;5;241m=\u001b[39mqdrant_url,\n\u001b[1;32m     83\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m     84\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhukuki_kararlar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     db\u001b[38;5;241m.\u001b[39mcreate_collection(recreate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(CSV_FILE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LegalDocumentVectorDB' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "\n",
    "# Qdrant tarafı (LegalDocumentVectorDB daha önce tanımladığın class)\n",
    "# from your_module import LegalDocumentVectorDB\n",
    "\n",
    "# ENV yükleme\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY bulunamadı. qdrant.env dosyasını kontrol et.\")\n",
    "\n",
    "CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"\n",
    "\n",
    "# ---------------- RAG Evaluator ---------------- #\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def load_csv(self):\n",
    "        print(f\"📂 CSV okunuyor: {self.csv_path}\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        print(f\"✅ Toplam satır: {len(self.df)}\")\n",
    "        self.df[\"contexts\"] = self.df.apply(\n",
    "            lambda row: [\n",
    "                f\"Daire: {row['daire']}\",\n",
    "                f\"Mahkeme: {row['mahkeme']}\",\n",
    "                f\"Karar Türü: {row['karar_turu']}\"\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        self.df[\"question\"] = self.df[\"chunk_text\"]\n",
    "        self.df[\"ground_truth\"] = self.df[\"esasNo\"].astype(str)\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"CSV yüklenmemiş. Önce load_csv() çağır.\")\n",
    "        self.dataset = Dataset.from_pandas(\n",
    "            self.df[[\"question\", \"chunk_text\", \"contexts\", \"ground_truth\"]].rename(\n",
    "                columns={\"chunk_text\": \"answer\"}\n",
    "            )\n",
    "        )\n",
    "        print(\"📊 Dataset hazır.\")\n",
    "\n",
    "    def evaluate_rag(self, output_csv: str = \"/tmp/rag_evaluation_results.csv\"):\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset hazırlanmadı. Önce prepare_dataset() çağır.\")\n",
    "        print(\"🔍 RAG değerlendirmesi başlatılıyor...\")\n",
    "        results = evaluate(\n",
    "            self.dataset,\n",
    "            metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "        )\n",
    "        # EvaluationResult nesnesinden dict çıkar\n",
    "        scores = {metric: results[metric] for metric in [\"faithfulness\", \"answer_relevancy\", \"context_precision\", \"context_recall\"]}\n",
    "        print(\"\\n============================================================\")\n",
    "        print(\"📈 RAG DEĞERLENDİRME SONUÇLARI\")\n",
    "        print(\"============================================================\")\n",
    "        for metric, score in scores.items():\n",
    "            print(f\"{metric:20} : {score:.4f}\")\n",
    "        print(\"============================================================\")\n",
    "        # Detaylı sonuçları CSV’ye kaydet\n",
    "        self.df.to_csv(output_csv, index=False)\n",
    "        print(f\"💾 Detaylı sonuçlar kaydedildi: {output_csv}\")\n",
    "        return scores\n",
    "\n",
    "# ---------------- Main ---------------- #\n",
    "def main():\n",
    "    # Qdrant veri tabanı\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=qdrant_url,\n",
    "        api_key=api_key,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    db.create_collection(recreate=True)\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    db.upload_to_qdrant(df, batch_size=50)\n",
    "\n",
    "    # Collection bilgilerini göster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Qdrant Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "\n",
    "    # Örnek arama\n",
    "    print(\"\\n=== Örnek Aramalar ===\")\n",
    "    results = db.search(\"ihtiyati tedbir tazminat\", limit=3, score_threshold=0.6)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Sonuç (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Daire: {result['payload']['daire']}\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "\n",
    "    # RAG Değerlendirmesi\n",
    "    evaluator = RAGEvaluator(CSV_FILE)\n",
    "    evaluator.load_csv()\n",
    "    evaluator.prepare_dataset()\n",
    "    evaluator.evaluate_rag(output_csv=\"/tmp/real_csv_rag_evaluation.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
