{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ GerÃ§ek CSV ile RAG DeÄŸerlendirmesi\n",
      "============================================================\n",
      "ğŸ“‚ GerÃ§ek CSV okunuyor: /home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\n",
      "âœ… CSV yÃ¼klendi: 137 satÄ±r\n",
      "ğŸ“‹ CSV SÃ¼tunlarÄ±: ['_id', 'location', 'esasNo', 'kararNo', 'extractedDates', 'esasNo_num', 'esasNo_tip', 'kararNo_num', 'kararNo_tip', 'daire', 'mahkeme', 'karar_turu', 'chunk_id', 'chunk_index', 'total_chunks', 'chunk_text', 'chunk_length']\n",
      "ğŸ“Š Ä°lk 3 satÄ±r Ã¶rneÄŸi:\n",
      "                        _id        location        esasNo      kararNo  \\\n",
      "0  6750524485b6640290c37b06  6.HukukDairesi   2023/596 E.  2024/257 K.   \n",
      "1  6750524485b6640290c37b07  6.HukukDairesi  2022/3281 E.  2024/117 K.   \n",
      "2  6750524485b6640290c37b07  6.HukukDairesi  2022/3281 E.  2024/117 K.   \n",
      "\n",
      "                                      extractedDates esasNo_num esasNo_tip  \\\n",
      "0                                         2024-01-18   2023/596          E   \n",
      "1  2009-06-26,2009-12-25,2009-12-31,2010-01-04,20...  2022/3281          E   \n",
      "2  2009-06-26,2009-12-25,2009-12-31,2010-01-04,20...  2022/3281          E   \n",
      "\n",
      "  kararNo_num kararNo_tip             daire  \\\n",
      "0    2024/257           K  6. Hukuk Dairesi   \n",
      "1    2024/117           K  6. Hukuk Dairesi   \n",
      "2    2024/117           K  6. Hukuk Dairesi   \n",
      "\n",
      "                                             mahkeme karar_turu  \\\n",
      "0  [...] BÃ¶lge Adliye Mahkemesi 13. Hukuk Dairesi...        RED   \n",
      "1              Asliye Hukuk Mahkemesi Taraflar arasÄ±        RED   \n",
      "2              Asliye Hukuk Mahkemesi Taraflar arasÄ±        RED   \n",
      "\n",
      "                           chunk_id  chunk_index  total_chunks  \\\n",
      "0  6750524485b6640290c37b06_chunk_1            1             1   \n",
      "1  6750524485b6640290c37b07_chunk_1            1            13   \n",
      "2  6750524485b6640290c37b07_chunk_2            2            13   \n",
      "\n",
      "                                          chunk_text  chunk_length  \n",
      "0  6. Hukuk Dairesi 2023/596 E. , 2024/257 K. \\n ...          1095  \n",
      "1  6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n...          1071  \n",
      "2  . maddesi, ihtiyati tedbir kararÄ±nÄ±n haksÄ±z ol...          1061  \n",
      "ğŸ”½ Sample alÄ±ndÄ±: 5 satÄ±r\n",
      "\n",
      "ğŸ” 5 Ã¶rnek deÄŸerlendiriliyor...\n",
      "â³ Ä°ÅŸleniyor: 106/5 - ID: 2023/4278 E.\n",
      "â³ Ä°ÅŸleniyor: 105/5 - ID: 2024/229 E.\n",
      "â³ Ä°ÅŸleniyor: 13/5 - ID: 2022/3281 E.\n",
      "â³ Ä°ÅŸleniyor: 27/5 - ID: 2023/4229 E.\n",
      "â³ Ä°ÅŸleniyor: 124/5 - ID: 2022/4331 E.\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ GERÃ‡EK CSV DEÄERLENDÄ°RME SONUÃ‡LARI\n",
      "============================================================\n",
      "faithfulness        : 0.1073\n",
      "answer_relevancy    : 0.2651\n",
      "context_precision   : 0.0000\n",
      "context_recall      : 0.0000\n",
      "\n",
      "ğŸ“Š En yÃ¼ksek faithfulness skoru: 0.2188\n",
      "ğŸ“Š En dÃ¼ÅŸÃ¼k faithfulness skoru: 0.0389\n",
      "\n",
      "ğŸ’¾ DetaylÄ± sonuÃ§lar kaydedildi: /tmp/real_csv_rag_evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "class RealCSVRAGEvaluator:\n",
    "    def __init__(self, csv_path: str, sample_size: int = 5):\n",
    "        \"\"\"\n",
    "        GerÃ§ek CSV dosyanÄ±zÄ± okuyarak RAG deÄŸerlendirmesi yapar\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.sample_size = sample_size\n",
    "        self.df = None\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    def load_real_csv(self):\n",
    "        \"\"\"GerÃ§ek CSV dosyanÄ±zÄ± okur ve iÅŸler\"\"\"\n",
    "        print(f\"ğŸ“‚ GerÃ§ek CSV okunuyor: {self.csv_path}\")\n",
    "        \n",
    "        if not os.path.exists(self.csv_path):\n",
    "            raise FileNotFoundError(f\"CSV dosyasÄ± bulunamadÄ±: {self.csv_path}\")\n",
    "        \n",
    "        # CSV'yi oku\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        print(f\"âœ… CSV yÃ¼klendi: {len(self.df)} satÄ±r\")\n",
    "        \n",
    "        # SÃ¼tunlarÄ± kontrol et ve yazdÄ±r\n",
    "        print(f\"ğŸ“‹ CSV SÃ¼tunlarÄ±: {list(self.df.columns)}\")\n",
    "        print(f\"ğŸ“Š Ä°lk 3 satÄ±r Ã¶rneÄŸi:\")\n",
    "        print(self.df.head(3))\n",
    "        \n",
    "        # Gerekli sÃ¼tunlarÄ± hazÄ±rla\n",
    "        self.prepare_evaluation_columns()\n",
    "        \n",
    "        # Sample al (bÃ¼yÃ¼k dosyalar iÃ§in)\n",
    "        if self.sample_size and len(self.df) > self.sample_size:\n",
    "            self.df = self.df.sample(self.sample_size, random_state=42)\n",
    "            print(f\"ğŸ”½ Sample alÄ±ndÄ±: {len(self.df)} satÄ±r\")\n",
    "    \n",
    "    def prepare_evaluation_columns(self):\n",
    "        \"\"\"RAG deÄŸerlendirmesi iÃ§in gerekli sÃ¼tunlarÄ± hazÄ±rlar\"\"\"\n",
    "        \n",
    "        # Context sÃ¼tunu oluÅŸtur (mevcut bilgilerden)\n",
    "        if 'contexts' not in self.df.columns:\n",
    "            context_parts = []\n",
    "            if 'daire' in self.df.columns:\n",
    "                context_parts.append(f\"Daire: {self.df['daire']}\")\n",
    "            if 'mahkeme' in self.df.columns:\n",
    "                context_parts.append(f\"Mahkeme: {self.df['mahkeme']}\")\n",
    "            if 'karar_turu' in self.df.columns:\n",
    "                context_parts.append(f\"Karar TÃ¼rÃ¼: {self.df['karar_turu']}\")\n",
    "            \n",
    "            self.df[\"contexts\"] = self.df.apply(\n",
    "                lambda row: [\n",
    "                    f\"Daire: {row.get('daire', 'Bilinmeyen')}\" if 'daire' in self.df.columns else \"\",\n",
    "                    f\"Mahkeme: {row.get('mahkeme', 'Bilinmeyen')}\" if 'mahkeme' in self.df.columns else \"\",\n",
    "                    f\"Karar TÃ¼rÃ¼: {row.get('karar_turu', 'Bilinmeyen')}\" if 'karar_turu' in self.df.columns else \"\"\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        # Question sÃ¼tunu oluÅŸtur\n",
    "        if 'question' not in self.df.columns:\n",
    "            if 'chunk_text' in self.df.columns:\n",
    "                # chunk_text'den soru oluÅŸtur\n",
    "                self.df['question'] = self.df['chunk_text'].apply(\n",
    "                    lambda text: f\"Bu hukuki karar hakkÄ±nda aÃ§Ä±klama yapÄ±nÄ±z: {text[:100]}...\"\n",
    "                )\n",
    "            else:\n",
    "                # Genel sorular oluÅŸtur\n",
    "                self.df['question'] = \"Bu hukuki karar hakkÄ±nda bilgi veriniz.\"\n",
    "        \n",
    "        # Answer sÃ¼tunu (chunk_text'i kullan)\n",
    "        if 'answer' not in self.df.columns:\n",
    "            if 'chunk_text' in self.df.columns:\n",
    "                self.df['answer'] = self.df['chunk_text']\n",
    "            else:\n",
    "                raise ValueError(\"'chunk_text' sÃ¼tunu bulunamadÄ±!\")\n",
    "        \n",
    "        # Ground truth sÃ¼tunu\n",
    "        if 'ground_truth' not in self.df.columns:\n",
    "            if 'esasNo' in self.df.columns:\n",
    "                self.df['ground_truth'] = self.df['esasNo'].astype(str)\n",
    "            elif 'kararNo' in self.df.columns:\n",
    "                self.df['ground_truth'] = self.df['kararNo'].astype(str)\n",
    "            else:\n",
    "                # Fallback: index kullan\n",
    "                self.df['ground_truth'] = self.df.index.astype(str)\n",
    "    \n",
    "    def evaluate_faithfulness(self, answer: str, contexts: list) -> float:\n",
    "        \"\"\"CevabÄ±n context'e ne kadar sadÄ±k olduÄŸunu Ã¶lÃ§er\"\"\"\n",
    "        if not contexts or not answer:\n",
    "            return 0.0\n",
    "        \n",
    "        # Context'leri birleÅŸtir ve temizle\n",
    "        combined_context = ' '.join([ctx for ctx in contexts if ctx.strip()])\n",
    "        \n",
    "        if not combined_context.strip():\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # TF-IDF ile benzerlik hesapla\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([answer, combined_context])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return min(similarity, 1.0)\n",
    "        except:\n",
    "            # Fallback: Kelime Ã¶rtÃ¼ÅŸmesi\n",
    "            answer_words = set(answer.lower().split())\n",
    "            context_words = set(combined_context.lower().split())\n",
    "            if not answer_words:\n",
    "                return 0.0\n",
    "            overlap = len(answer_words.intersection(context_words))\n",
    "            return overlap / len(answer_words)\n",
    "    \n",
    "    def evaluate_relevancy(self, question: str, answer: str) -> float:\n",
    "        \"\"\"CevabÄ±n soruyla ne kadar ilgili olduÄŸunu Ã¶lÃ§er\"\"\"\n",
    "        if not question or not answer:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([question, answer])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return min(similarity, 1.0)\n",
    "        except:\n",
    "            # Fallback: Kelime Ã¶rtÃ¼ÅŸmesi\n",
    "            q_words = set(question.lower().split())\n",
    "            a_words = set(answer.lower().split())\n",
    "            if not q_words:\n",
    "                return 0.0\n",
    "            overlap = len(q_words.intersection(a_words))\n",
    "            return overlap / len(q_words)\n",
    "    \n",
    "    def evaluate_context_precision(self, contexts: list, ground_truth: str) -> float:\n",
    "        \"\"\"Context'in ne kadar kesin/ilgili olduÄŸunu Ã¶lÃ§er\"\"\"\n",
    "        if not contexts or not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_contexts = 0\n",
    "        valid_contexts = [ctx for ctx in contexts if ctx.strip()]\n",
    "        \n",
    "        if not valid_contexts:\n",
    "            return 0.0\n",
    "        \n",
    "        for context in valid_contexts:\n",
    "            if ground_truth.lower() in context.lower():\n",
    "                relevant_contexts += 1\n",
    "            elif any(word in context.lower() for word in ground_truth.lower().split() if len(word) > 2):\n",
    "                relevant_contexts += 0.5\n",
    "        \n",
    "        return min(relevant_contexts / len(valid_contexts), 1.0)\n",
    "    \n",
    "    def evaluate_context_recall(self, contexts: list, ground_truth: str) -> float:\n",
    "        \"\"\"Ground truth'un context'te ne kadar coverage'Ä± var\"\"\"\n",
    "        if not contexts or not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        combined_context = ' '.join([ctx for ctx in contexts if ctx.strip()]).lower()\n",
    "        gt_words = set(ground_truth.lower().split())\n",
    "        \n",
    "        if not gt_words or not combined_context:\n",
    "            return 0.0\n",
    "        \n",
    "        found_words = sum(1 for word in gt_words if len(word) > 2 and word in combined_context)\n",
    "        return found_words / len(gt_words) if gt_words else 0.0\n",
    "    \n",
    "    def evaluate_dataset(self) -> dict:\n",
    "        \"\"\"Dataset'teki tÃ¼m Ã¶rnekleri deÄŸerlendirir\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"CSV yÃ¼klenmemiÅŸ! Ã–nce load_real_csv() Ã§aÄŸÄ±rÄ±n.\")\n",
    "        \n",
    "        results = {\n",
    "            'faithfulness': [],\n",
    "            'answer_relevancy': [],\n",
    "            'context_precision': [],\n",
    "            'context_recall': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ” {len(self.df)} Ã¶rnek deÄŸerlendiriliyor...\")\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            print(f\"â³ Ä°ÅŸleniyor: {idx+1}/{len(self.df)} - ID: {row.get('esasNo', idx)}\")\n",
    "            \n",
    "            question = str(row['question'])\n",
    "            answer = str(row['answer'])\n",
    "            contexts = row['contexts'] if isinstance(row['contexts'], list) else [str(row['contexts'])]\n",
    "            ground_truth = str(row['ground_truth'])\n",
    "            \n",
    "            # Debug bilgileri\n",
    "            if idx == 0:\n",
    "                print(f\"ğŸ“ Ã–rnek veri:\")\n",
    "                print(f\"   Soru: {question[:100]}...\")\n",
    "                print(f\"   Cevap: {answer[:100]}...\")\n",
    "                print(f\"   Context: {contexts[:2]}\")\n",
    "                print(f\"   Ground Truth: {ground_truth}\")\n",
    "            \n",
    "            # Metrikleri hesapla\n",
    "            faithfulness = self.evaluate_faithfulness(answer, contexts)\n",
    "            relevancy = self.evaluate_relevancy(question, answer)\n",
    "            precision = self.evaluate_context_precision(contexts, ground_truth)\n",
    "            recall = self.evaluate_context_recall(contexts, ground_truth)\n",
    "            \n",
    "            results['faithfulness'].append(faithfulness)\n",
    "            results['answer_relevancy'].append(relevancy)\n",
    "            results['context_precision'].append(precision)\n",
    "            results['context_recall'].append(recall)\n",
    "        \n",
    "        # Ortalama skorlarÄ± hesapla\n",
    "        avg_results = {\n",
    "            metric: np.mean(scores) for metric, scores in results.items()\n",
    "        }\n",
    "        \n",
    "        return avg_results, results\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸš€ GerÃ§ek CSV ile RAG DeÄŸerlendirmesi\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # GerÃ§ek CSV dosya yolu (sizinkini kullanÄ±n)\n",
    "    CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Evaluator'Ã¼ baÅŸlat\n",
    "        evaluator = RealCSVRAGEvaluator(CSV_FILE, sample_size=5)  # 5 Ã¶rnek test\n",
    "        \n",
    "        # GerÃ§ek CSV'yi yÃ¼kle\n",
    "        evaluator.load_real_csv()\n",
    "        \n",
    "        # DeÄŸerlendirmeyi Ã§alÄ±ÅŸtÄ±r\n",
    "        avg_results, detailed_results = evaluator.evaluate_dataset()\n",
    "        \n",
    "        # SonuÃ§larÄ± gÃ¶ster\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“ˆ GERÃ‡EK CSV DEÄERLENDÄ°RME SONUÃ‡LARI\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for metric, score in avg_results.items():\n",
    "            print(f\"{metric:20}: {score:.4f}\")\n",
    "        \n",
    "        # En iyi ve en kÃ¶tÃ¼ Ã¶rnekleri gÃ¶ster\n",
    "        print(f\"\\nğŸ“Š En yÃ¼ksek faithfulness skoru: {max(detailed_results['faithfulness']):.4f}\")\n",
    "        print(f\"ğŸ“Š En dÃ¼ÅŸÃ¼k faithfulness skoru: {min(detailed_results['faithfulness']):.4f}\")\n",
    "        \n",
    "        # SonuÃ§larÄ± kaydet\n",
    "        results_df = pd.DataFrame(detailed_results)\n",
    "        output_file = \"/tmp/real_csv_rag_evaluation.csv\"\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nğŸ’¾ DetaylÄ± sonuÃ§lar kaydedildi: {output_file}\")\n",
    "        \n",
    "        return avg_results\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Dosya hatasÄ±: {e}\")\n",
    "        print(\"CSV dosya yolunu kontrol edin!\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DeÄŸerlendirme hatasÄ±: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam satÄ±r: 137\n",
      "Sample alÄ±ndÄ±: 50 satÄ±r\n",
      "Q&A dataset oluÅŸturuldu ve kaydedildi: /home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_qa_dataset.csv\n",
      "                                              question  \\\n",
      "105  6. Hukuk Dairesi tarafÄ±ndan verilen RED kararÄ±...   \n",
      "104  6. Hukuk Dairesi tarafÄ±ndan verilen RED kararÄ±...   \n",
      "12   Asliye Hukuk Mahkemesi Taraflar arasÄ± kararÄ±na...   \n",
      "26   Esas No 2023/4229 E. ve Karar No 2024/136 K. i...   \n",
      "123  [...] BÃ¶lge Adliye Mahkemesi 46. Hukuk Dairesi...   \n",
      "\n",
      "                                                answer  \\\n",
      "105  6. Hukuk Dairesi 2023/4278 E. , 2024/689 K. \\n...   \n",
      "104  . DavalÄ± vekili 19.12.2023 tarihli dilekÃ§esi i...   \n",
      "12   . 2. Ä°lgili Hukuk 6100 sayÄ±lÄ± Hukuk Muhakemele...   \n",
      "26   . B. Ä°stinaf Sebepleri AlacaklÄ± [...] [...] TÄ±...   \n",
      "123  . III. Ä°LK DERECE MAHKEMESÄ° KARARI Ä°lk Derece ...   \n",
      "\n",
      "                                               context        esasNo  \\\n",
      "105  6. Hukuk Dairesi 2023/4278 E. , 2024/689 K. \\n...  2023/4278 E.   \n",
      "104  . DavalÄ± vekili 19.12.2023 tarihli dilekÃ§esi i...   2024/229 E.   \n",
      "12   . 2. Ä°lgili Hukuk 6100 sayÄ±lÄ± Hukuk Muhakemele...  2022/3281 E.   \n",
      "26   . B. Ä°stinaf Sebepleri AlacaklÄ± [...] [...] TÄ±...  2023/4229 E.   \n",
      "123  . III. Ä°LK DERECE MAHKEMESÄ° KARARI Ä°lk Derece ...  2022/4331 E.   \n",
      "\n",
      "         kararNo             daire  \\\n",
      "105  2024/689 K.  6. Hukuk Dairesi   \n",
      "104  2024/489 K.  6. Hukuk Dairesi   \n",
      "12   2024/117 K.  6. Hukuk Dairesi   \n",
      "26   2024/136 K.  6. Hukuk Dairesi   \n",
      "123  2024/516 K.  6. Hukuk Dairesi   \n",
      "\n",
      "                                               mahkeme karar_turu  \n",
      "105  Asliye Hukuk Mahkemesi - K A R A R - DavacÄ± ve...        RED  \n",
      "104  Asliye Hukuk Mahkemesi DavacÄ±lar vekili dava d...        RED  \n",
      "12               Asliye Hukuk Mahkemesi Taraflar arasÄ±        RED  \n",
      "26   [...] BÃ¶lge Adliye Mahkemesi 17. Hukuk Dairesi...        RED  \n",
      "123  [...] BÃ¶lge Adliye Mahkemesi 46. Hukuk Dairesi...        RED  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_qa_dataset(csv_path, output_path, sample_size=None, seed=42):\n",
    "    \"\"\"\n",
    "    CSV'den otomatik Q&A dataset Ã¼retir.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Chunk CSV dosyasÄ±\n",
    "        output_path (str): OluÅŸturulacak CSV dosyasÄ±\n",
    "        sample_size (int, optional): KaÃ§ satÄ±r Ã¶rnek alÄ±nacak. Default tÃ¼m veri.\n",
    "        seed (int): Random seed\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Toplam satÄ±r: {len(df)}\")\n",
    "\n",
    "    # Sample al\n",
    "    if sample_size and len(df) > sample_size:\n",
    "        df = df.sample(sample_size, random_state=seed)\n",
    "        print(f\"Sample alÄ±ndÄ±: {len(df)} satÄ±r\")\n",
    "\n",
    "    # Soru oluÅŸturma (basit template)\n",
    "    def create_question(row):\n",
    "        # Ã–rnek soru ÅŸablonlarÄ±\n",
    "        templates = [\n",
    "            f\"{row['daire']} tarafÄ±ndan verilen {row['karar_turu']} kararÄ± hakkÄ±nda bilgi veriniz.\",\n",
    "            f\"Esas No {row['esasNo']} ve Karar No {row['kararNo']} ile ilgili kararda ne sÃ¶yleniyor?\",\n",
    "            f\"{row['mahkeme']} kararÄ±na gÃ¶re durum nedir?\",\n",
    "            f\"{row['daire']} {row['karar_turu']} kararÄ±nda ne karar verilmiÅŸ?\"\n",
    "        ]\n",
    "        return random.choice(templates)\n",
    "\n",
    "    df['question'] = df.apply(create_question, axis=1)\n",
    "    df['answer'] = df['chunk_text']\n",
    "    df['context'] = df['chunk_text']  # RAG iÃ§in context olarak chunk'Ä± kullanabiliriz\n",
    "\n",
    "    # Sadece gerekli sÃ¼tunlarÄ± seÃ§\n",
    "    qa_df = df[['question', 'answer', 'context', 'esasNo', 'kararNo', 'daire', 'mahkeme', 'karar_turu']]\n",
    "\n",
    "    # Kaydet\n",
    "    qa_df.to_csv(output_path, index=False)\n",
    "    print(f\"Q&A dataset oluÅŸturuldu ve kaydedildi: {output_path}\")\n",
    "\n",
    "    return qa_df\n",
    "\n",
    "\n",
    "# KullanÄ±m Ã¶rneÄŸi\n",
    "csv_file = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"\n",
    "output_file = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_qa_dataset.csv\"\n",
    "qa_df = generate_qa_dataset(csv_file, output_file, sample_size=50)\n",
    "print(qa_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLegalDocumentVectorDB\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, qdrant_url: \u001b[38;5;28mstr\u001b[39m, api_key: \u001b[38;5;28mstr\u001b[39m, collection_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhukuki kararlar\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m        Hukuki belgeler iÃ§in vector database sÄ±nÄ±fÄ±\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m            collection_name: Collection adÄ±\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mLegalDocumentVectorDB\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_csv\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"CSV dosyasÄ±nÄ± iÅŸle\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV dosyasÄ± okunuyor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler iÃ§in vector database sÄ±nÄ±fÄ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarÄ±  \n",
    "            collection_name: Collection adÄ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ multilingual model\n",
    "        print(\"Sentence Transformer modeli yÃ¼kleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluÅŸtur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluÅŸturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarÄ±nÄ± dÃ¼zelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'ÃƒÂ¤': 'Ã¤', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÅ¸': 'ÃŸ',\n",
    "            'Ãƒâ€¡': 'Ã‡', 'Ã„Â±': 'Ä±', 'Ã„Â°': 'Ä°', 'Ã…Å¸': 'ÅŸ',\n",
    "            'Ã„\\x9f': 'ÄŸ', 'ÃƒÂ§': 'Ã§', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasÄ±nÄ± iÅŸle\"\"\"\n",
    "        print(f\"CSV dosyasÄ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlÄ±ÄŸÄ±nÄ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âŒ HATA: Dosya bulunamadÄ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"âœ… UTF-8 encoding ile baÅŸarÄ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"âœ… Latin-1 encoding ile baÅŸarÄ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satÄ±r sayÄ±sÄ±: {len(df)}\")\n",
    "        print(f\"SÃ¼tunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # BoÅŸ chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"BoÅŸ metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e Ã§evir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluÅŸturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity iÃ§in normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yÃ¼kle\"\"\"\n",
    "        print(\"Qdrant'a yÃ¼kleme baÅŸlÄ±yor...\")\n",
    "        \n",
    "        # Embeddings oluÅŸtur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluÅŸtur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"daire\": str(row['daire']),\n",
    "                \"mahkeme\": str(row['mahkeme']),\n",
    "                \"karar_turu\": str(row['karar_turu']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \"chunk_index\": int(row['chunk_index']),\n",
    "                \"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"chunk_length\": int(row['chunk_length'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yÃ¼kle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yÃ¼kleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"YÃ¼kleme hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dokÃ¼man baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapÄ±lÄ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluÅŸtur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"GeliÅŸmiÅŸ filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter oluÅŸtur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'daire' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"daire\",\n",
    "                    match=models.MatchValue(value=filters['daire'])\n",
    "                ))\n",
    "            \n",
    "            if 'karar_turu' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"karar_turu\", \n",
    "                    match=models.MatchValue(value=filters['karar_turu'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarÄ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LegalDocumentVectorDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m     evaluator\u001b[38;5;241m.\u001b[39mevaluate_rag(output_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/real_csv_rag_evaluation.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Qdrant veri tabanÄ±\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mLegalDocumentVectorDB\u001b[49m(\n\u001b[1;32m     82\u001b[0m         qdrant_url\u001b[38;5;241m=\u001b[39mqdrant_url,\n\u001b[1;32m     83\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m     84\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhukuki_kararlar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     db\u001b[38;5;241m.\u001b[39mcreate_collection(recreate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(CSV_FILE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LegalDocumentVectorDB' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "\n",
    "# Qdrant tarafÄ± (LegalDocumentVectorDB daha Ã¶nce tanÄ±mladÄ±ÄŸÄ±n class)\n",
    "# from your_module import LegalDocumentVectorDB\n",
    "\n",
    "# ENV yÃ¼kleme\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY bulunamadÄ±. qdrant.env dosyasÄ±nÄ± kontrol et.\")\n",
    "\n",
    "CSV_FILE = \"/home/yapayzeka/ahsen_bulbul/model/langchain/recursive/yargitay_chunks.csv\"\n",
    "\n",
    "# ---------------- RAG Evaluator ---------------- #\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def load_csv(self):\n",
    "        print(f\"ğŸ“‚ CSV okunuyor: {self.csv_path}\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        print(f\"âœ… Toplam satÄ±r: {len(self.df)}\")\n",
    "        self.df[\"contexts\"] = self.df.apply(\n",
    "            lambda row: [\n",
    "                f\"Daire: {row['daire']}\",\n",
    "                f\"Mahkeme: {row['mahkeme']}\",\n",
    "                f\"Karar TÃ¼rÃ¼: {row['karar_turu']}\"\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        self.df[\"question\"] = self.df[\"chunk_text\"]\n",
    "        self.df[\"ground_truth\"] = self.df[\"esasNo\"].astype(str)\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"CSV yÃ¼klenmemiÅŸ. Ã–nce load_csv() Ã§aÄŸÄ±r.\")\n",
    "        self.dataset = Dataset.from_pandas(\n",
    "            self.df[[\"question\", \"chunk_text\", \"contexts\", \"ground_truth\"]].rename(\n",
    "                columns={\"chunk_text\": \"answer\"}\n",
    "            )\n",
    "        )\n",
    "        print(\"ğŸ“Š Dataset hazÄ±r.\")\n",
    "\n",
    "    def evaluate_rag(self, output_csv: str = \"/tmp/rag_evaluation_results.csv\"):\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset hazÄ±rlanmadÄ±. Ã–nce prepare_dataset() Ã§aÄŸÄ±r.\")\n",
    "        print(\"ğŸ” RAG deÄŸerlendirmesi baÅŸlatÄ±lÄ±yor...\")\n",
    "        results = evaluate(\n",
    "            self.dataset,\n",
    "            metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "        )\n",
    "        # EvaluationResult nesnesinden dict Ã§Ä±kar\n",
    "        scores = {metric: results[metric] for metric in [\"faithfulness\", \"answer_relevancy\", \"context_precision\", \"context_recall\"]}\n",
    "        print(\"\\n============================================================\")\n",
    "        print(\"ğŸ“ˆ RAG DEÄERLENDÄ°RME SONUÃ‡LARI\")\n",
    "        print(\"============================================================\")\n",
    "        for metric, score in scores.items():\n",
    "            print(f\"{metric:20} : {score:.4f}\")\n",
    "        print(\"============================================================\")\n",
    "        # DetaylÄ± sonuÃ§larÄ± CSVâ€™ye kaydet\n",
    "        self.df.to_csv(output_csv, index=False)\n",
    "        print(f\"ğŸ’¾ DetaylÄ± sonuÃ§lar kaydedildi: {output_csv}\")\n",
    "        return scores\n",
    "\n",
    "# ---------------- Main ---------------- #\n",
    "def main():\n",
    "    # Qdrant veri tabanÄ±\n",
    "    db = LegalDocumentVectorDB(\n",
    "        qdrant_url=qdrant_url,\n",
    "        api_key=api_key,\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "    db.create_collection(recreate=True)\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    db.upload_to_qdrant(df, batch_size=50)\n",
    "\n",
    "    # Collection bilgilerini gÃ¶ster\n",
    "    info = db.get_collection_info()\n",
    "    print(\"\\n=== Qdrant Collection Bilgileri ===\")\n",
    "    print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "\n",
    "    # Ã–rnek arama\n",
    "    print(\"\\n=== Ã–rnek Aramalar ===\")\n",
    "    results = db.search(\"ihtiyati tedbir tazminat\", limit=3, score_threshold=0.6)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. SonuÃ§ (Skor: {result['score']:.3f})\")\n",
    "        print(f\"   Daire: {result['payload']['daire']}\")\n",
    "        print(f\"   Esas No: {result['payload']['esas_no']}\")\n",
    "        print(f\"   Karar No: {result['payload']['karar_no']}\")\n",
    "        print(f\"   Metin: {result['payload']['chunk_text'][:200]}...\")\n",
    "\n",
    "    # RAG DeÄŸerlendirmesi\n",
    "    evaluator = RAGEvaluator(CSV_FILE)\n",
    "    evaluator.load_csv()\n",
    "    evaluator.prepare_dataset()\n",
    "    evaluator.evaluate_rag(output_csv=\"/tmp/real_csv_rag_evaluation.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
