{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e1d91-935a-4ede-aec3-5518319ae62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78836350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler için vector database sınıfı\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarı  \n",
    "            collection_name: Collection adı\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Türkçe için optimize edilmiş multilingual model\n",
    "        print(\"Sentence Transformer modeli yükleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model başarıyla yüklendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluştur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluşturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarını düzelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'Ã¤': 'ä', 'Ã¶': 'ö', 'Ã¼': 'ü', 'ÃŸ': 'ß',\n",
    "            'Ã‡': 'Ç', 'Ä±': 'ı', 'Ä°': 'İ', 'ÅŸ': 'ş',\n",
    "            'Ä\\x9f': 'ğ', 'Ã§': 'ç', 'Ã¶': 'ö', 'Ã¼': 'ü'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasını işle\"\"\"\n",
    "        print(f\"CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlığını kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ HATA: Dosya bulunamadı: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"✅ UTF-8 encoding ile başarıyla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"✅ Latin-1 encoding ile başarıyla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satır sayısı: {len(df)}\")\n",
    "        print(f\"Sütunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Boş chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Boş metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e çevir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluşturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity için normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yükle\"\"\"\n",
    "        print(\"Qdrant'a yükleme başlıyor...\")\n",
    "        \n",
    "        # Embeddings oluştur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluştur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"daire\": str(row['daire']),\n",
    "                \"mahkeme\": str(row['mahkeme']),\n",
    "                \"karar_turu\": str(row['karar_turu']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \"chunk_index\": int(row['chunk_index']),\n",
    "                \"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"chunk_length\": int(row['chunk_length'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yükleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Yükleme hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} doküman başarıyla yüklendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapılıyor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluştur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Gelişmiş filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter oluştur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'daire' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"daire\",\n",
    "                    match=models.MatchValue(value=filters['daire'])\n",
    "                ))\n",
    "            \n",
    "            if 'karar_turu' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"karar_turu\", \n",
    "                    match=models.MatchValue(value=filters['karar_turu'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributeları kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb206f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEvaluator:\n",
    "    def __init__(self, db: LegalDocumentVectorDB, queries_file: str):\n",
    "        self.db = db\n",
    "        with open(queries_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.queries = json.load(f)\n",
    "\n",
    "    def evaluate(self, limit: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Arama kalitesini ölç\"\"\"\n",
    "        results_summary = []\n",
    "\n",
    "        for q in self.queries:\n",
    "            query_text = q[\"query\"]\n",
    "            expected_keywords = q.get(\"expected_keywords\", [])\n",
    "\n",
    "            res = self.db.search(query_text, limit=limit)\n",
    "\n",
    "            # Basit keyword match skoru\n",
    "            hits = 0\n",
    "            for r in res:\n",
    "                text = r[\"payload\"][\"chunk_text\"].lower()\n",
    "                if any(kw.lower() in text for kw in expected_keywords):\n",
    "                    hits += 1\n",
    "\n",
    "            results_summary.append({\n",
    "                \"query\": query_text,\n",
    "                \"expected_keywords\": expected_keywords,\n",
    "                \"retrieved\": len(res),\n",
    "                \"hits\": hits,\n",
    "                \"hit_rate\": round(hits / max(1, len(res)), 3),\n",
    "                \"avg_score\": round(np.mean([r[\"score\"] for r in res]), 3) if res else 0\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(results_summary)\n",
    "        return df\n",
    "\n",
    "    def save_results(self, df: pd.DataFrame, filename: str = \"eval_results.csv\"):\n",
    "        df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"📊 Sonuçlar kaydedildi: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str,collection_name: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu başlıyor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandı!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKİ KARAR ARAMA SİSTEMİ\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediğiniz metni girin (çıkmak için 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"Kaç sonuç gösterilsin? (varsayılan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"❌ Sonuç bulunamadı.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n🔍 '{query}' için {len(results)} sonuç bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\n📄 {i}. SONUÇ (Benzerlik: {result['score']:.3f})\")\n",
    "                    print(f\"   📂 Daire: {payload['daire']}\")\n",
    "                    print(f\"   📋 Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   ⚖️ Karar No: {payload['karar_no']}\")\n",
    "                    print(f\"   🏛️ Mahkeme: {payload['mahkeme']}\")\n",
    "                    print(f\"   📅 Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   📝 Chunk: {payload['chunk_index']}/{payload['total_chunks']}\")\n",
    "                    print(f\"   📄 Metin Önizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b313e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717247/3988472342.py:11: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.12.4. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformer modeli yükleniyor...\n",
      "Model başarıyla yüklendi!\n",
      "Vector boyutu: 384\n",
      "Arama yapılıyor: 'Yargıtay 6. Hukuk Dairesi kira sözleşmesiyle ilgili hangi kararı verdi?'\n",
      "Arama yapılıyor: 'Müteselsil borç kavramı hangi kararda nasıl yorumlandı?'\n",
      "Arama yapılıyor: 'Yargıtay 13. Hukuk Dairesi'nin karar gerekçesi nedir?'\n",
      "Arama yapılıyor: 'Tazminat davalarında hangi esaslara göre hüküm verilmektedir?'\n",
      "Arama yapılıyor: 'Hangi davada taraflar arasında sözleşme feshi söz konusuydu?'\n",
      "Arama yapılıyor: 'Yargıtay hangi kararda usul hatası nedeniyle bozma kararı verdi?'\n",
      "Arama yapılıyor: 'Mahkeme kararında davacının talebi ne şekilde değerlendirilmiştir?'\n",
      "Arama yapılıyor: 'İş mahkemesi kararlarında işçi alacakları nasıl hükme bağlanıyor?'\n",
      "Arama yapılıyor: 'Hangi kararda icra takibine ilişkin hüküm yer alıyor?'\n",
      "Arama yapılıyor: 'Borçlu temerrüdü hangi içtihatta nasıl açıklanmıştır?'\n",
      "\n",
      "=== Evaluation Sonuçları ===\n",
      "                                               query  \\\n",
      "0  Yargıtay 6. Hukuk Dairesi kira sözleşmesiyle i...   \n",
      "1  Müteselsil borç kavramı hangi kararda nasıl yo...   \n",
      "2  Yargıtay 13. Hukuk Dairesi'nin karar gerekçesi...   \n",
      "3  Tazminat davalarında hangi esaslara göre hüküm...   \n",
      "4  Hangi davada taraflar arasında sözleşme feshi ...   \n",
      "5  Yargıtay hangi kararda usul hatası nedeniyle b...   \n",
      "6  Mahkeme kararında davacının talebi ne şekilde ...   \n",
      "7  İş mahkemesi kararlarında işçi alacakları nası...   \n",
      "8  Hangi kararda icra takibine ilişkin hüküm yer ...   \n",
      "9  Borçlu temerrüdü hangi içtihatta nasıl açıklan...   \n",
      "\n",
      "                               expected_keywords  retrieved  hits  hit_rate  \\\n",
      "0             [6. Hukuk Dairesi, kira, sözleşme]          5     5       1.0   \n",
      "1  [müteselsil borç, Borçlar Kanunu, sorumluluk]          4     0       0.0   \n",
      "2            [13. Hukuk Dairesi, gerekçe, karar]          5     5       1.0   \n",
      "3                        [tazminat, esas, hüküm]          5     2       0.4   \n",
      "4                       [fesih, sözleşme, taraf]          5     5       1.0   \n",
      "5                    [bozma, usul hatası, karar]          5     5       1.0   \n",
      "6                 [davacı, talep, değerlendirme]          5     5       1.0   \n",
      "7                   [işçi, alacak, iş mahkemesi]          5     5       1.0   \n",
      "8                           [icra, takip, hüküm]          5     2       0.4   \n",
      "9                    [borçlu, temerrüt, içtihat]          1     0       0.0   \n",
      "\n",
      "   avg_score  \n",
      "0      0.709  \n",
      "1      0.526  \n",
      "2      0.754  \n",
      "3      0.704  \n",
      "4      0.598  \n",
      "5      0.766  \n",
      "6      0.753  \n",
      "7      0.668  \n",
      "8      0.722  \n",
      "9      0.505  \n",
      "📊 Sonuçlar kaydedildi: eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mode = input(\"Mod seç (1=Interaktif Arama, 2=Evaluation): \")\n",
    "\n",
    "    searcher = InteractiveLegalSearch(\n",
    "        qdrant_url=\"https://qdrant.adalet.gov.tr:443\",\n",
    "        api_key=\"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\",\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "\n",
    "    if mode == \"1\":\n",
    "        searcher.interactive_search()\n",
    "    elif mode == \"2\":\n",
    "        evaluator = RetrievalEvaluator(searcher.db, \"queries.json\")\n",
    "        df = evaluator.evaluate(limit=5)\n",
    "        print(\"\\n=== Evaluation Sonuçları ===\")\n",
    "        print(df)\n",
    "        evaluator.save_results(df, \"eval_results.csv\")\n",
    "    else:\n",
    "        print(\"❌ Geçersiz seçim!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05689c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
