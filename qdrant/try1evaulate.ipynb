{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e1d91-935a-4ede-aec3-5518319ae62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from qdrant_client.http import models\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78836350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalDocumentVectorDB:\n",
    "    def __init__(self, qdrant_url: str, api_key: str, collection_name: str = \"hukuki kararlar\"):\n",
    "        \"\"\"\n",
    "        Hukuki belgeler iÃ§in vector database sÄ±nÄ±fÄ±\n",
    "        \n",
    "        Args:\n",
    "            qdrant_url: Qdrant sunucu URL'i\n",
    "            api_key: Qdrant API anahtarÄ±  \n",
    "            collection_name: Collection adÄ±\n",
    "        \"\"\"\n",
    "        self.client = QdrantClient(\n",
    "            url=qdrant_url,\n",
    "            api_key=api_key,\n",
    "            timeout=60\n",
    "        )\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ multilingual model\n",
    "        print(\"Sentence Transformer modeli yÃ¼kleniyor...\")\n",
    "        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        print(\"Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "        \n",
    "        # Vector boyutunu al\n",
    "        self.vector_size = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"Vector boyutu: {self.vector_size}\")\n",
    "\n",
    "    def create_collection(self, recreate: bool = False):\n",
    "        \"\"\"Collection oluÅŸtur\"\"\"\n",
    "        try:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name=self.collection_name)\n",
    "                print(f\"Eski collection '{self.collection_name}' silindi.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.vector_size, \n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"Collection '{self.collection_name}' oluÅŸturuldu.\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" in str(e).lower():\n",
    "                print(f\"Collection '{self.collection_name}' zaten mevcut.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding sorunlarÄ±nÄ± dÃ¼zelt\n",
    "        text = str(text)\n",
    "        replacements = {\n",
    "            'ÃƒÂ¤': 'Ã¤', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼', 'ÃƒÅ¸': 'ÃŸ',\n",
    "            'Ãƒâ€¡': 'Ã‡', 'Ã„Â±': 'Ä±', 'Ã„Â°': 'Ä°', 'Ã…Å¸': 'ÅŸ',\n",
    "            'Ã„\\x9f': 'ÄŸ', 'ÃƒÂ§': 'Ã§', 'ÃƒÂ¶': 'Ã¶', 'ÃƒÂ¼': 'Ã¼'\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process_csv(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"CSV dosyasÄ±nÄ± iÅŸle\"\"\"\n",
    "        print(f\"CSV dosyasÄ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        # Dosya varlÄ±ÄŸÄ±nÄ± kontrol et\n",
    "        import os\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âŒ HATA: Dosya bulunamadÄ±: {csv_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Encoding denemesi\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "                print(\"âœ… UTF-8 encoding ile baÅŸarÄ±yla okundu\")\n",
    "            except UnicodeDecodeError:\n",
    "                df = pd.read_csv(csv_path, encoding='latin-1')\n",
    "                print(\"âœ… Latin-1 encoding ile baÅŸarÄ±yla okundu\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Toplam satÄ±r sayÄ±sÄ±: {len(df)}\")\n",
    "        print(f\"SÃ¼tunlar: {df.columns.tolist()}\")\n",
    "        \n",
    "        # BoÅŸ chunk_text'leri filtrele\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['chunk_text'])\n",
    "        df = df[df['chunk_text'].str.strip() != '']\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"BoÅŸ metin filtrelemesi: {initial_count} -> {final_count}\")\n",
    "        \n",
    "        # Metinleri temizle\n",
    "        df['chunk_text_clean'] = df['chunk_text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def create_embeddings_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "        \"\"\"Metinleri batch halinde embedding'e Ã§evir\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding oluÅŸturuluyor\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch, \n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=False,\n",
    "                normalize_embeddings=True  # Cosine similarity iÃ§in normalize et\n",
    "            )\n",
    "            embeddings.extend(batch_embeddings.tolist())\n",
    "            print(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "    def upload_to_qdrant(self, df: pd.DataFrame, batch_size: int = 100):\n",
    "        \"\"\"DataFrame'i Qdrant'a yÃ¼kle\"\"\"\n",
    "        print(\"Qdrant'a yÃ¼kleme baÅŸlÄ±yor...\")\n",
    "        \n",
    "        # Embeddings oluÅŸtur\n",
    "        texts = df['chunk_text_clean'].tolist()\n",
    "        embeddings = self.create_embeddings_batch(texts, batch_size=32)\n",
    "        \n",
    "        # Points oluÅŸtur\n",
    "        points = []\n",
    "        for idx, (_, row) in enumerate(df.iterrows()):\n",
    "            payload = {\n",
    "                \"document_id\": str(row['_id']),\n",
    "                \"location\": str(row['location']),\n",
    "                \"esas_no\": str(row['esasNo']),\n",
    "                \"karar_no\": str(row['kararNo']),\n",
    "                \"dates\": str(row['extractedDates']),\n",
    "                \"daire\": str(row['daire']),\n",
    "                \"mahkeme\": str(row['mahkeme']),\n",
    "                \"karar_turu\": str(row['karar_turu']),\n",
    "                \"chunk_id\": str(row['chunk_id']),\n",
    "                \"chunk_index\": int(row['chunk_index']),\n",
    "                \"total_chunks\": int(row['total_chunks']),\n",
    "                \"chunk_text\": str(row['chunk_text_clean']),\n",
    "                \"chunk_length\": int(row['chunk_length'])\n",
    "            }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embeddings[idx],\n",
    "                payload=payload\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yÃ¼kle\n",
    "        for i in tqdm(range(0, len(points), batch_size), desc=\"Qdrant'a yÃ¼kleniyor\"):\n",
    "            batch_points = points[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=batch_points\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"YÃ¼kleme hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Toplam {len(points)} dokÃ¼man baÅŸarÄ±yla yÃ¼klendi!\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, score_threshold: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantik arama yap\"\"\"\n",
    "        print(f\"Arama yapÄ±lÄ±yor: '{query}'\")\n",
    "        \n",
    "        # Query embedding'i oluÅŸtur\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Arama yap (query_points kullan)\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def advanced_search(self, query: str, filters: Dict = None, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"GeliÅŸmiÅŸ filtreleme ile arama\"\"\"\n",
    "        query_embedding = self.model.encode(\n",
    "            [query], \n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )[0].tolist()\n",
    "        \n",
    "        # Filter oluÅŸtur\n",
    "        filter_conditions = None\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            \n",
    "            if 'daire' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"daire\",\n",
    "                    match=models.MatchValue(value=filters['daire'])\n",
    "                ))\n",
    "            \n",
    "            if 'karar_turu' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"karar_turu\", \n",
    "                    match=models.MatchValue(value=filters['karar_turu'])\n",
    "                ))\n",
    "            \n",
    "            if 'year' in filters:\n",
    "                conditions.append(models.FieldCondition(\n",
    "                    key=\"dates\",\n",
    "                    match=models.MatchText(text=str(filters['year']))\n",
    "                ))\n",
    "            \n",
    "            if conditions:\n",
    "                filter_conditions = models.Filter(must=conditions)\n",
    "        \n",
    "        # query_points kullan\n",
    "        search_results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_conditions,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for result in search_results.points:\n",
    "            results.append({\n",
    "                \"score\": result.score,\n",
    "                \"payload\": result.payload\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        \"\"\"Collection bilgilerini getir\"\"\"\n",
    "        try:\n",
    "            info = self.client.get_collection(collection_name=self.collection_name)\n",
    "            result = {\n",
    "                \"status\": str(info.status),\n",
    "                \"vectors_count\": info.vectors_count if hasattr(info, 'vectors_count') else 0,\n",
    "            }\n",
    "            \n",
    "            # Mevcut attributelarÄ± kontrol et ve ekle\n",
    "            if hasattr(info, 'segments_count'):\n",
    "                result[\"segments_count\"] = info.segments_count\n",
    "            if hasattr(info, 'indexed_vectors_count'):\n",
    "                result[\"indexed_vectors_count\"] = info.indexed_vectors_count\n",
    "            if hasattr(info, 'points_count'):\n",
    "                result[\"points_count\"] = info.points_count\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb206f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEvaluator:\n",
    "    def __init__(self, db: LegalDocumentVectorDB, queries_file: str):\n",
    "        self.db = db\n",
    "        with open(queries_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.queries = json.load(f)\n",
    "\n",
    "    def evaluate(self, limit: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Arama kalitesini Ã¶lÃ§\"\"\"\n",
    "        results_summary = []\n",
    "\n",
    "        for q in self.queries:\n",
    "            query_text = q[\"query\"]\n",
    "            expected_keywords = q.get(\"expected_keywords\", [])\n",
    "\n",
    "            res = self.db.search(query_text, limit=limit)\n",
    "\n",
    "            # Basit keyword match skoru\n",
    "            hits = 0\n",
    "            for r in res:\n",
    "                text = r[\"payload\"][\"chunk_text\"].lower()\n",
    "                if any(kw.lower() in text for kw in expected_keywords):\n",
    "                    hits += 1\n",
    "\n",
    "            results_summary.append({\n",
    "                \"query\": query_text,\n",
    "                \"expected_keywords\": expected_keywords,\n",
    "                \"retrieved\": len(res),\n",
    "                \"hits\": hits,\n",
    "                \"hit_rate\": round(hits / max(1, len(res)), 3),\n",
    "                \"avg_score\": round(np.mean([r[\"score\"] for r in res]), 3) if res else 0\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(results_summary)\n",
    "        return df\n",
    "\n",
    "    def save_results(self, df: pd.DataFrame, filename: str = \"eval_results.csv\"):\n",
    "        df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ“Š SonuÃ§lar kaydedildi: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveLegalSearch:\n",
    "        def __init__(self, qdrant_url: str, api_key: str,collection_name: str):\n",
    "            self.db = LegalDocumentVectorDB(qdrant_url, api_key,collection_name)\n",
    "        \n",
    "        def setup_database(self, csv_file: str):\n",
    "            \"\"\"Database'i kur\"\"\"\n",
    "            print(\"Database kurulumu baÅŸlÄ±yor...\")\n",
    "            self.db.create_collection(recreate=True)\n",
    "            \n",
    "            df = self.db.process_csv(csv_file)\n",
    "            if df is not None:\n",
    "                self.db.upload_to_qdrant(df)\n",
    "                print(\"Database kurulumu tamamlandÄ±!\")\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def interactive_search(self):\n",
    "            \"\"\"Interaktif arama\"\"\"\n",
    "            while True:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"HUKUKÄ° KARAR ARAMA SÄ°STEMÄ°\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                query = input(\"\\nAramak istediÄŸiniz metni girin (Ã§Ä±kmak iÃ§in 'q'): \")\n",
    "                if query.lower() == 'q':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    limit = int(input(\"KaÃ§ sonuÃ§ gÃ¶sterilsin? (varsayÄ±lan 5): \") or \"5\")\n",
    "                except:\n",
    "                    limit = 5\n",
    "                \n",
    "                results = self.db.search(query, limit=limit)\n",
    "                \n",
    "                if not results:\n",
    "                    print(\"âŒ SonuÃ§ bulunamadÄ±.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nğŸ” '{query}' iÃ§in {len(results)} sonuÃ§ bulundu:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results, 1):\n",
    "                    payload = result['payload']\n",
    "                    print(f\"\\nğŸ“„ {i}. SONUÃ‡ (Benzerlik: {result['score']:.3f})\")\n",
    "                    print(f\"   ğŸ“‚ Daire: {payload['daire']}\")\n",
    "                    print(f\"   ğŸ“‹ Esas No: {payload['esas_no']}\")\n",
    "                    print(f\"   âš–ï¸ Karar No: {payload['karar_no']}\")\n",
    "                    print(f\"   ğŸ›ï¸ Mahkeme: {payload['mahkeme']}\")\n",
    "                    print(f\"   ğŸ“… Tarihler: {payload['dates']}\")\n",
    "                    print(f\"   ğŸ“ Chunk: {payload['chunk_index']}/{payload['total_chunks']}\")\n",
    "                    print(f\"   ğŸ“„ Metin Ã–nizleme:\")\n",
    "                    print(f\"      {payload['chunk_text'][:300]}...\")\n",
    "                    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b313e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717247/3988472342.py:11: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.12.4. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformer modeli yÃ¼kleniyor...\n",
      "Model baÅŸarÄ±yla yÃ¼klendi!\n",
      "Vector boyutu: 384\n",
      "Arama yapÄ±lÄ±yor: 'YargÄ±tay 6. Hukuk Dairesi kira sÃ¶zleÅŸmesiyle ilgili hangi kararÄ± verdi?'\n",
      "Arama yapÄ±lÄ±yor: 'MÃ¼teselsil borÃ§ kavramÄ± hangi kararda nasÄ±l yorumlandÄ±?'\n",
      "Arama yapÄ±lÄ±yor: 'YargÄ±tay 13. Hukuk Dairesi'nin karar gerekÃ§esi nedir?'\n",
      "Arama yapÄ±lÄ±yor: 'Tazminat davalarÄ±nda hangi esaslara gÃ¶re hÃ¼kÃ¼m verilmektedir?'\n",
      "Arama yapÄ±lÄ±yor: 'Hangi davada taraflar arasÄ±nda sÃ¶zleÅŸme feshi sÃ¶z konusuydu?'\n",
      "Arama yapÄ±lÄ±yor: 'YargÄ±tay hangi kararda usul hatasÄ± nedeniyle bozma kararÄ± verdi?'\n",
      "Arama yapÄ±lÄ±yor: 'Mahkeme kararÄ±nda davacÄ±nÄ±n talebi ne ÅŸekilde deÄŸerlendirilmiÅŸtir?'\n",
      "Arama yapÄ±lÄ±yor: 'Ä°ÅŸ mahkemesi kararlarÄ±nda iÅŸÃ§i alacaklarÄ± nasÄ±l hÃ¼kme baÄŸlanÄ±yor?'\n",
      "Arama yapÄ±lÄ±yor: 'Hangi kararda icra takibine iliÅŸkin hÃ¼kÃ¼m yer alÄ±yor?'\n",
      "Arama yapÄ±lÄ±yor: 'BorÃ§lu temerrÃ¼dÃ¼ hangi iÃ§tihatta nasÄ±l aÃ§Ä±klanmÄ±ÅŸtÄ±r?'\n",
      "\n",
      "=== Evaluation SonuÃ§larÄ± ===\n",
      "                                               query  \\\n",
      "0  YargÄ±tay 6. Hukuk Dairesi kira sÃ¶zleÅŸmesiyle i...   \n",
      "1  MÃ¼teselsil borÃ§ kavramÄ± hangi kararda nasÄ±l yo...   \n",
      "2  YargÄ±tay 13. Hukuk Dairesi'nin karar gerekÃ§esi...   \n",
      "3  Tazminat davalarÄ±nda hangi esaslara gÃ¶re hÃ¼kÃ¼m...   \n",
      "4  Hangi davada taraflar arasÄ±nda sÃ¶zleÅŸme feshi ...   \n",
      "5  YargÄ±tay hangi kararda usul hatasÄ± nedeniyle b...   \n",
      "6  Mahkeme kararÄ±nda davacÄ±nÄ±n talebi ne ÅŸekilde ...   \n",
      "7  Ä°ÅŸ mahkemesi kararlarÄ±nda iÅŸÃ§i alacaklarÄ± nasÄ±...   \n",
      "8  Hangi kararda icra takibine iliÅŸkin hÃ¼kÃ¼m yer ...   \n",
      "9  BorÃ§lu temerrÃ¼dÃ¼ hangi iÃ§tihatta nasÄ±l aÃ§Ä±klan...   \n",
      "\n",
      "                               expected_keywords  retrieved  hits  hit_rate  \\\n",
      "0             [6. Hukuk Dairesi, kira, sÃ¶zleÅŸme]          5     5       1.0   \n",
      "1  [mÃ¼teselsil borÃ§, BorÃ§lar Kanunu, sorumluluk]          4     0       0.0   \n",
      "2            [13. Hukuk Dairesi, gerekÃ§e, karar]          5     5       1.0   \n",
      "3                        [tazminat, esas, hÃ¼kÃ¼m]          5     2       0.4   \n",
      "4                       [fesih, sÃ¶zleÅŸme, taraf]          5     5       1.0   \n",
      "5                    [bozma, usul hatasÄ±, karar]          5     5       1.0   \n",
      "6                 [davacÄ±, talep, deÄŸerlendirme]          5     5       1.0   \n",
      "7                   [iÅŸÃ§i, alacak, iÅŸ mahkemesi]          5     5       1.0   \n",
      "8                           [icra, takip, hÃ¼kÃ¼m]          5     2       0.4   \n",
      "9                    [borÃ§lu, temerrÃ¼t, iÃ§tihat]          1     0       0.0   \n",
      "\n",
      "   avg_score  \n",
      "0      0.709  \n",
      "1      0.526  \n",
      "2      0.754  \n",
      "3      0.704  \n",
      "4      0.598  \n",
      "5      0.766  \n",
      "6      0.753  \n",
      "7      0.668  \n",
      "8      0.722  \n",
      "9      0.505  \n",
      "ğŸ“Š SonuÃ§lar kaydedildi: eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mode = input(\"Mod seÃ§ (1=Interaktif Arama, 2=Evaluation): \")\n",
    "\n",
    "    searcher = InteractiveLegalSearch(\n",
    "        qdrant_url=\"https://qdrant.adalet.gov.tr:443\",\n",
    "        api_key=\"kMy0juEwUcsLjKDjWTPUAWTYlYpR3kjh\",\n",
    "        collection_name=\"hukuki_kararlar\"\n",
    "    )\n",
    "\n",
    "    if mode == \"1\":\n",
    "        searcher.interactive_search()\n",
    "    elif mode == \"2\":\n",
    "        evaluator = RetrievalEvaluator(searcher.db, \"queries.json\")\n",
    "        df = evaluator.evaluate(limit=5)\n",
    "        print(\"\\n=== Evaluation SonuÃ§larÄ± ===\")\n",
    "        print(df)\n",
    "        evaluator.save_results(df, \"eval_results.csv\")\n",
    "    else:\n",
    "        print(\"âŒ GeÃ§ersiz seÃ§im!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05689c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
