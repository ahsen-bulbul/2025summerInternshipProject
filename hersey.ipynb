{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahsen/MasaÃ¼stÃ¼/stajProjesi/venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from qdrant_client. models import  Prefetch, FusionQuery, Fusion, SparseVector\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import json\n",
    "from qdrant_client.models import ScoredPoint\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "from fastembed import SparseTextEmbedding, SparseEmbedding\n",
    "from config import Config\n",
    "from config import Models, model\n",
    "from typing import List, Dict\n",
    "print(load_dotenv(\"/home/ahsen/MasaÃ¼stÃ¼/stajProjesi/2025summerInternshipProject/qdrant/.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \n",
    "    def __init__(self, selected_model: str, runtime_config: Config):\n",
    "        self.selected_model = selected_model\n",
    "        self.runtime_config = runtime_config\n",
    "        self.model_config = getattr(model, str(selected_model))  # config.pyâ€™deki model nesnesi\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "\n",
    "        \n",
    "    def load_model(self):\n",
    "        model_name = self.model_config.model_name\n",
    "        model_type = self.model_config.model_type\n",
    "        print(f\"ðŸ”® Model yÃ¼kleniyor: {model_name} ({model_type})\")\n",
    "\n",
    "        if model_type == \"bge\":\n",
    "            self.model = BGEM3FlagModel(\n",
    "                model_name,\n",
    "                use_fp16=self.model_config.USE_FP16,\n",
    "                device=self.model_config.DEVICE\n",
    "            )\n",
    "        elif model_type == \"sentence_transformer\":\n",
    "            self.model = SentenceTransformer(model_name, device=self.model_config.DEVICE)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "\n",
    "        print(f\"âœ… Model yÃ¼klendi: {model_name}\")\n",
    "        return True\n",
    "\n",
    "    # def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "    #     # Dense embedding\n",
    "    #     dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    #     # Sparse embedding\n",
    "    #     model_sparse = SparseTextEmbedding(model_name=self.runtime_config.SPARSE_MODEL)\n",
    "    #     sparse_embeddings_raw = list(model_sparse.embed(texts, batch_size=100))  # liste\n",
    "\n",
    "    #     sparse_embeddings = []\n",
    "    #     for s in sparse_embeddings_raw:\n",
    "    #         sparse_embeddings.append({\n",
    "    #             \"indices\": s.indices.tolist(),\n",
    "    #             \"values\": s.values.tolist()\n",
    "    #         })\n",
    "\n",
    "    #     # Dense embedding boyutunu runtime configâ€™e gÃ¶re ayarla\n",
    "    #     target_dim = self.runtime_config.embedding_dim\n",
    "    #     dense_clean = []\n",
    "    #     for vec in dense_embeddings:\n",
    "    #         if vec is None:\n",
    "    #             dense_clean.append([0.0] * target_dim)\n",
    "    #         elif len(vec) < target_dim:\n",
    "    #             dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "    #         else:\n",
    "    #             dense_clean.append(vec[:target_dim])\n",
    "\n",
    "    #     return dense_clean, sparse_embeddings\n",
    "    def encode_texts(model_manager, texts, target_dim=512):\n",
    "        # Dense embedding\n",
    "        result = model_manager.model.encode(texts, return_dense=True, return_sparse=True)\n",
    "        dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "        #dense_embeddings = model_manager.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "        dense_embeddings = [d[:target_dim] for d in dense_embeddings]  # truncate 512\n",
    "        sparse_model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\n",
    "        # Sparse embedding\n",
    "        sparse_embeddings_raw = list(sparse_model.embed(texts, batch_size=100))\n",
    "        sparse_embeddings = []\n",
    "        for s in sparse_embeddings_raw:\n",
    "            sparse_embeddings.append({\n",
    "                \"indices\": s.indices.tolist(),\n",
    "                \"values\": s.values.tolist()\n",
    "            })\n",
    "\n",
    "        return dense_embeddings, sparse_embeddings\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"model_name\": self.model_config.model_name,\n",
    "            \"model_type\": self.model_config.model_type,\n",
    "            \"embedding_dim\": self.model_config.embedding_dim,\n",
    "            \"description\": self.model_config.description,\n",
    "            \"loaded\": self.model is not None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "\n",
    "    def __init__(self, runtime_config: Config, selected_model: str):\n",
    "        self.runtime_config = runtime_config\n",
    "        self.model_manager = ModelManager(selected_model, runtime_config)\n",
    "        self.model_manager.load_model()\n",
    "        \n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, runtime_config.TOKEN_SIZE)\n",
    "\n",
    "        self.qdrant_client = QdrantClient(url=runtime_config.QDRANT_URL)\n",
    "\n",
    "        model_name = self.runtime_config.SPARSE_MODEL \n",
    "        # This triggers the model download\n",
    "        self.sparse_model = SparseTextEmbedding(model_name=model_name)\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = True):\n",
    "        collection_name = Config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ðŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse iÃ§in yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(size=self.runtime_config.embedding_dim, distance=models.Distance.COSINE),\n",
    "                }\n",
    "                sparse_config = {\n",
    "                    \"sparse_vec\": models.SparseVectorParams(\n",
    "                        index=models.SparseIndexParams(on_disk=False))\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config = sparse_config\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result = []\n",
    "        for i, c in enumerate(chunks):\n",
    "            if c.strip():\n",
    "                cd = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': c.strip(),\n",
    "                    'token_count': len(self.encoding.encode(c)),\n",
    "                    'char_count': len(c)\n",
    "                }\n",
    "                if metadata:\n",
    "                    cd.update(metadata)\n",
    "                result.append(cd)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str = \"/home/ahsen/MasaÃ¼stÃ¼/stajProjesi/2025summerInternshipProject/data/cleaned10chunk.csv\") -> List[Dict]:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        text_column = next((c for c in ['rawText','chunk_text','text','content','metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"âŒ Ana metin sÃ¼tunu bulunamadÄ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding oluÅŸtur\"\"\"\n",
    "        batch_size = batch_size or self.runtime_config.BATCH_SIZE\n",
    "        \n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"ðŸ”® {total} metin iÅŸleniyor (model: {self.runtime_config.model_name})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  ðŸ“Š Batch iÅŸlendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Embedding hatasÄ± (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.runtime_config.embedding_dim for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    # def upload_to_qdrant(qdrant_client, chunks, dense_embeddings, sparse_embeddings, collection_name):\n",
    "    #     points = []\n",
    "\n",
    "    #     for c, d, s in zip(chunks, dense_embeddings, sparse_embeddings):\n",
    "    #         vector_dict = {\"dense_vec\": d}\n",
    "    #         if s and len(s[\"indices\"]) > 0:\n",
    "    #             vector_dict[\"sparse_vec\"] = SparseVector(indices=s[\"indices\"], values=s[\"values\"])\n",
    "    #         points.append(PointStruct(id=str(uuid.uuid4()), vector=vector_dict, payload=c))\n",
    "\n",
    "    #     # Batch upload\n",
    "    #     batch_size = 64\n",
    "    #     for i in range(0, len(points), batch_size):\n",
    "    #         qdrant_client.upsert(collection_name=collection_name, points=points[i:i+batch_size])\n",
    "\n",
    "    #     print(f\"âœ… {len(points)} noktalar Qdrant'a yÃ¼klendi!\")\n",
    "    \n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        dense_embeddings, sparse_embeddings = self.model_manager.encode_texts([c[\"text\"] for c in chunks])\n",
    "        \n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, dense_embeddings, sparse_embeddings):\n",
    "            vector_dict = {\"dense_vec\": d[:512]}\n",
    "            \n",
    "            if s is not None:\n",
    "                # s artÄ±k dict formatÄ±nda: {\"indices\": [...], \"values\": [...]}\n",
    "                indices = s.get(\"indices\", [])\n",
    "                values = s.get(\"values\", [])\n",
    "                \n",
    "                if len(indices) > 0:\n",
    "                    vector_dict[\"sparse_vec\"] = SparseVector(indices=indices, values=values)\n",
    "            \n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=vector_dict, payload=c))\n",
    "        \n",
    "        # batch upload\n",
    "        batch_size = self.runtime_config.BATCH_SIZE\n",
    "        for i in range(0, len(points), batch_size):\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=Config.COLLECTION_NAME,\n",
    "                points=points[i:i+batch_size]\n",
    "            )\n",
    "        print(f\"âœ… {len(points)} noktalar Qdrant'a yÃ¼klendi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model() -> str:\n",
    "    print(\"ðŸ¤– Model SeÃ§imi:\")\n",
    "    for name in vars(model):\n",
    "        m = getattr(model, name)\n",
    "        print(f\"{name}: {m.description} (Dim: {m.embedding_dim})\")\n",
    "    choice = input(\"Model seÃ§in (default bge_m3): \").strip() or \"bge_m3\"\n",
    "    if choice not in vars(model):\n",
    "        choice = \"bge_m3\"\n",
    "    return choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Model SeÃ§imi:\n",
      "bge_m3: BGE-M3 - Ã‡ok dilli, dense+sparse embedding destekli (Dim: 1024)\n",
      "bge_large: BGE Large - Sadece dense embedding (Dim: 1024)\n",
      "multilingual_e5: E5 Multilingual Large - Ã‡ok dilli dense embedding (Dim: 1024)\n",
      "turkish_bert: Turkish BERT - TÃ¼rkÃ§e Ã¶zelleÅŸtirilmiÅŸ (Dim: 768)\n",
      "distilbert_turkish: HÄ±zlÄ± TÃ¼rkÃ§e DistilBERT (Dim: 768)\n",
      "all_mpnet: All-MiniLM - Genel amaÃ§lÄ±, hÄ±zlÄ± (Dim: 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bge_m3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_model = select_model()\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge_m3 <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(SPARSE_MODEL='Qdrant/bm25', USE_FP16=True, DEVICE='cpu', TOKEN_SIZE=512, ENCODING_NAME='cl100k_base', QDRANT_URL='http://localhost:6333', COLLECTION_NAME='deneme', CSV_FILE='/home/ahsen/MasaÃ¼stÃ¼/stajProjesi/2025summerInternshipProject/data/cleaned10chunk.csv', BATCH_SIZE=100, DB_BATCH=256, model_name='BAAI/bge-m3', model_type='bge', embedding_dim=1024, max_seq_length=8192, description='BGE-M3 - Ã‡ok dilli, dense+sparse embedding destekli')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(selected_model, type(selected_model))\n",
    "selected_config = getattr(model, selected_model)\n",
    "selected_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = YargitaySemanticProcessor(Config, selected_model)\n",
    "processor.create_qdrant_collection(recreate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = processor.process_csv_file(Config.CSV_FILE)\n",
    "processor.upload_to_qdrant(chunks)\n",
    "print(\"âœ… Pipeline tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”® Model yÃ¼kleniyor: BAAI/bge-m3 (bge)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 190074.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model yÃ¼klendi: BAAI/bge-m3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x7f6f306cff40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager = ModelManager(selected_model, selected_config)\n",
    "model_manager.load_model()\n",
    "\n",
    "qdrant_client = QdrantClient(url=Config.QDRANT_URL)\n",
    "qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ihtiyati tedbir taazminat nedir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG encoded: {'dense_vecs': array([[-0.02884811,  0.00678069, -0.00575113, ..., -0.02282614,\n",
      "        -0.05229395, -0.00700623]], shape=(1, 1024), dtype=float32), 'lexical_weights': None, 'colbert_vecs': None}\n",
      "DEBUG type: <class 'dict'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dense_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dense_emb\n",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m, in \u001b[0;36mModelManager.encode_texts\u001b[0;34m(model_manager, texts, target_dim)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(encoded))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoded, \u001b[38;5;28mdict\u001b[39m):  \n\u001b[0;32m---> 64\u001b[0m     dense_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mencoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[:target_dim]]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     dense_embeddings \u001b[38;5;241m=\u001b[39m encoded\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embedding'"
     ]
    }
   ],
   "source": [
    "dense_emb = model_manager.encode_texts([query])\n",
    "dense_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_semantic(query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        model_manager = ModelManager(selected_model, selected_config)\n",
    "        model_manager.load_model()\n",
    "        qdrant_client = QdrantClient(url=Config.QDRANT_URL)\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "\n",
    "        try:\n",
    "            \n",
    "            dense_emb,sparse_emb = model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0][:512]\n",
    "            sparse_emb=None\n",
    "            \n",
    "            qr = qdrant_client.query_points(\n",
    "                collection_name=Config.COLLECTION_NAME,\n",
    "                query=query_vector, \n",
    "                using=\"dense_vec\",\n",
    "                limit=5\n",
    "            )\n",
    "            \n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr.points]\n",
    "            print(f\"ðŸ“Š {len(results)} sonuÃ§ bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Semantic search hatasÄ±: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hybrid(query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "    model_manager = ModelManager(selected_model, selected_config)\n",
    "    model_manager.load_model()\n",
    "    qdrant_client = QdrantClient(url=Config.QDRANT_URL)\n",
    "    \"\"\"Hybrid search (Dense + Sparse)\"\"\"\n",
    "\n",
    "    try:\n",
    "        dense_emb,sparse_emb = model_manager.encode_texts([query])\n",
    "        #query_vector = dense_emb[0][:512]\n",
    "        \n",
    "\n",
    "        s = sparse_emb[0]\n",
    "        query_sparse_vector = SparseVector(indices=s[\"indices\"], values=s[\"values\"])\n",
    "        qr=qdrant_client.query_points(\n",
    "            collection_name=Config.COLLECTION_NAME,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=query_sparse_vector,  # sparse vector\n",
    "                    using=\"sparse_vec\",\n",
    "                    limit=5,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=dense_emb[0],  # <-- dense vector\n",
    "                    using=\"dense_vec\",\n",
    "                    limit=20,\n",
    "                ),\n",
    "            ],\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        )\n",
    "            \n",
    "\n",
    "            # results = []\n",
    "            # for sp in qr:  # sp: ScoredPoint\n",
    "                \n",
    "            #     results.append({\n",
    "            #         \"score\": sp.score,\n",
    "            #         \"payload\": sp.point.payload\n",
    "            #     })\n",
    "\n",
    "            \n",
    "        results = [{\"score\": p.score, \"payload\": p.payload} for p in qr.points]\n",
    "        print(f\"ðŸ“Š {len(results)} sonuÃ§ bulundu \")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Hybrid search hatasÄ±: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ihtiyati tedbir tazminat \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager(selected_model, selected_config)\n",
    "model_manager.load_model()\n",
    "qdrant_client = QdrantClient(url=Config.QDRANT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_emb,sparse_emb = model_manager.encode_texts([query])\n",
    "query_vector = dense_emb[0][:512]\n",
    "sparse_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    print(\"\\nðŸ”Ž Ä°nteraktif arama baÅŸlatÄ±ldÄ±\")\n",
    "    print(processor.model_manager.get_model_info())\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"ðŸ” ARAMA SEÃ‡ENEKLERÄ°\")\n",
    "            print(\"1-only dense\")\n",
    "            print(\"2-dense + sparse\")\n",
    "            choice = input(\"SeÃ§iminiz (1/2/3, Ã§Ä±kmak iÃ§in q): \").strip()\n",
    "            if choice.lower() == 'q':\n",
    "                print(\"Ã‡Ä±kÄ±lÄ±yor...\")\n",
    "                break\n",
    "            if choice not in ['1', '2', '3']:\n",
    "                print(\"GeÃ§ersiz seÃ§im, tekrar deneyin.\")\n",
    "                continue\n",
    "            query = input(\"Arama sorgusu girin: \").strip()\n",
    "            if not query:\n",
    "                print(\"BoÅŸ sorgu, tekrar deneyin.\")\n",
    "                continue\n",
    "            if choice == '1':\n",
    "                results = search_semantic(query, limit=10, score_threshold=0.6)\n",
    "            elif choice == '2':\n",
    "                results = search_hybrid(query, limit=10, score_threshold=0.6)\n",
    "            print(f\"\\nðŸ“Š {len(results)} sonuÃ§ bulundu:\")\n",
    "            \n",
    "            for idx, r in enumerate(results, 1):\n",
    "                print(f\"{idx}. Score: {r['score']:.4f}, Text: {r['payload'].get('text','')[:200]}...\")  # ilk 200 karakter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search()\n",
    "print(search())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihtiyati tedbir tazminat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
