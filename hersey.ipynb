{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from qdrant_client. models import  Prefetch, FusionQuery, Fusion, SparseVector\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "from fastembed import SparseTextEmbedding, SparseEmbedding\n",
    "from config import Config\n",
    "from config import Models, model\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "\n",
    "    def __init__(self, selected_model: str, runtime_config: Config):\n",
    "        self.selected_model = selected_model\n",
    "        self.runtime_config = runtime_config\n",
    "        self.model_config = getattr(model, selected_model)  # config.py‚Äôdeki model nesnesi\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "\n",
    "    def load_model(self):\n",
    "        model_name = self.model_config.model_name\n",
    "        model_type = self.model_config.model_type\n",
    "        print(f\"üîÆ Model y√ºkleniyor: {model_name} ({model_type})\")\n",
    "\n",
    "        if model_type == \"bge\":\n",
    "            self.model = BGEM3FlagModel(\n",
    "                model_name,\n",
    "                use_fp16=self.model_config.USE_FP16,\n",
    "                device=self.model_config.DEVICE\n",
    "            )\n",
    "        elif model_type == \"sentence_transformer\":\n",
    "            self.model = SentenceTransformer(model_name, device=self.model_config.DEVICE)\n",
    "            # TF-IDF sparse embedding\n",
    "            self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        else:\n",
    "            raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "\n",
    "        print(f\"‚úÖ Model y√ºklendi: {model_name}\")\n",
    "        return True\n",
    "\n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        dense_embeddings, sparse_embeddings = [], []\n",
    "\n",
    "        if self.model_config.model_type == \"bge\" and hasattr(self.model, 'encode'):\n",
    "            result = self.model.encode(texts, return_dense=True, return_sparse=True)\n",
    "            dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "            sparse_raw = result.get(\"sparse_vecs\", [])\n",
    "            sparse_embeddings = [\n",
    "                {\"indices\": s.get(\"indices\", []), \"values\": s.get(\"values\", [])}\n",
    "                for s in sparse_raw\n",
    "            ] if sparse_raw else [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "        else:\n",
    "            dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "            if self.vectorizer:\n",
    "                X_sparse = self.vectorizer.fit_transform(texts)\n",
    "                sparse_embeddings = []\n",
    "                for i in range(X_sparse.shape[0]):\n",
    "                    row = X_sparse[i].tocoo()\n",
    "                    sparse_embeddings.append({\n",
    "                        \"indices\": row.col.tolist(),\n",
    "                        \"values\": row.data.tolist()\n",
    "                    })\n",
    "            else:\n",
    "                sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "\n",
    "        # Embedding boyutunu runtime config‚Äôe g√∂re ayarla\n",
    "        target_dim = self.runtime_config.EMBEDDING_DIM\n",
    "        dense_clean = []\n",
    "        for vec in dense_embeddings:\n",
    "            if vec is None:\n",
    "                dense_clean.append([0.0] * target_dim)\n",
    "            elif len(vec) < target_dim:\n",
    "                dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "            else:\n",
    "                dense_clean.append(vec[:target_dim])\n",
    "\n",
    "        return dense_clean, sparse_embeddings\n",
    "\n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"model_name\": self.model_config.model_name,\n",
    "            \"model_type\": self.model_config.model_type,\n",
    "            \"embedding_dim\": self.model_config.embedding_dim,\n",
    "            \"description\": self.model_config.description,\n",
    "            \"loaded\": self.model is not None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YargitaySemanticProcessor:\n",
    "\n",
    "    def __init__(self, runtime_config: Config, selected_model: str):\n",
    "        self.runtime_config = runtime_config\n",
    "        self.model_manager = ModelManager(selected_model, runtime_config)\n",
    "        self.model_manager.load_model()\n",
    "\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, runtime_config.TOKEN_SIZE)\n",
    "\n",
    "        self.qdrant_client = QdrantClient(url=runtime_config.QDRANT_URL)\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = True):\n",
    "        collection_name = Config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse i√ßin yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(size=self.config.EMBEDDING_DIM, distance=models.Distance.COSINE),\n",
    "                }\n",
    "                sparse_config = {\n",
    "                    \"sparse_vec\": models.SparseVectorParams(\n",
    "                        index=models.SparseIndexParams(on_disk=False))\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config = sparse_config\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result = []\n",
    "        for i, c in enumerate(chunks):\n",
    "            if c.strip():\n",
    "                cd = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': c.strip(),\n",
    "                    'token_count': len(self.encoding.encode(c)),\n",
    "                    'char_count': len(c)\n",
    "                }\n",
    "                if metadata:\n",
    "                    cd.update(metadata)\n",
    "                result.append(cd)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        text_column = next((c for c in ['rawText','chunk_text','text','content','metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.runtime_config.BATCH_SIZE\n",
    "        all_dense, all_sparse = [], []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            emb_res = self.bge_model.encode(\n",
    "                    batch_texts,\n",
    "                    return_dense=True,\n",
    "                    return_sparse=True\n",
    "                )\n",
    "            dense = emb_res.get(\"dense_vecs\", [[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "                # Dense i√ßinde None veya kƒ±sa vekt√∂r varsa d√ºzelt\n",
    "            dense_clean = []\n",
    "            for vec in dense:\n",
    "                if vec is None:\n",
    "                    dense_clean.append([0.0]*self.config.EMBEDDING_DIM)\n",
    "                elif len(vec) < self.config.EMBEDDING_DIM:\n",
    "                    dense_clean.append(vec + [0.0]*(self.config.EMBEDDING_DIM - len(vec)))\n",
    "                else:\n",
    "                    dense_clean.append(vec[:self.config.EMBEDDING_DIM])\n",
    "    \n",
    "            sparse_vectors = []\n",
    "            for text in batch_texts:\n",
    "                # SparseEmbedding √ºret\n",
    "                sparse_embedding = list(model.embed(text))[0]  # ilk embedding\n",
    "                sparse_vectors.append({\n",
    "                    \"indices\": sparse_embedding.indices.tolist(),\n",
    "                    \"values\": sparse_embedding.values.tolist()\n",
    "                })\n",
    "            # Listeye ekle\n",
    "            all_dense.extend(dense_clean)\n",
    "            all_sparse.extend(sparse_vectors)\n",
    "\n",
    "            print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_dense, all_sparse\n",
    "\n",
    "\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        points = []\n",
    "        dense_embeddings, sparse_embeddings = self.create_embeddings([c['text'] for c in chunks])\n",
    "        for c, d, s in zip(chunks, dense_embeddings, sparse_embeddings):\n",
    "            vector_dict = {\"dense_vec\": d}\n",
    "            if s[\"indices\"]:\n",
    "                vector_dict[\"sparse_vec\"] = SparseVector(indices=s[\"indices\"], values=s[\"values\"])\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=vector_dict, payload=c))\n",
    "\n",
    "        batch = self.runtime_config.BATCH_SIZE\n",
    "        for i in range(0, len(points), batch):\n",
    "            self.qdrant_client.upsert(collection_name=self.runtime_config.COLLECTION_NAME, points=points[i:i+batch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model() -> str:\n",
    "    print(\"ü§ñ Model Se√ßimi:\")\n",
    "    for name in vars(model):\n",
    "        m = getattr(model, name)\n",
    "        print(f\"{name}: {m.description} (Dim: {m.embedding_dim})\")\n",
    "    choice = input(\"Model se√ßin (default bge_m3): \").strip() or \"bge_m3\"\n",
    "    if choice not in vars(model):\n",
    "        choice = \"bge_m3\"\n",
    "    return choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = select_model()\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_config = getattr(model, selected_model)\n",
    "selected_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = YargitaySemanticProcessor(Config, selected_model)\n",
    "processor.create_qdrant_collection(recreate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = processor.process_csv_file(Config.CSV_FILE)\n",
    "processor.upload_to_qdrant(chunks)\n",
    "print(\"‚úÖ Pipeline tamamlandƒ±!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
