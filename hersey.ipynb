{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahsen/Masa√ºst√º/stajProjesi/venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from qdrant_client. models import  Prefetch, FusionQuery, Fusion, SparseVector\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from qdrant_client.models import ScoredPoint\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "from fastembed import SparseTextEmbedding, SparseEmbedding\n",
    "from config import Config\n",
    "from config import Models, model\n",
    "from typing import List, Dict\n",
    "print(load_dotenv(\"/home/ahsen/Masa√ºst√º/stajProjesi/2025summerInternshipProject/qdrant/.env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \n",
    "    def __init__(self, selected_model: str, runtime_config: Config):\n",
    "        self.selected_model = selected_model\n",
    "        self.runtime_config = runtime_config\n",
    "        self.model_config = getattr(model, str(selected_model))  # config.py‚Äôdeki model nesnesi\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "\n",
    "        \n",
    "    def load_model(self):\n",
    "        model_name = self.model_config.model_name\n",
    "        model_type = self.model_config.model_type\n",
    "        print(f\"üîÆ Model y√ºkleniyor: {model_name} ({model_type})\")\n",
    "\n",
    "        if model_type == \"bge\":\n",
    "            self.model = BGEM3FlagModel(\n",
    "                model_name,\n",
    "                use_fp16=self.model_config.USE_FP16,\n",
    "                device=self.model_config.DEVICE\n",
    "            )\n",
    "        elif model_type == \"sentence_transformer\":\n",
    "            self.model = SentenceTransformer(model_name, device=self.model_config.DEVICE)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "\n",
    "        print(f\"‚úÖ Model y√ºklendi: {model_name}\")\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def encode_texts(model_manager, texts, target_dim=512):\n",
    "        # Dense embedding\n",
    "        result = model_manager.model.encode(texts, return_dense=True, return_sparse=True)\n",
    "        dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "        #dense_embeddings = model_manager.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "        dense_embeddings = [d[:target_dim] for d in dense_embeddings]  # truncate 512\n",
    "        sparse_model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\n",
    "        # Sparse embedding\n",
    "        sparse_embeddings_raw = list(sparse_model.embed(texts, batch_size=100))\n",
    "        sparse_embeddings = []\n",
    "        for s in sparse_embeddings_raw:\n",
    "            sparse_embeddings.append({\n",
    "                \"indices\": s.indices.tolist(),\n",
    "                \"values\": s.values.tolist()\n",
    "            })\n",
    "\n",
    "        return dense_embeddings, sparse_embeddings\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"model_name\": self.model_config.model_name,\n",
    "            \"model_type\": self.model_config.model_type,\n",
    "            \"embedding_dim\": self.model_config.embedding_dim,\n",
    "            \"description\": self.model_config.description,\n",
    "            \"loaded\": self.model is not None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "\n",
    "    def __init__(self, runtime_config: Config, selected_model: str):\n",
    "        self.runtime_config = runtime_config\n",
    "        self.model_manager = ModelManager(selected_model, runtime_config)\n",
    "        self.model_manager.load_model()\n",
    "        \n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, runtime_config.TOKEN_SIZE)\n",
    "\n",
    "        self.qdrant_client = QdrantClient(url=runtime_config.QDRANT_URL)\n",
    "\n",
    "        model_name = self.runtime_config.SPARSE_MODEL \n",
    "        # This triggers the model download\n",
    "        self.sparse_model = SparseTextEmbedding(model_name=model_name)\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = True):\n",
    "        collection_name = Config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse i√ßin yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(size=self.runtime_config.embedding_dim, distance=models.Distance.COSINE),\n",
    "                }\n",
    "                sparse_config = {\n",
    "                    \"sparse_vec\": models.SparseVectorParams(\n",
    "                        index=models.SparseIndexParams(on_disk=False))\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config = sparse_config\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result = []\n",
    "        for i, c in enumerate(chunks):\n",
    "            if c.strip():\n",
    "                cd = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': c.strip(),\n",
    "                    'token_count': len(self.encoding.encode(c)),\n",
    "                    'char_count': len(c)\n",
    "                }\n",
    "                if metadata:\n",
    "                    cd.update(metadata)\n",
    "                result.append(cd)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str = \"/home/ahsen/Masa√ºst√º/stajProjesi/2025summerInternshipProject/data/cleaned10chunk.csv\") -> List[Dict]:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        text_column = next((c for c in ['rawText','chunk_text','text','content','metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding olu≈ütur\"\"\"\n",
    "        batch_size = batch_size or self.runtime_config.BATCH_SIZE\n",
    "        \n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (model: {self.runtime_config.model_name})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.runtime_config.embedding_dim for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        dense_embeddings, sparse_embeddings = self.model_manager.encode_texts([c[\"text\"] for c in chunks])\n",
    "        \n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, dense_embeddings, sparse_embeddings):\n",
    "            vector_dict = {\"dense_vec\": d[:512]}\n",
    "            \n",
    "            if s is not None:\n",
    "                # s artƒ±k dict formatƒ±nda: {\"indices\": [...], \"values\": [...]}\n",
    "                indices = s.get(\"indices\", [])\n",
    "                values = s.get(\"values\", [])\n",
    "                \n",
    "                if len(indices) > 0:\n",
    "                    vector_dict[\"sparse_vec\"] = SparseVector(indices=indices, values=values)\n",
    "            \n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=vector_dict, payload=c))\n",
    "        \n",
    "        # batch upload\n",
    "        batch_size = self.runtime_config.BATCH_SIZE\n",
    "        for i in range(0, len(points), batch_size):\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=Config.COLLECTION_NAME,\n",
    "                points=points[i:i+batch_size]\n",
    "            )\n",
    "        print(f\"‚úÖ {len(points)} noktalar Qdrant'a y√ºklendi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model() -> str:\n",
    "    print(\"ü§ñ Model Se√ßimi:\")\n",
    "    for name in vars(model):\n",
    "        m = getattr(model, name)\n",
    "        print(f\"{name}: {m.description} (Dim: {m.embedding_dim})\")\n",
    "    choice = input(\"Model se√ßin (default bge_m3): \").strip() or \"bge_m3\"\n",
    "    if choice not in vars(model):\n",
    "        choice = \"bge_m3\"\n",
    "    return choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model Se√ßimi:\n",
      "bge_m3: BGE-M3 - √áok dilli, dense+sparse embedding destekli (Dim: 1024)\n",
      "bge_large: BGE Large - Sadece dense embedding (Dim: 1024)\n",
      "multilingual_e5: E5 Multilingual Large - √áok dilli dense embedding (Dim: 1024)\n",
      "turkish_bert: Turkish BERT - T√ºrk√ße √∂zelle≈ütirilmi≈ü (Dim: 768)\n",
      "distilbert_turkish: Hƒ±zlƒ± T√ºrk√ße DistilBERT (Dim: 768)\n",
      "all_mpnet: All-MiniLM - Genel ama√ßlƒ±, hƒ±zlƒ± (Dim: 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bge_m3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_model = select_model()\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bge_m3 <class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(SPARSE_MODEL='Qdrant/bm25', USE_FP16=True, DEVICE='cpu', TOKEN_SIZE=512, ENCODING_NAME='cl100k_base', QDRANT_URL='http://localhost:6333', COLLECTION_NAME='deneme', CSV_FILE='/home/ahsen/Masa√ºst√º/stajProjesi/2025summerInternshipProject/data/cleaned10chunk.csv', BATCH_SIZE=100, DB_BATCH=256, model_name='BAAI/bge-m3', model_type='bge', embedding_dim=1024, max_seq_length=8192, description='BGE-M3 - √áok dilli, dense+sparse embedding destekli')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(selected_model, type(selected_model))\n",
    "selected_config = getattr(model, selected_model)\n",
    "selected_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Model y√ºkleniyor: BAAI/bge-m3 (bge)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 143150.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model y√ºklendi: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:01<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Eski koleksiyon silindi: deneme\n",
      "‚úÖ Koleksiyon olu≈üturuldu: deneme (Dense+Sparse)\n"
     ]
    }
   ],
   "source": [
    "processor = YargitaySemanticProcessor(Config, selected_model)\n",
    "processor.create_qdrant_collection(recreate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 59 noktalar Qdrant'a y√ºklendi!\n",
      "‚úÖ Pipeline tamamlandƒ±!\n"
     ]
    }
   ],
   "source": [
    "chunks = processor.process_csv_file(Config.CSV_FILE)\n",
    "processor.upload_to_qdrant(chunks)\n",
    "print(\"‚úÖ Pipeline tamamlandƒ±!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Model y√ºkleniyor: BAAI/bge-m3 (bge)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 95252.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model y√ºklendi: BAAI/bge-m3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x7f4e97262aa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager = ModelManager(selected_model, selected_config)\n",
    "model_manager.load_model()\n",
    "\n",
    "qdrant_client = QdrantClient(url=Config.QDRANT_URL)\n",
    "qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ihtiyati tedbir taazminat nedir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([-2.88481135e-02,  6.78069005e-03, -5.75112691e-03, -3.40650715e-02,\n",
       "         -2.82505136e-02, -2.20167357e-02,  5.81906773e-02,  7.58740027e-03,\n",
       "         -6.67017186e-03, -3.26118544e-02, -3.48149016e-02,  1.15371337e-02,\n",
       "         -7.64328316e-02,  2.24253684e-02,  1.14032095e-02,  2.03358214e-02,\n",
       "          4.02332060e-02,  1.02652507e-02,  3.65760177e-02, -4.24560234e-02,\n",
       "         -3.14213037e-02, -2.76389010e-02, -4.19816701e-03, -1.05831483e-02,\n",
       "         -2.48087626e-02,  2.93866638e-02,  2.91311257e-02,  2.21866872e-02,\n",
       "          3.17512564e-02, -3.50190923e-02, -1.26664876e-03, -5.46354381e-03,\n",
       "         -8.19494762e-03, -1.67326233e-03, -1.99179575e-02,  4.12217416e-02,\n",
       "         -1.61503553e-02, -7.26180598e-02, -2.40964871e-02,  2.49175783e-02,\n",
       "          1.20750535e-02,  4.71724989e-03,  2.31970730e-03, -1.39305284e-02,\n",
       "          2.21467521e-02, -4.28945497e-02,  5.39295655e-03, -2.67709829e-02,\n",
       "         -4.82691545e-03, -4.37719114e-02, -2.15039607e-02, -6.11547641e-02,\n",
       "          5.29943332e-02, -5.40826656e-02,  1.96993742e-02,  4.29633260e-02,\n",
       "          3.82776401e-04,  3.06343064e-02, -5.69337346e-02, -1.17805423e-02,\n",
       "         -4.03123759e-02, -3.35619040e-02,  5.18853124e-03, -1.32883806e-02,\n",
       "          6.12005778e-03,  3.59694660e-02,  3.46048526e-03,  1.33351171e-02,\n",
       "         -8.78325384e-03, -2.13329345e-02, -3.61467525e-02,  2.15582978e-02,\n",
       "         -7.72289233e-03,  2.03710832e-02, -5.23130447e-02,  1.42493630e-02,\n",
       "          5.51942699e-02,  2.91469190e-02,  5.79528324e-03, -1.58406477e-02,\n",
       "         -9.78233851e-03,  4.23828047e-03, -3.86206619e-02,  6.22468069e-02,\n",
       "         -3.60627994e-02,  2.06219237e-02,  1.63045097e-02, -1.19023556e-02,\n",
       "          2.71385089e-02, -1.41621428e-02, -3.06268483e-02, -8.44621274e-04,\n",
       "          1.00592673e-02, -4.83697429e-02, -7.71569507e-03,  5.46738412e-03,\n",
       "         -6.80104569e-02, -2.76459996e-02,  1.92295294e-02,  2.38933545e-02,\n",
       "          3.27996239e-02,  1.47763994e-02,  2.23276522e-02, -4.67215367e-02,\n",
       "          1.49373664e-02, -4.66978773e-02,  1.92141496e-02,  4.01350111e-02,\n",
       "          2.19150377e-03,  4.47324663e-03, -2.67209020e-02, -1.17219461e-03,\n",
       "          7.87281245e-03,  1.46569386e-02, -1.50012923e-02,  1.55850137e-02,\n",
       "          1.45406183e-02, -2.92387605e-02,  2.74193399e-02,  4.36569285e-03,\n",
       "          1.17456112e-02, -2.20310092e-02, -1.68490820e-02, -4.62737978e-02,\n",
       "          1.09683909e-02, -2.26199511e-03,  1.57114398e-02, -1.55931413e-02,\n",
       "          3.04998327e-02,  3.57559808e-02,  2.31280271e-03,  2.67884824e-02,\n",
       "         -2.14450005e-02, -1.87066775e-02, -6.10438175e-02, -1.13835661e-02,\n",
       "          3.43807824e-02,  1.02068977e-02,  7.11182086e-03, -5.81360087e-02,\n",
       "          5.77931181e-02,  3.70120965e-02, -7.41478009e-03, -3.28267291e-02,\n",
       "         -3.99511511e-04, -5.61509542e-02,  3.34007740e-02, -3.75020457e-03,\n",
       "          1.84311774e-02,  4.82540671e-03,  1.27408514e-02,  1.29729519e-02,\n",
       "         -1.15388064e-02, -1.30694471e-02,  1.35139488e-02,  9.32347309e-03,\n",
       "         -3.04473434e-02, -1.06407686e-04,  1.81036759e-02,  2.76129320e-03,\n",
       "          2.01877542e-02,  5.52034639e-02,  3.51332105e-03, -3.67665887e-02,\n",
       "         -2.42430605e-02,  1.12604210e-02,  9.45408270e-03,  6.58422615e-03,\n",
       "         -9.07848962e-03,  6.04057536e-02,  1.36600630e-02, -2.51910072e-02,\n",
       "         -3.36205936e-03, -7.42562581e-03,  1.26750544e-02, -9.27166641e-03,\n",
       "          3.75896879e-02, -1.25815731e-03, -2.91937552e-02, -2.23603342e-02,\n",
       "         -4.85555418e-02,  1.83187202e-02,  1.52054764e-02,  2.11223159e-02,\n",
       "         -1.93594322e-02,  4.85013127e-02,  4.94223367e-03,  2.28307284e-02,\n",
       "          4.37215948e-03,  4.72451076e-02,  2.08941437e-02,  2.23698467e-02,\n",
       "          4.94757853e-02,  9.61503759e-03,  1.49065498e-02, -5.29943071e-02,\n",
       "          4.40479033e-02, -5.85649535e-02,  2.13440731e-02,  1.31837083e-02,\n",
       "         -2.64019091e-02,  1.01852436e-02, -4.65132529e-03,  1.71930809e-02,\n",
       "          9.24781058e-03,  4.93254745e-03, -3.72334570e-02,  1.56933011e-03,\n",
       "          3.15606296e-02, -8.33767056e-02, -2.32936386e-02,  9.53444373e-03,\n",
       "          5.19062206e-02,  6.11517951e-03, -4.66362238e-02, -2.66871657e-02,\n",
       "         -1.88867107e-03, -4.48675156e-02,  1.16786628e-03,  2.87141651e-02,\n",
       "         -5.09865247e-02, -6.74392190e-03,  2.33330354e-02, -1.76852010e-02,\n",
       "          2.70468350e-02,  3.28625813e-02,  2.01368071e-02,  1.82684921e-02,\n",
       "         -5.57439774e-03, -4.24601249e-02, -6.84607495e-03,  3.37469280e-02,\n",
       "         -3.23620215e-02, -3.44809070e-02,  9.31506604e-02,  4.59243990e-02,\n",
       "         -6.55894820e-03,  8.48632213e-03,  2.46874610e-04,  1.81544537e-03,\n",
       "         -1.13396524e-02,  1.44905141e-02, -2.78739119e-03,  1.84231531e-02,\n",
       "         -1.08163841e-02, -2.89516263e-02,  1.97360236e-02, -5.99473761e-03,\n",
       "         -4.74318536e-03, -1.66632067e-02, -3.67323495e-02, -8.50426685e-03,\n",
       "         -4.66072857e-02,  4.92543280e-02,  9.44567472e-03,  2.73737051e-02,\n",
       "         -1.39491819e-02,  1.53584257e-02,  5.68438470e-02,  6.85016736e-02,\n",
       "         -5.91692748e-03,  1.41452188e-02, -2.84757512e-03,  4.72741760e-02,\n",
       "          9.23551060e-03,  4.97934222e-03, -1.03031467e-04,  3.14523503e-02,\n",
       "          1.37501843e-02, -1.31827742e-02, -2.54597235e-02, -5.73811494e-02,\n",
       "         -1.84090976e-02,  2.85472739e-02,  1.01026362e-02, -2.70329416e-02,\n",
       "         -9.36281518e-04,  3.51826288e-02,  7.75125390e-03,  5.86768659e-03,\n",
       "         -9.26224887e-02,  3.74345900e-03,  7.56262289e-03, -3.14722909e-03,\n",
       "         -4.95293504e-03, -5.28901629e-03, -5.06054536e-02, -4.00136411e-03,\n",
       "          2.95738392e-02,  2.25660391e-02, -2.46385690e-02, -3.26199867e-02,\n",
       "          6.76275492e-02, -4.09930153e-03, -1.97227835e-03,  3.34051214e-02,\n",
       "         -1.76443439e-02, -1.69516683e-01,  1.39296800e-02, -1.60255365e-03,\n",
       "          3.49213071e-02,  1.13773989e-02, -6.15680474e-04, -2.82559264e-02,\n",
       "         -2.63703540e-02, -7.94903375e-03,  2.29000002e-02,  1.11018671e-02,\n",
       "         -4.41438481e-02,  1.68801975e-02,  3.86919156e-02,  5.91815189e-02,\n",
       "         -1.05998674e-02,  1.81968196e-03,  5.47579629e-03,  1.29263848e-02,\n",
       "         -5.11021307e-03, -7.91687369e-02, -2.83585396e-02,  2.82838270e-02,\n",
       "         -1.09755537e-02, -3.70265520e-03,  4.23920825e-02,  1.04833633e-01,\n",
       "          2.76590046e-02, -3.83274481e-02, -2.56398786e-02, -3.46152820e-02,\n",
       "         -4.56592301e-03, -6.31993916e-03,  4.56248745e-02, -1.15539376e-02,\n",
       "          8.67169257e-03,  3.08777280e-02, -6.57079834e-03,  2.96738762e-02,\n",
       "          9.05579422e-03,  2.20040493e-02,  1.34577311e-03,  1.11882407e-02,\n",
       "          6.53361753e-02, -2.40378547e-02,  2.11034417e-02,  2.29529720e-02,\n",
       "          2.72082561e-03, -1.70622598e-02,  4.83554862e-02, -1.37336385e-02,\n",
       "          4.11655987e-03,  1.60263684e-02, -2.05963012e-02, -5.16385399e-02,\n",
       "         -3.53891179e-02,  5.30243628e-02,  7.87908807e-02,  7.38641107e-03,\n",
       "         -5.39397495e-03, -6.33695349e-02, -2.43018977e-02,  5.31969685e-03,\n",
       "          9.78980958e-03, -1.63365400e-03, -7.05048665e-02,  2.57736314e-02,\n",
       "          3.38326916e-02,  5.36122127e-03,  6.88377768e-04, -2.66914386e-02,\n",
       "         -1.82482935e-02, -1.79173555e-02, -2.92176977e-02, -4.85580927e-03,\n",
       "          3.72947752e-02, -1.69604104e-02, -1.40193691e-02, -9.99126490e-03,\n",
       "         -1.22387826e-01,  2.01021624e-03,  2.63884403e-02,  1.18698478e-02,\n",
       "         -3.35486635e-04, -6.16569584e-03, -8.02273005e-02,  2.45671701e-02,\n",
       "          2.75313873e-02, -3.08301486e-02,  2.01143876e-01, -1.32256923e-02,\n",
       "          3.25129181e-02, -3.69411078e-03, -4.43596952e-02, -3.82210389e-02,\n",
       "         -2.80250460e-02, -3.77057269e-02,  3.61205474e-03, -1.92902274e-02,\n",
       "         -8.08547214e-02, -3.22407484e-02,  5.94807882e-03,  3.52994278e-02,\n",
       "         -2.90721916e-02,  1.46410184e-03, -1.27043100e-02, -1.58036910e-02,\n",
       "          6.29800856e-02, -3.76030281e-02,  3.77198495e-02, -2.11747959e-02,\n",
       "         -6.64057024e-03,  1.05967382e-02, -2.16657831e-03,  1.81280181e-03,\n",
       "          3.47827934e-02,  8.33905116e-02, -4.25139815e-02, -8.34101718e-03,\n",
       "         -3.80703993e-02, -3.55840810e-02,  4.79402170e-02, -2.58665849e-02,\n",
       "         -3.85886915e-02, -1.13423052e-03, -6.03524372e-02, -1.56207988e-02,\n",
       "         -4.62354533e-02, -2.37851031e-03,  7.53554329e-03, -6.46651536e-03,\n",
       "          1.92561466e-02,  1.30094765e-02,  4.81309183e-02,  2.78056506e-02,\n",
       "         -6.06494732e-02, -3.86960022e-02,  5.15427534e-03, -2.26533432e-02,\n",
       "          4.44761924e-02,  4.53209765e-02, -3.63500826e-02,  2.10772939e-02,\n",
       "         -1.03581659e-02, -1.22546600e-02, -3.35700102e-02, -6.58565620e-03,\n",
       "          3.45420162e-03, -1.79449059e-02,  5.05282469e-02, -3.57554341e-03,\n",
       "         -6.61956146e-03, -3.27087119e-02,  1.38868643e-02,  9.68931708e-03,\n",
       "         -2.10127756e-02,  8.92097969e-03,  1.25573166e-02,  2.92978752e-02,\n",
       "          2.00934499e-03,  1.65044162e-02,  4.90431848e-04, -1.95641909e-02,\n",
       "          3.23980898e-02, -1.80082640e-03,  1.34195574e-03,  4.94320877e-02,\n",
       "          1.91278625e-02,  5.85885206e-03, -4.54998203e-02,  1.12099657e-02,\n",
       "         -4.32144962e-02, -4.12717462e-03,  2.83921976e-02,  1.94203593e-02,\n",
       "         -3.89624648e-02, -1.21133006e-03, -6.71547046e-03,  1.20743969e-02,\n",
       "         -7.08982116e-03,  3.48289125e-03,  2.75267325e-02,  2.81982496e-03,\n",
       "         -1.09556159e-02,  4.27285098e-02,  3.80325690e-02, -7.17555592e-03,\n",
       "         -4.91396375e-02, -1.01848422e-02, -8.98911618e-03, -3.27572576e-03,\n",
       "          1.95153467e-02, -7.48072285e-03,  1.63527839e-02, -2.54046288e-03,\n",
       "          2.16696076e-02,  3.30400355e-02, -3.92844900e-02,  8.52515083e-03,\n",
       "          8.01404379e-03,  1.44581348e-02, -4.15919255e-03, -3.05855405e-02,\n",
       "          3.64342965e-02,  5.60767651e-02,  1.80742946e-02,  2.99586747e-02,\n",
       "          3.39398198e-02,  1.12335226e-02, -5.96361980e-02,  2.87258308e-02,\n",
       "         -2.83931615e-03,  2.28851568e-02,  7.84916192e-05, -1.90331601e-02],\n",
       "        dtype=float32)],\n",
       " [{'indices': [2028035748, 2142705205, 1534607136, 1130420785],\n",
       "   'values': [1.6741973840665876,\n",
       "    1.6741973840665876,\n",
       "    1.6741973840665876,\n",
       "    1.6741973840665876]}])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_emb = model_manager.encode_texts([query])\n",
    "dense_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_semantic(query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        model_manager = ModelManager(selected_model, selected_config)\n",
    "        model_manager.load_model()\n",
    "        qdrant_client = QdrantClient(url=Config.QDRANT_URL)\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "\n",
    "        try:\n",
    "            \n",
    "            dense_emb,sparse_emb = model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0][:512]\n",
    "            sparse_emb=None\n",
    "            \n",
    "            qr = qdrant_client.query_points(\n",
    "                collection_name=Config.COLLECTION_NAME,\n",
    "                query=query_vector, \n",
    "                using=\"dense_vec\",\n",
    "                limit=5\n",
    "            )\n",
    "            \n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr.points]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Semantic search hatasƒ±: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA mevcut mu: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA mevcut mu:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU cihaz sayƒ±sƒ±:\", torch.cuda.device_count())\n",
    "    print(\"≈ûu an kullanƒ±lan cihaz:\", torch.cuda.current_device())\n",
    "    print(\"Cihaz ismi:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hybrid(query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "    model_manager = ModelManager(selected_model, selected_config)\n",
    "    model_manager.load_model()\n",
    "    qdrant_client = QdrantClient(url=Config.QDRANT_URL)\n",
    "    \"\"\"Hybrid search (Dense + Sparse)\"\"\"\n",
    "\n",
    "    try:\n",
    "        dense_emb,sparse_emb = model_manager.encode_texts([query])\n",
    "        #query_vector = dense_emb[0][:512]\n",
    "        \n",
    "\n",
    "        s = sparse_emb[0]\n",
    "        query_sparse_vector = SparseVector(indices=s[\"indices\"], values=s[\"values\"])\n",
    "        qr=qdrant_client.query_points(\n",
    "            collection_name=Config.COLLECTION_NAME,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=query_sparse_vector,  # sparse vector\n",
    "                    using=\"sparse_vec\",\n",
    "                    limit=5,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=dense_emb[0],  # <-- dense vector\n",
    "                    using=\"dense_vec\",\n",
    "                    limit=20,\n",
    "                ),\n",
    "            ],\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        )\n",
    "            \n",
    "        results = [{\"score\": p.score, \"payload\": p.payload} for p in qr.points]\n",
    "        print(f\"üìä {len(results)} sonu√ß bulundu \")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hybrid search hatasƒ±: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ihtiyati tedbir tazminat \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manager = ModelManager(selected_model, selected_config)\n",
    "model_manager.load_model()\n",
    "qdrant_client = QdrantClient(url=Config.QDRANT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_emb,sparse_emb = model_manager.encode_texts([query])\n",
    "query_vector = dense_emb[0][:512]\n",
    "sparse_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "    print(processor.model_manager.get_model_info())\n",
    "    \n",
    "    while True:\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"üîç ARAMA SE√áENEKLERƒ∞\")\n",
    "            print(\"1-only dense\")\n",
    "            print(\"2-dense + sparse\")\n",
    "            choice = input(\"Se√ßiminiz (1/2/3, √ßƒ±kmak i√ßin q): \").strip()\n",
    "            if choice.lower() == 'q':\n",
    "                print(\"√áƒ±kƒ±lƒ±yor...\")\n",
    "                break\n",
    "            if choice not in ['1', '2', '3']:\n",
    "                print(\"Ge√ßersiz se√ßim, tekrar deneyin.\")\n",
    "                continue\n",
    "            query = input(\"Arama sorgusu girin: \").strip()\n",
    "            if not query:\n",
    "                print(\"Bo≈ü sorgu, tekrar deneyin.\")\n",
    "                continue\n",
    "            if choice == '1':\n",
    "                results = search_semantic(query, limit=10, score_threshold=0.6)\n",
    "            elif choice == '2':\n",
    "                results = search_hybrid(query, limit=10, score_threshold=0.6)\n",
    "            print(f\"\\nüìä {len(results)} sonu√ß bulundu:\")\n",
    "            \n",
    "            for idx, r in enumerate(results, 1):\n",
    "                print(f\"{idx}. Score: {r['score']:.4f}, Text: {r['payload'].get('text','')[:200]}...\")  # ilk 200 karakter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\n",
      "{'model_name': 'BAAI/bge-m3', 'model_type': 'bge', 'embedding_dim': 1024, 'description': 'BGE-M3 - √áok dilli, dense+sparse embedding destekli', 'loaded': True}\n",
      "\n",
      "==================================================\n",
      "üîç ARAMA SE√áENEKLERƒ∞\n",
      "1-only dense\n",
      "2-dense + sparse\n",
      "üîÆ Model y√ºkleniyor: BAAI/bge-m3 (bge)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 137218.23it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model y√ºklendi: BAAI/bge-m3\n",
      "üìä 10 sonu√ß bulundu \n",
      "\n",
      "üìä 10 sonu√ß bulundu:\n",
      "1. Score: 1.0000, Text: B√∂lge Adliye Mahkemesinin yukarƒ±da belirtilen kararƒ±na kar≈üƒ± s√ºresi i√ßinde davalƒ± ... vekili temyiz isteminde bulunmu≈ütur. 2. Dairemizin 17.02.2022 tarihli ve 2021/2532 Esas, 2022/901 Karar sayƒ±lƒ± ila...\n",
      "2. Score: 0.4333, Text: arasƒ±nda arsa payƒ± kar≈üƒ±lƒ±ƒüƒ± in≈üaat s√∂zle≈ümesi imzalandƒ±ƒüƒ±nƒ±, s√∂zle≈üme uyarƒ±nca arsa sahibi davacƒ±ya ait 2 ve 3 no.lu parsellerde bulunan paylarƒ±n davacƒ± y√ºklenici ≈üirkete devri kar≈üƒ±lƒ±ƒüƒ±, y√ºklenici ≈ü...\n",
      "3. Score: 0.4000, Text: ki≈üilere devretse bile 3. ki≈üi ve daha sonraki devralanlarƒ±n iyiniyet savunmasƒ±nda bulunmasƒ±nƒ±n m√ºmk√ºn olmadƒ±ƒüƒ±nƒ±, davalƒ± ...‚Äôƒ±n y√ºkleniciye kat kar≈üƒ±lƒ±ƒüƒ± in≈üaat s√∂zle≈ümesi gereƒüi avans olarak verilmi...\n",
      "4. Score: 0.3333, Text: 6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±ndaki tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda ƒ∞lk Derece Mahkemesince davanƒ±...\n",
      "5. Score: 0.2500, Text: kat kar≈üƒ±lƒ±ƒüƒ± in≈üaat s√∂zle≈ümesi gereƒüince arsa sahibine verilmesi kararla≈ütƒ±rƒ±lan (noter kura √ßekimi ile m√ºvekkiline isabet etmi≈ü bulunan) baƒüƒ±msƒ±z b√∂l√ºm olduƒüuna g√∂re artƒ±k y√ºklenicinin edimini yerin...\n",
      "6. Score: 0.2500, Text: vekili cevap dilek√ßesinde √∂zetle; ....A.≈û.'nin diƒüer davalƒ± ≈üirket ile yapmƒ±≈ü olduƒüu s√∂zle≈üme √ßer√ßevesinde davacƒ±ya dava konusu villanƒ±n satƒ±ldƒ±ƒüƒ±nƒ± ve teslim edildiƒüini, dava konusu villanƒ±n tapusunu...\n",
      "7. Score: 0.2292, Text: maddelerinin koruyuculuƒüundan yararlanmasƒ±nƒ±n s√∂z konusu olamayacaƒüƒ±nƒ±, ilk derece mahkemesinin davanƒ±n tahkikatƒ± esnasƒ±nda Yargƒ±tay'ƒ±n baskƒ±n g√∂r√º≈ü√º etrafƒ±nda hareket ederek, arsa sahibine vaat ettiƒü...\n",
      "8. Score: 0.1667, Text: maddesindeki on g√ºnl√ºk s√ºre i√ßerisinde esas hakkƒ±nda dava a√ßmazsa, ihtiyati tedbirin haksƒ±z konulmu≈ü sayƒ±lacaƒüƒ±, haksƒ±z ihtiyati tedbirden dolayƒ± tazminat davasƒ± a√ßan davacƒ±nƒ±n √∂denmesini istediƒüi zar...\n",
      "9. Score: 0.1429, Text: 6. Hukuk Dairesi 2023/4462 E. , 2024/493 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±ndaki r√ºcuen tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda, Mahkemece davanƒ±n kƒ±sm...\n",
      "10. Score: 0.1250, Text: ki≈üi olan diƒüer davalƒ±nƒ±n m√ºlkiyet hakkƒ±nƒ±n doƒümadƒ±ƒüƒ±nƒ±, yapƒ±lan satƒ±≈ü ve devrin sebepten yoksun hale geldiƒüini ve yolsuz tescil durumuna d√º≈üt√ºƒü√ºn√º, buna g√∂re de davalƒ±larƒ±n iyi niyet iddiasƒ±nda bulun...\n",
      "\n",
      "==================================================\n",
      "üîç ARAMA SE√áENEKLERƒ∞\n",
      "1-only dense\n",
      "2-dense + sparse\n",
      "üîÆ Model y√ºkleniyor: BAAI/bge-m3 (bge)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 135591.72it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model y√ºklendi: BAAI/bge-m3\n",
      "üìä 5 sonu√ß bulundu (Dense only)\n",
      "\n",
      "üìä 5 sonu√ß bulundu:\n",
      "1. Score: 0.6177, Text: B√∂lge Adliye Mahkemesinin yukarƒ±da belirtilen kararƒ±na kar≈üƒ± s√ºresi i√ßinde davalƒ± ... vekili temyiz isteminde bulunmu≈ütur. 2. Dairemizin 17.02.2022 tarihli ve 2021/2532 Esas, 2022/901 Karar sayƒ±lƒ± ila...\n",
      "2. Score: 0.6098, Text: 6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±ndaki tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda ƒ∞lk Derece Mahkemesince davanƒ±...\n",
      "3. Score: 0.6022, Text: vekili cevap dilek√ßesinde √∂zetle; ....A.≈û.'nin diƒüer davalƒ± ≈üirket ile yapmƒ±≈ü olduƒüu s√∂zle≈üme √ßer√ßevesinde davacƒ±ya dava konusu villanƒ±n satƒ±ldƒ±ƒüƒ±nƒ± ve teslim edildiƒüini, dava konusu villanƒ±n tapusunu...\n",
      "4. Score: 0.5953, Text: ki≈üilere devretse bile 3. ki≈üi ve daha sonraki devralanlarƒ±n iyiniyet savunmasƒ±nda bulunmasƒ±nƒ±n m√ºmk√ºn olmadƒ±ƒüƒ±nƒ±, davalƒ± ...‚Äôƒ±n y√ºkleniciye kat kar≈üƒ±lƒ±ƒüƒ± in≈üaat s√∂zle≈ümesi gereƒüi avans olarak verilmi...\n",
      "5. Score: 0.5942, Text: maddesindeki on g√ºnl√ºk s√ºre i√ßerisinde esas hakkƒ±nda dava a√ßmazsa, ihtiyati tedbirin haksƒ±z konulmu≈ü sayƒ±lacaƒüƒ±, haksƒ±z ihtiyati tedbirden dolayƒ± tazminat davasƒ± a√ßan davacƒ±nƒ±n √∂denmesini istediƒüi zar...\n",
      "\n",
      "==================================================\n",
      "üîç ARAMA SE√áENEKLERƒ∞\n",
      "1-only dense\n",
      "2-dense + sparse\n",
      "√áƒ±kƒ±lƒ±yor...\n",
      "\n",
      "üîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\n",
      "{'model_name': 'BAAI/bge-m3', 'model_type': 'bge', 'embedding_dim': 1024, 'description': 'BGE-M3 - √áok dilli, dense+sparse embedding destekli', 'loaded': True}\n",
      "\n",
      "==================================================\n",
      "üîç ARAMA SE√áENEKLERƒ∞\n",
      "1-only dense\n",
      "2-dense + sparse\n",
      "√áƒ±kƒ±lƒ±yor...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "search()\n",
    "print(search())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihtiyati tedbir tazminat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
