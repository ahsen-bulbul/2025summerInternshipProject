{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# Yargƒ±tay Kararlarƒ± i√ßin Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarƒ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 768  # embeddinggemma-300m embedding boyutu\n",
    "\n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 256\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargƒ±tay kararlarƒ± i√ßin semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"‚úÖ SemChunk hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ Google EmbeddingGemma modeli hazƒ±r\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 256) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "            all_embeddings.extend(batch_embeddings.tolist())\n",
    "            print(f\"  üîπ Embedding olu≈üturuldu: {i + len(batch_texts)}/{total}\")\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"üìÑ Toplam {total_rows} satƒ±r i≈ülenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"üîÆ Embedding'ler olu≈üturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"üöÄ {total_points} chunk Qdrant'a y√ºkleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(f\"üéâ {total_points} chunk Qdrant'a y√ºklendi!\")\n",
    "\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"Semantic arama yap\"\"\"\n",
    "        query_embedding = self.model.encode([query])[0]\n",
    "        search_results = self.qdrant_client.search(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Pipeline sƒ±nƒ±fƒ±\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Yargƒ±tay Semantic Pipeline Ba≈ülƒ±yor\")\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå ƒ∞≈ülenecek chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        while True:\n",
    "            query = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            results = self.processor.search_semantic(query, limit=limit)\n",
    "            for i, r in enumerate(results, 1):\n",
    "                payload = r['payload']\n",
    "                text_preview = payload.get('text', '')[:300] + \"...\"\n",
    "                print(f\"\\n{i}. Score: {r['score']:.3f}, Esas No: {payload.get('esas_no')}, Metin: {text_preview}\")\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_embeddinggemma_chunks\",\n",
    "        DIMENSION=768\n",
    "    )\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n1. Full pipeline √ßalƒ±≈ütƒ±r\\n2. Arama yap\\n3. Koleksiyon bilgisi\\n4. √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßim: \")\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
