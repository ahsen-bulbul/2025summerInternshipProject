{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "âœ… SemChunk hazÄ±r (Token boyutu: 512)\n",
      "âœ… Google EmbeddingGemma modeli hazÄ±r\n",
      "âœ… Qdrant client hazÄ±r (http://localhost:6333)\n",
      "\n",
      "1. Full pipeline Ã§alÄ±ÅŸtÄ±r\n",
      "2. Arama yap\n",
      "3. Koleksiyon bilgisi\n",
      "4. Ã‡Ä±kÄ±ÅŸ\n",
      "ðŸ” Ä°nteraktif Arama BaÅŸlatÄ±ldÄ±\n",
      "ðŸ“Š Koleksiyon Durumu: {\n",
      "  \"collection_name\": \"google_embeddinggemma_chunks\",\n",
      "  \"points_count\": 0,\n",
      "  \"vectors_count\": null,\n",
      "  \"status\": \"green\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "ðŸ” Query vector boyutu: 512 (hedef: 512)\n",
      "âŒ HiÃ§ sonuÃ§ bulunamadÄ±. Score threshold'u dÃ¼ÅŸÃ¼rmeyi deneyin.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3493654/2839841565.py:250: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ” Query vector boyutu: 512 (hedef: 512)\n",
      "âŒ HiÃ§ sonuÃ§ bulunamadÄ±. Score threshold'u dÃ¼ÅŸÃ¼rmeyi deneyin.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ðŸ” Query vector boyutu: 512 (hedef: 512)\n",
      "âŒ HiÃ§ sonuÃ§ bulunamadÄ±. Score threshold'u dÃ¼ÅŸÃ¼rmeyi deneyin.\n",
      "============================================================\n",
      "\n",
      "1. Full pipeline Ã§alÄ±ÅŸtÄ±r\n",
      "2. Arama yap\n",
      "3. Koleksiyon bilgisi\n",
      "4. Ã‡Ä±kÄ±ÅŸ\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# YargÄ±tay KararlarÄ± iÃ§in Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarÄ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarÄ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarÄ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512  #collection boyutu\n",
    "\n",
    "    # Dosya ayarlarÄ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE: int = 256\n",
    " \n",
    "    \n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"YargÄ±tay kararlarÄ± iÃ§in semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        # Reducer modeli - tek instance kullanacaÄŸÄ±z\n",
    "        self.reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"âœ… SemChunk hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… Google EmbeddingGemma modeli hazÄ±r\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ðŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_embed = time.time()\n",
    "            \n",
    "            # Embedding oluÅŸtur\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True, convert_to_tensor=True).to('cuda:0')\n",
    "            batch_embeddings = batch_embeddings.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            # Boyut dÃ¼ÅŸÃ¼r\n",
    "            reduced_vector = self.reducer(batch_embeddings)\n",
    "            print(\"*\" * 40)\n",
    "            print(f\"Reduced vector shape: {reduced_vector.shape}\")\n",
    "            print(\"*\" * 40)\n",
    "            \n",
    "            # CPU'ya taÅŸÄ± ve listeye Ã§evir\n",
    "            all_embeddings.extend(reduced_vector.cpu().tolist())\n",
    "            \n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding sÃ¼resi: {end_embed - start_embed:.2f} saniye\")\n",
    "            print(f\"  ðŸ”¹ Embedding oluÅŸturuldu: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"ðŸ“„ Toplam {total_rows} satÄ±r iÅŸlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ðŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"ðŸ”® Embedding'ler oluÅŸturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"ðŸš€ {total_points} chunk Qdrant'a yÃ¼kleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                start_upload = time.time()\n",
    "\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "\n",
    "                end_upload = time.time()\n",
    "                print(f\"batch Qdrant yÃ¼kleme sÃ¼resi: {end_upload - start_upload:.2f} saniye\")\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(f\"ðŸŽ‰ {total_points} chunk Qdrant'a yÃ¼klendi!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"DÃ¼zeltilmiÅŸ search metodu - 512 dimension uyumlu\"\"\"\n",
    "        \n",
    "        # Query embedding Ã§Ä±kar (768 boyut)\n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=True).to('cuda:0')\n",
    "        query_embedding = query_embedding.clone().detach()\n",
    "        \n",
    "        # Boyutu 512'ye dÃ¼ÅŸÃ¼r (aynÄ± reducer kullanarak)\n",
    "        with torch.no_grad():  # Inference modunda\n",
    "            reduced_query_embedding = self.reducer(query_embedding)\n",
    "        \n",
    "        # CPU'ya taÅŸÄ± ve numpy array'e Ã§evir\n",
    "        query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "        \n",
    "        print(f\"ðŸ” Query vector boyutu: {len(query_vector)} (hedef: {self.config.DIMENSION})\")\n",
    "        \n",
    "        # Qdrant aramasÄ±\n",
    "        search_results = self.qdrant_client.search(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Pipeline sÄ±nÄ±fÄ±\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ðŸš€ YargÄ±tay Semantic Pipeline BaÅŸlÄ±yor\")\n",
    "\n",
    "        total_start = time.time()  # Toplam sÃ¼re baÅŸlangÄ±cÄ±\n",
    "\n",
    "        # Koleksiyon oluÅŸturma\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        # CSV iÅŸlemleri ve chunk oluÅŸturma\n",
    "        chunk_start = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        chunk_end = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"âŒ Ä°ÅŸlenecek chunk bulunamadÄ±\")\n",
    "            return False\n",
    "\n",
    "        # Embedding oluÅŸturma ve Qdrant yÃ¼kleme\n",
    "        upload_start = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        upload_end = time.time()\n",
    "\n",
    "        total_end = time.time()  # Toplam sÃ¼re bitiÅŸi\n",
    "\n",
    "        # Toplam istatistikler\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nðŸ“Š Pipeline SÃ¼releri ve Ä°statistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(chunk_end - chunk_start, 2),\n",
    "            \"embedding_and_upload_time_s\": round(upload_end - upload_start, 2),\n",
    "            \"total_pipeline_time_s\": round(total_end - total_start, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"ðŸ” Ä°nteraktif Arama BaÅŸlatÄ±ldÄ±\")\n",
    "        \n",
    "        # Ã–nce koleksiyon durumunu kontrol et\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(f\"ðŸ“Š Koleksiyon Durumu: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nðŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                print(\"âŒ BoÅŸ sorgu, tekrar deneyin\")\n",
    "                continue\n",
    "            \n",
    "            limit_input = input(\"KaÃ§ sonuÃ§? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "                limit = max(1, min(limit, 50))  # 1-50 arasÄ± sÄ±nÄ±rla\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz sayÄ±, varsayÄ±lan 5 kullanÄ±lÄ±yor\")\n",
    "                limit = 5\n",
    "\n",
    "            score_input = input(\"Minimum score? (default 0.7): \").strip()\n",
    "            try:\n",
    "                score_threshold = float(score_input) if score_input else 0.7\n",
    "                score_threshold = max(0.0, min(score_threshold, 1.0))  # 0-1 arasÄ± sÄ±nÄ±rla\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz score, varsayÄ±lan 0.7 kullanÄ±lÄ±yor\")\n",
    "                score_threshold = 0.7\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            results = self.processor.search_semantic(query, limit=limit, score_threshold=score_threshold)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"âŒ HiÃ§ sonuÃ§ bulunamadÄ±. Score threshold'u dÃ¼ÅŸÃ¼rmeyi deneyin.\")\n",
    "            else:\n",
    "                print(f\"\\nðŸ“‹ {len(results)} SonuÃ§ Bulundu:\")\n",
    "                for i, r in enumerate(results, 1):\n",
    "                    payload = r['payload']\n",
    "                    text_preview = payload.get('text', '')[:200] + \"...\" if len(payload.get('text', '')) > 200 else payload.get('text', '')\n",
    "                    print(f\"\\n{i}. ðŸ“Š Score: {r['score']:.4f}\")\n",
    "                    print(f\"   ðŸ“‹ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                    print(f\"   ðŸ“‹ Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                    print(f\"   ðŸ“‹ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                    print(f\"   ðŸ“‹ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                    print(f\"   ðŸ“„ Metin: {text_preview}\")\n",
    "                    print(f\"   {'â”€'*50}\")\n",
    "            \n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_embeddinggemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"1. Full pipeline Ã§alÄ±ÅŸtÄ±r\")\n",
    "        print(\"2. Arama yap\") \n",
    "        print(\"3. Koleksiyon bilgisi\")\n",
    "        print(\"4. ðŸ”§ Debug arama sorunu\")\n",
    "        print(\"5. ðŸ—‘ï¸ Dimension uyumsuzluÄŸunu dÃ¼zelt\")\n",
    "        print(\"6. Ã‡Ä±kÄ±ÅŸ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        choice = input(\"SeÃ§im: \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "            \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "            \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nðŸ“Š Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "            \n",
    "        elif choice == \"4\":\n",
    "            query = input(\"Debug iÃ§in test query (Enter: 'test'): \").strip() or \"test\"\n",
    "            pipeline.processor.debug_search_issue(query)\n",
    "            \n",
    "        elif choice == \"5\":\n",
    "            if pipeline.processor.fix_dimension_mismatch():\n",
    "                print(\"âœ… Koleksiyon sÄ±fÄ±rlandÄ±. Åžimdi '1' seÃ§eneÄŸi ile veriyi yeniden yÃ¼kleyin.\")\n",
    "            else:\n",
    "                print(\"âŒ Ä°ÅŸlem iptal edildi\")\n",
    "                \n",
    "        elif choice == \"6\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# YargÄ±tay KararlarÄ± iÃ§in Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from torch import nn \n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarÄ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarÄ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarÄ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512  #collection boyutu\n",
    "\n",
    "    # Dosya ayarlarÄ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE=256\n",
    " \n",
    "    \n",
    "    \n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"YargÄ±tay kararlarÄ± iÃ§in semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        self.reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"âœ… SemChunk hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… Google EmbeddingGemma modeli hazÄ±r\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ðŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100, target_dim: int=512) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_embed = time.time()\n",
    "            \n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True, convert_to_tensor=True).to('cuda:0')\n",
    "            batch_embeddings = batch_embeddings.clone().detach().requires_grad_(True) #kopyasÄ± alÄ±ndÄ± baÄŸÄ±msÄ±z oldu ve tekrar gradyan hesaplanÄ±labilir versiyona getirildi\n",
    "            reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "            reduced_vector = reducer(batch_embeddings)\n",
    "            print(\"*\"*40)\n",
    "            print(reduced_vector.shape)\n",
    "            print(\"*\"*40)\n",
    "            all_embeddings.extend(reduced_vector.tolist())\n",
    "            \n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding sÃ¼resi: {end_embed - start_embed:.2f} saniye\")\n",
    "            print(f\"  ðŸ”¹ Embedding oluÅŸturuldu: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        # all_embeddings=np.array(all_embeddings)\n",
    "        # print(f\"ðŸ”¹ PCA ile boyut dÃ¼ÅŸÃ¼rÃ¼lÃ¼yor: {all_embeddings.shape[1]} -> {target_dim}\")\n",
    "        # pca = PCA(n_components=target_dim)\n",
    "        # reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    "        # print(f\"âœ… PCA tamamlandÄ±, shape: {reduced_embeddings.shape}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"ðŸ“„ Toplam {total_rows} satÄ±r iÅŸlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            #start_chunk = time.time()\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            #end_chunk = time.time()\n",
    "\n",
    "            \n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ðŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"ðŸ”® Embedding'ler oluÅŸturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"ðŸš€ {total_points} chunk Qdrant'a yÃ¼kleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                start_upload = time.time()\n",
    "\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "\n",
    "                end_upload = time.time()\n",
    "                print(f\"batch Qdrant yÃ¼kleme sÃ¼resi: {end_upload - start_upload:.2f} saniye\")\n",
    "\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(f\"ðŸŽ‰ {total_points} chunk Qdrant'a yÃ¼klendi!\")\n",
    "\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        # Query embedding Ã§Ä±kar\n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=True)\n",
    "        query_embedding = query_embedding.clone().detach().to('cuda:0')  # ðŸ”¹ clone ve detach eklendi\n",
    "        query_embedding = self.reducer(query_embedding)\n",
    "        query_embedding = query_embedding[0].cpu().tolist()  # numpy list\n",
    "        \n",
    "        # Qdrant aramasÄ±\n",
    "        search_results = self.qdrant_client.search(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Pipeline sÄ±nÄ±fÄ±\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ðŸš€ YargÄ±tay Semantic Pipeline BaÅŸlÄ±yor\")\n",
    "\n",
    "        total_start = time.time()  # Toplam sÃ¼re baÅŸlangÄ±cÄ±\n",
    "\n",
    "        # Koleksiyon oluÅŸturma\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        # CSV iÅŸlemleri ve chunk oluÅŸturma\n",
    "        chunk_start = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        chunk_end = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"âŒ Ä°ÅŸlenecek chunk bulunamadÄ±\")\n",
    "            return False\n",
    "\n",
    "        # Embedding oluÅŸturma ve Qdrant yÃ¼kleme\n",
    "        upload_start = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        upload_end = time.time()\n",
    "\n",
    "        total_end = time.time()  # Toplam sÃ¼re bitiÅŸi\n",
    "\n",
    "        # Toplam istatistikler\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nðŸ“Š Pipeline SÃ¼releri ve Ä°statistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(chunk_end - chunk_start, 2),\n",
    "            \"embedding_and_upload_time_s\": round(upload_end - upload_start, 2),\n",
    "            \"total_pipeline_time_s\": round(total_end - total_start, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        while True:\n",
    "            query = input(\"ðŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            limit_input = input(\"KaÃ§ sonuÃ§? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz sayÄ±, varsayÄ±lan 5 kullanÄ±lÄ±yor\")\n",
    "                limit = 5\n",
    "\n",
    "            results = self.processor.search_semantic(query, limit=limit)\n",
    "            for i, r in enumerate(results, 1):\n",
    "                payload = r['payload']\n",
    "                text_preview = payload.get('text', '')[:300] + \"...\"\n",
    "                print(f\"\\n{i}. Score: {r['score']:.3f}, Esas No: {payload.get('esas_no')}, Metin: {text_preview}\")\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_embeddinggemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n1. Full pipeline Ã§alÄ±ÅŸtÄ±r\\n2. Arama yap\\n3. Koleksiyon bilgisi\\n4. Ã‡Ä±kÄ±ÅŸ\")\n",
    "        choice = input(\"SeÃ§im: \")\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Model oluÅŸtur\n",
    "reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "\n",
    "# Embed iÅŸlemi\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\", device=device)\n",
    "dense_vector = model.encode(texts, convert_to_tensor=True).to('cuda:0')\n",
    "dense_vector = dense_vector.clone().detach().requires_grad_(True) \n",
    "\n",
    "print(reducer)\n",
    "\n",
    "reduced_vector = reducer(dense_vector)\n",
    "\n",
    "print(reduced_vector.shape)\n",
    "print(reduced_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# YargÄ±tay KararlarÄ± iÃ§in Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ------------------------ ENV ------------------------\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# ------------------------ EmbedReducer ------------------------\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# ------------------------ Config ------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE: int = 256\n",
    "\n",
    "# ------------------------ Processor ------------------------\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"YargÄ±tay kararlarÄ± iÃ§in semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        self.reducer = EmbedReducer(768, config.DIMENSION).to('cuda:0')\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"âœ… SemChunk hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… Google EmbeddingGemma modeli hazÄ±r\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n",
    "\n",
    "    # ------------------------ Qdrant ------------------------\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(name)\n",
    "                print(f\"ðŸ—‘ï¸ Eski koleksiyon silindi: {name}\")\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            collections = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if name not in collections:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {name}\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    # ------------------------ Chunking ------------------------\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if chunk.strip():\n",
    "                data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk)),\n",
    "                    'char_count': len(chunk),\n",
    "                }\n",
    "                if metadata:\n",
    "                    data.update(metadata)\n",
    "                result.append(data)\n",
    "        return result\n",
    "\n",
    "    # ------------------------ Embedding ------------------------\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_time = time.time()\n",
    "\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch_texts, show_progress_bar=True, convert_to_tensor=True\n",
    "            ).to('cuda:0')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_vectors = self.reducer(batch_embeddings).cpu().tolist()\n",
    "\n",
    "            all_embeddings.extend(reduced_vectors)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"Batch embedding sÃ¼resi: {end_time - start_time:.2f}s | {i+len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # ------------------------ CSV ------------------------\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"ðŸ“„ Toplam {total_rows} satÄ±r iÅŸlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{total_rows} | Toplam chunk: {len(all_chunks)}\")\n",
    "\n",
    "        print(f\"ðŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    # ------------------------ Upload ------------------------\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        print(\"ðŸ”® Embedding'ler oluÅŸturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(id=str(uuid.uuid4()), vector=e, payload=c)\n",
    "            for c, e in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"ðŸš€ {total_points} chunk Qdrant'a yÃ¼kleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i+batch_size]\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                print(f\"Batch yÃ¼kleme sÃ¼resi: {end_time - start_time:.2f}s | {min(i+batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(f\"ðŸŽ‰ {total_points} chunk Qdrant'a yÃ¼klendi!\")\n",
    "\n",
    "    # ------------------------ Search ------------------------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "    # Query embedding Ã§Ä±kar\n",
    "        query_emb = self.model.encode([query], convert_to_tensor=True).to('cuda:0')\n",
    "        with torch.no_grad():\n",
    "            query_emb = self.reducer(query_emb)\n",
    "        query_emb = query_emb[0].cpu().tolist()\n",
    "\n",
    "        # Qdrant aramasÄ± (search kullanÄ±mÄ±)\n",
    "        search_results = self.qdrant_client.search(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query_vector=query_emb,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        results = [{'score': r.score, 'payload': r.payload} for r in search_results]\n",
    "        return results\n",
    "\n",
    "\n",
    "    # ------------------------ Collection Info ------------------------\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# ------------------------ Pipeline ------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ðŸš€ YargÄ±tay Semantic Pipeline BaÅŸlÄ±yor\")\n",
    "\n",
    "        start_total = time.time()\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        start_chunk = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        end_chunk = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"âŒ Ä°ÅŸlenecek chunk yok\")\n",
    "            return False\n",
    "\n",
    "        start_upload = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        end_upload = time.time()\n",
    "\n",
    "        end_total = time.time()\n",
    "        info = self.processor.get_collection_info()\n",
    "\n",
    "        print(\"\\nðŸ“Š Pipeline SÃ¼releri ve Ä°statistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(end_chunk - start_chunk, 2),\n",
    "            \"embedding_and_upload_time_s\": round(end_upload - start_upload, 2),\n",
    "            \"total_pipeline_time_s\": round(end_total - start_total, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    # ------------------------ Interactive Search ------------------------\n",
    "    def interactive_search(self):\n",
    "        while True:\n",
    "            query = input(\"ðŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "\n",
    "            limit_input = input(\"KaÃ§ sonuÃ§? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz sayÄ±, varsayÄ±lan 5 kullanÄ±lÄ±yor\")\n",
    "                limit = 5\n",
    "\n",
    "            results = self.processor.search_semantic(query, limit=limit)\n",
    "            for i, r in enumerate(results, 1):\n",
    "                payload = r['payload']\n",
    "                text_preview = payload.get('text', '')[:300] + \"...\"\n",
    "                print(f\"\\n{i}. Score: {r['score']:.3f}, Esas No: {payload.get('esas_no')}, Metin: {text_preview}\")\n",
    "\n",
    "# ------------------------ Main ------------------------\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_embeddinggemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n1. Full pipeline Ã§alÄ±ÅŸtÄ±r\\n2. Arama yap\\n3. Koleksiyon bilgisi\\n4. Ã‡Ä±kÄ±ÅŸ\")\n",
    "        choice = input(\"SeÃ§im: \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
