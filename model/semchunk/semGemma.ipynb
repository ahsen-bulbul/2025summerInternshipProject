{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "✅ SemChunk hazır (Token boyutu: 512)\n",
      "✅ Google EmbeddingGemma modeli hazır\n",
      "✅ Qdrant client hazır (http://localhost:6333)\n",
      "\n",
      "1. Full pipeline çalıştır\n",
      "2. Arama yap\n",
      "3. Koleksiyon bilgisi\n",
      "4. Çıkış\n",
      "🔍 İnteraktif Arama Başlatıldı\n",
      "📊 Koleksiyon Durumu: {\n",
      "  \"collection_name\": \"google_embeddinggemma_chunks\",\n",
      "  \"points_count\": 0,\n",
      "  \"vectors_count\": null,\n",
      "  \"status\": \"green\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "🔍 Query vector boyutu: 512 (hedef: 512)\n",
      "❌ Hiç sonuç bulunamadı. Score threshold'u düşürmeyi deneyin.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3493654/2839841565.py:250: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔍 Query vector boyutu: 512 (hedef: 512)\n",
      "❌ Hiç sonuç bulunamadı. Score threshold'u düşürmeyi deneyin.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "🔍 Query vector boyutu: 512 (hedef: 512)\n",
      "❌ Hiç sonuç bulunamadı. Score threshold'u düşürmeyi deneyin.\n",
      "============================================================\n",
      "\n",
      "1. Full pipeline çalıştır\n",
      "2. Arama yap\n",
      "3. Koleksiyon bilgisi\n",
      "4. Çıkış\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# Yargıtay Kararları için Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Konfigürasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarları\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarları\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarları\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512  #collection boyutu\n",
    "\n",
    "    # Dosya ayarları\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE: int = 256\n",
    " \n",
    "    \n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargıtay kararları için semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        # Reducer modeli - tek instance kullanacağız\n",
    "        self.reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"✅ SemChunk hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ Google EmbeddingGemma modeli hazır\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_embed = time.time()\n",
    "            \n",
    "            # Embedding oluştur\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True, convert_to_tensor=True).to('cuda:0')\n",
    "            batch_embeddings = batch_embeddings.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            # Boyut düşür\n",
    "            reduced_vector = self.reducer(batch_embeddings)\n",
    "            print(\"*\" * 40)\n",
    "            print(f\"Reduced vector shape: {reduced_vector.shape}\")\n",
    "            print(\"*\" * 40)\n",
    "            \n",
    "            # CPU'ya taşı ve listeye çevir\n",
    "            all_embeddings.extend(reduced_vector.cpu().tolist())\n",
    "            \n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding süresi: {end_embed - start_embed:.2f} saniye\")\n",
    "            print(f\"  🔹 Embedding oluşturuldu: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"📄 Toplam {total_rows} satır işlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"🔮 Embedding'ler oluşturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"🚀 {total_points} chunk Qdrant'a yükleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                start_upload = time.time()\n",
    "\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "\n",
    "                end_upload = time.time()\n",
    "                print(f\"batch Qdrant yükleme süresi: {end_upload - start_upload:.2f} saniye\")\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(f\"🎉 {total_points} chunk Qdrant'a yüklendi!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"Düzeltilmiş search metodu - 512 dimension uyumlu\"\"\"\n",
    "        \n",
    "        # Query embedding çıkar (768 boyut)\n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=True).to('cuda:0')\n",
    "        query_embedding = query_embedding.clone().detach()\n",
    "        \n",
    "        # Boyutu 512'ye düşür (aynı reducer kullanarak)\n",
    "        with torch.no_grad():  # Inference modunda\n",
    "            reduced_query_embedding = self.reducer(query_embedding)\n",
    "        \n",
    "        # CPU'ya taşı ve numpy array'e çevir\n",
    "        query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "        \n",
    "        print(f\"🔍 Query vector boyutu: {len(query_vector)} (hedef: {self.config.DIMENSION})\")\n",
    "        \n",
    "        # Qdrant araması\n",
    "        search_results = self.qdrant_client.search(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "        return results\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Pipeline sınıfı\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Yargıtay Semantic Pipeline Başlıyor\")\n",
    "\n",
    "        total_start = time.time()  # Toplam süre başlangıcı\n",
    "\n",
    "        # Koleksiyon oluşturma\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        # CSV işlemleri ve chunk oluşturma\n",
    "        chunk_start = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        chunk_end = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk bulunamadı\")\n",
    "            return False\n",
    "\n",
    "        # Embedding oluşturma ve Qdrant yükleme\n",
    "        upload_start = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        upload_end = time.time()\n",
    "\n",
    "        total_end = time.time()  # Toplam süre bitişi\n",
    "\n",
    "        # Toplam istatistikler\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Pipeline Süreleri ve İstatistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(chunk_end - chunk_start, 2),\n",
    "            \"embedding_and_upload_time_s\": round(upload_end - upload_start, 2),\n",
    "            \"total_pipeline_time_s\": round(total_end - total_start, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"🔍 İnteraktif Arama Başlatıldı\")\n",
    "        \n",
    "        # Önce koleksiyon durumunu kontrol et\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(f\"📊 Koleksiyon Durumu: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\n🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                print(\"❌ Boş sorgu, tekrar deneyin\")\n",
    "                continue\n",
    "            \n",
    "            limit_input = input(\"Kaç sonuç? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "                limit = max(1, min(limit, 50))  # 1-50 arası sınırla\n",
    "            except ValueError:\n",
    "                print(\"❌ Geçersiz sayı, varsayılan 5 kullanılıyor\")\n",
    "                limit = 5\n",
    "\n",
    "            score_input = input(\"Minimum score? (default 0.7): \").strip()\n",
    "            try:\n",
    "                score_threshold = float(score_input) if score_input else 0.7\n",
    "                score_threshold = max(0.0, min(score_threshold, 1.0))  # 0-1 arası sınırla\n",
    "            except ValueError:\n",
    "                print(\"❌ Geçersiz score, varsayılan 0.7 kullanılıyor\")\n",
    "                score_threshold = 0.7\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            results = self.processor.search_semantic(query, limit=limit, score_threshold=score_threshold)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"❌ Hiç sonuç bulunamadı. Score threshold'u düşürmeyi deneyin.\")\n",
    "            else:\n",
    "                print(f\"\\n📋 {len(results)} Sonuç Bulundu:\")\n",
    "                for i, r in enumerate(results, 1):\n",
    "                    payload = r['payload']\n",
    "                    text_preview = payload.get('text', '')[:200] + \"...\" if len(payload.get('text', '')) > 200 else payload.get('text', '')\n",
    "                    print(f\"\\n{i}. 📊 Score: {r['score']:.4f}\")\n",
    "                    print(f\"   📋 Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                    print(f\"   📋 Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                    print(f\"   📋 Daire: {payload.get('daire', 'N/A')}\")\n",
    "                    print(f\"   📋 Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                    print(f\"   📄 Metin: {text_preview}\")\n",
    "                    print(f\"   {'─'*50}\")\n",
    "            \n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_embeddinggemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"1. Full pipeline çalıştır\")\n",
    "        print(\"2. Arama yap\") \n",
    "        print(\"3. Koleksiyon bilgisi\")\n",
    "        print(\"4. 🔧 Debug arama sorunu\")\n",
    "        print(\"5. 🗑️ Dimension uyumsuzluğunu düzelt\")\n",
    "        print(\"6. Çıkış\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        choice = input(\"Seçim: \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "            \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "            \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "            \n",
    "        elif choice == \"4\":\n",
    "            query = input(\"Debug için test query (Enter: 'test'): \").strip() or \"test\"\n",
    "            pipeline.processor.debug_search_issue(query)\n",
    "            \n",
    "        elif choice == \"5\":\n",
    "            if pipeline.processor.fix_dimension_mismatch():\n",
    "                print(\"✅ Koleksiyon sıfırlandı. Şimdi '1' seçeneği ile veriyi yeniden yükleyin.\")\n",
    "            else:\n",
    "                print(\"❌ İşlem iptal edildi\")\n",
    "                \n",
    "        elif choice == \"6\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# Yargıtay Kararları için Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from torch import nn \n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Konfigürasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarları\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarları\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarları\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512  #collection boyutu\n",
    "\n",
    "    # Dosya ayarları\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE=256\n",
    " \n",
    "    \n",
    "    \n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargıtay kararları için semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        self.reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"✅ SemChunk hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ Google EmbeddingGemma modeli hazır\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100, target_dim: int=512) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_embed = time.time()\n",
    "            \n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True, convert_to_tensor=True).to('cuda:0')\n",
    "            batch_embeddings = batch_embeddings.clone().detach().requires_grad_(True) #kopyası alındı bağımsız oldu ve tekrar gradyan hesaplanılabilir versiyona getirildi\n",
    "            reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "            reduced_vector = reducer(batch_embeddings)\n",
    "            print(\"*\"*40)\n",
    "            print(reduced_vector.shape)\n",
    "            print(\"*\"*40)\n",
    "            all_embeddings.extend(reduced_vector.tolist())\n",
    "            \n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding süresi: {end_embed - start_embed:.2f} saniye\")\n",
    "            print(f\"  🔹 Embedding oluşturuldu: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        # all_embeddings=np.array(all_embeddings)\n",
    "        # print(f\"🔹 PCA ile boyut düşürülüyor: {all_embeddings.shape[1]} -> {target_dim}\")\n",
    "        # pca = PCA(n_components=target_dim)\n",
    "        # reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    "        # print(f\"✅ PCA tamamlandı, shape: {reduced_embeddings.shape}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"📄 Toplam {total_rows} satır işlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            #start_chunk = time.time()\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            #end_chunk = time.time()\n",
    "\n",
    "            \n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"🔮 Embedding'ler oluşturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"🚀 {total_points} chunk Qdrant'a yükleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                start_upload = time.time()\n",
    "\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "\n",
    "                end_upload = time.time()\n",
    "                print(f\"batch Qdrant yükleme süresi: {end_upload - start_upload:.2f} saniye\")\n",
    "\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(f\"🎉 {total_points} chunk Qdrant'a yüklendi!\")\n",
    "\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        # Query embedding çıkar\n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=True)\n",
    "        query_embedding = query_embedding.clone().detach().to('cuda:0')  # 🔹 clone ve detach eklendi\n",
    "        query_embedding = self.reducer(query_embedding)\n",
    "        query_embedding = query_embedding[0].cpu().tolist()  # numpy list\n",
    "        \n",
    "        # Qdrant araması\n",
    "        search_results = self.qdrant_client.search(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        \n",
    "        results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Pipeline sınıfı\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Yargıtay Semantic Pipeline Başlıyor\")\n",
    "\n",
    "        total_start = time.time()  # Toplam süre başlangıcı\n",
    "\n",
    "        # Koleksiyon oluşturma\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        # CSV işlemleri ve chunk oluşturma\n",
    "        chunk_start = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        chunk_end = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk bulunamadı\")\n",
    "            return False\n",
    "\n",
    "        # Embedding oluşturma ve Qdrant yükleme\n",
    "        upload_start = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        upload_end = time.time()\n",
    "\n",
    "        total_end = time.time()  # Toplam süre bitişi\n",
    "\n",
    "        # Toplam istatistikler\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Pipeline Süreleri ve İstatistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(chunk_end - chunk_start, 2),\n",
    "            \"embedding_and_upload_time_s\": round(upload_end - upload_start, 2),\n",
    "            \"total_pipeline_time_s\": round(total_end - total_start, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        while True:\n",
    "            query = input(\"🔍 Arama metni (çıkmak için 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            limit_input = input(\"Kaç sonuç? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "            except ValueError:\n",
    "                print(\"❌ Geçersiz sayı, varsayılan 5 kullanılıyor\")\n",
    "                limit = 5\n",
    "\n",
    "            results = self.processor.search_semantic(query, limit=limit)\n",
    "            for i, r in enumerate(results, 1):\n",
    "                payload = r['payload']\n",
    "                text_preview = payload.get('text', '')[:300] + \"...\"\n",
    "                print(f\"\\n{i}. Score: {r['score']:.3f}, Esas No: {payload.get('esas_no')}, Metin: {text_preview}\")\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_embeddinggemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n1. Full pipeline çalıştır\\n2. Arama yap\\n3. Koleksiyon bilgisi\\n4. Çıkış\")\n",
    "        choice = input(\"Seçim: \")\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Model oluştur\n",
    "reducer = EmbedReducer(768, 512).to('cuda:0')\n",
    "\n",
    "# Embed işlemi\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\", device=device)\n",
    "dense_vector = model.encode(texts, convert_to_tensor=True).to('cuda:0')\n",
    "dense_vector = dense_vector.clone().detach().requires_grad_(True) \n",
    "\n",
    "print(reducer)\n",
    "\n",
    "reduced_vector = reducer(dense_vector)\n",
    "\n",
    "print(reduced_vector.shape)\n",
    "print(reduced_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# Yargıtay Kararları için Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ------------------------ ENV ------------------------\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# ------------------------ EmbedReducer ------------------------\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# ------------------------ Config ------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE: int = 256\n",
    "\n",
    "# ------------------------ Processor ------------------------\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargıtay kararları için semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        self.reducer = EmbedReducer(768, config.DIMENSION).to('cuda:0')\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"✅ SemChunk hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ Google EmbeddingGemma modeli hazır\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n",
    "\n",
    "    # ------------------------ Qdrant ------------------------\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {name}\")\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            collections = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if name not in collections:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {name}\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    # ------------------------ Chunking ------------------------\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if chunk.strip():\n",
    "                data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk)),\n",
    "                    'char_count': len(chunk),\n",
    "                }\n",
    "                if metadata:\n",
    "                    data.update(metadata)\n",
    "                result.append(data)\n",
    "        return result\n",
    "\n",
    "    # ------------------------ Embedding ------------------------\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_time = time.time()\n",
    "\n",
    "            batch_embeddings = self.model.encode(\n",
    "                batch_texts, show_progress_bar=True, convert_to_tensor=True\n",
    "            ).to('cuda:0')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_vectors = self.reducer(batch_embeddings).cpu().tolist()\n",
    "\n",
    "            all_embeddings.extend(reduced_vectors)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"Batch embedding süresi: {end_time - start_time:.2f}s | {i+len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # ------------------------ CSV ------------------------\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"📄 Toplam {total_rows} satır işlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{total_rows} | Toplam chunk: {len(all_chunks)}\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    # ------------------------ Upload ------------------------\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        print(\"🔮 Embedding'ler oluşturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(id=str(uuid.uuid4()), vector=e, payload=c)\n",
    "            for c, e in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"🚀 {total_points} chunk Qdrant'a yükleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i+batch_size]\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                end_time = time.time()\n",
    "                print(f\"Batch yükleme süresi: {end_time - start_time:.2f}s | {min(i+batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(f\"🎉 {total_points} chunk Qdrant'a yüklendi!\")\n",
    "\n",
    "    # ------------------------ Search ------------------------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "    # Query embedding çıkar\n",
    "        query_emb = self.model.encode([query], convert_to_tensor=True).to('cuda:0')\n",
    "        with torch.no_grad():\n",
    "            query_emb = self.reducer(query_emb)\n",
    "        query_emb = query_emb[0].cpu().tolist()\n",
    "\n",
    "        # Qdrant araması (search kullanımı)\n",
    "        search_results = self.qdrant_client.search(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query_vector=query_emb,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        results = [{'score': r.score, 'payload': r.payload} for r in search_results]\n",
    "        return results\n",
    "\n",
    "\n",
    "    # ------------------------ Collection Info ------------------------\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# ------------------------ Pipeline ------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Yargıtay Semantic Pipeline Başlıyor\")\n",
    "\n",
    "        start_total = time.time()\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        start_chunk = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        end_chunk = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk yok\")\n",
    "            return False\n",
    "\n",
    "        start_upload = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        end_upload = time.time()\n",
    "\n",
    "        end_total = time.time()\n",
    "        info = self.processor.get_collection_info()\n",
    "\n",
    "        print(\"\\n📊 Pipeline Süreleri ve İstatistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(end_chunk - start_chunk, 2),\n",
    "            \"embedding_and_upload_time_s\": round(end_upload - start_upload, 2),\n",
    "            \"total_pipeline_time_s\": round(end_total - start_total, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    # ------------------------ Interactive Search ------------------------\n",
    "    def interactive_search(self):\n",
    "        while True:\n",
    "            query = input(\"🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "\n",
    "            limit_input = input(\"Kaç sonuç? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "            except ValueError:\n",
    "                print(\"❌ Geçersiz sayı, varsayılan 5 kullanılıyor\")\n",
    "                limit = 5\n",
    "\n",
    "            results = self.processor.search_semantic(query, limit=limit)\n",
    "            for i, r in enumerate(results, 1):\n",
    "                payload = r['payload']\n",
    "                text_preview = payload.get('text', '')[:300] + \"...\"\n",
    "                print(f\"\\n{i}. Score: {r['score']:.3f}, Esas No: {payload.get('esas_no')}, Metin: {text_preview}\")\n",
    "\n",
    "# ------------------------ Main ------------------------\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_embeddinggemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n1. Full pipeline çalıştır\\n2. Arama yap\\n3. Koleksiyon bilgisi\\n4. Çıkış\")\n",
    "        choice = input(\"Seçim: \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
