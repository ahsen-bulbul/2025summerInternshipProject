{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing char...\n",
      "Chunking austen-emma.txt with chunk size 10...\n",
      "Chunking austen-persuasion.txt with chunk size 10...\n",
      "Chunking austen-sense.txt with chunk size 10...\n",
      "Chunking bible-kjv.txt with chunk size 10...\n",
      "Chunking blake-poems.txt with chunk size 10...\n",
      "Chunking bryant-stories.txt with chunk size 10...\n",
      "Chunking burgess-busterbrown.txt with chunk size 10...\n",
      "Chunking carroll-alice.txt with chunk size 10...\n",
      "Chunking chesterton-ball.txt with chunk size 10...\n",
      "Chunking chesterton-brown.txt with chunk size 10...\n",
      "Chunking chesterton-thursday.txt with chunk size 10...\n",
      "Chunking edgeworth-parents.txt with chunk size 10...\n",
      "Chunking melville-moby_dick.txt with chunk size 10...\n",
      "Chunking milton-paradise.txt with chunk size 10...\n",
      "Chunking shakespeare-caesar.txt with chunk size 10...\n",
      "Chunking shakespeare-hamlet.txt with chunk size 10...\n",
      "Chunking shakespeare-macbeth.txt with chunk size 10...\n",
      "Chunking whitman-leaves.txt with chunk size 10...\n",
      "Chunking austen-emma.txt with chunk size 512...\n",
      "Chunking austen-persuasion.txt with chunk size 512...\n",
      "Chunking austen-sense.txt with chunk size 512...\n",
      "Chunking bible-kjv.txt with chunk size 512...\n",
      "Chunking blake-poems.txt with chunk size 512...\n",
      "Chunking bryant-stories.txt with chunk size 512...\n",
      "Chunking burgess-busterbrown.txt with chunk size 512...\n",
      "Chunking carroll-alice.txt with chunk size 512...\n",
      "Chunking chesterton-ball.txt with chunk size 512...\n",
      "Chunking chesterton-brown.txt with chunk size 512...\n",
      "Chunking chesterton-thursday.txt with chunk size 512...\n",
      "Chunking edgeworth-parents.txt with chunk size 512...\n",
      "Chunking melville-moby_dick.txt with chunk size 512...\n",
      "Chunking milton-paradise.txt with chunk size 512...\n",
      "Chunking shakespeare-caesar.txt with chunk size 512...\n",
      "Chunking shakespeare-hamlet.txt with chunk size 512...\n",
      "Chunking shakespeare-macbeth.txt with chunk size 512...\n",
      "Chunking whitman-leaves.txt with chunk size 512...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 604.32it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 14952.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test semchunk.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "import semchunk\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from helpers import GUTENBERG, initialize_test_token_counters\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TEST_TOKEN_COUNTERS = (\n",
    "    # 'emubert_transformers',\n",
    "    # 'gpt4_tiktoken',\n",
    "    # 'word',\n",
    "    'char',\n",
    ")\n",
    "TEST_CHUNK_SIZES = (\n",
    "    10,\n",
    "    512,\n",
    ")\n",
    "TEST_OFFSETS = True\n",
    "\n",
    "DETERMINISTIC_TEST_INPUT = 'ThisIs\\tATest.'\n",
    "DETERMINISTIC_TEST_CHUNK_SIZE = 4\n",
    "DETERMINISTIC_TEST_OUTPUT_CHUNKS = {\n",
    "    'gpt4_tiktoken': ['ThisIs', 'ATest.'],\n",
    "    'emubert_transformers': ['ThisIs', 'ATest.'],\n",
    "    'word': ['ThisIs\\tATest.'],\n",
    "    'char': ['This', 'Is', 'ATes', 't.'],\n",
    "}\n",
    "DETERMINISTIC_TEST_OUTPUT_OFFSETS = {\n",
    "    'gpt4_tiktoken': [(0, 6), (7, 13)],\n",
    "    'emubert_transformers': [(0, 6), (7, 13)],\n",
    "    'word': [(0, 13)],\n",
    "    'char': [(0, 4), (4, 6), (7, 11), (11, 13)],\n",
    "}\n",
    "\n",
    "def test_semchunk() -> None:\n",
    "    \"\"\"Test semchunk.\"\"\"\n",
    "\n",
    "    # Initalize test token counters.\n",
    "    token_counters = initialize_test_token_counters()\n",
    "    token_counters = {name: token_counters[name] for name in TEST_TOKEN_COUNTERS}\n",
    "    \n",
    "    # Test chunking with the token counters.\n",
    "    for name, token_counter in token_counters.items():\n",
    "        print(f'Testing {name}...')\n",
    "        \n",
    "        # Test chunking with a variety of chunk sizes.\n",
    "        for chunk_size in TEST_CHUNK_SIZES:\n",
    "            # Add the number of special tokens added by the tokenizer to the chunk size + 1 if the tokenizer adds special tokens.\n",
    "            if token_counter(''):\n",
    "                chunk_size += token_counter('') + 1\n",
    "            \n",
    "            # Test chunking with a variety of texts.\n",
    "            for fileid in GUTENBERG.fileids():\n",
    "                sample = GUTENBERG.raw(fileid)\n",
    "                print(f'Chunking {fileid} with chunk size {chunk_size}...')\n",
    "                \n",
    "                chunker = semchunk.chunkerify(token_counter, chunk_size)\n",
    "                chunks = chunker(sample)\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    assert token_counter(chunk) <= chunk_size\n",
    "                    assert chunk and not chunk.isspace()\n",
    "                \n",
    "                if TEST_OFFSETS:\n",
    "                    chunks, offsets = chunker(sample, offsets = True)\n",
    "                    \n",
    "                    for chunk, (start, end) in zip(chunks, offsets):\n",
    "                        assert token_counter(chunk) <= chunk_size\n",
    "                        assert chunk == sample[start:end]\n",
    "                        assert chunk and not chunk.isspace()\n",
    "                \n",
    "                # Verify that recombining lowercased chunks stripped of whitespace yields the original text.\n",
    "                lowercased_no_whitespace = ''.join(sample.lower().split())\n",
    "\n",
    "                if TEST_OFFSETS:\n",
    "                    chunks, offsets = chunker(lowercased_no_whitespace, offsets = True)\n",
    "                    assert ''.join(chunks) == lowercased_no_whitespace\n",
    "                    assert ''.join(lowercased_no_whitespace[start:end] for start, end in offsets) == lowercased_no_whitespace\n",
    "                    \n",
    "                chunks = chunker(lowercased_no_whitespace)\n",
    "                assert ''.join(chunks) == lowercased_no_whitespace\n",
    "        \n",
    "        # Test overlapping.\n",
    "        chunker = semchunk.chunkerify(token_counter, DETERMINISTIC_TEST_CHUNK_SIZE)\n",
    "        low_overlap_chunks = chunker(DETERMINISTIC_TEST_INPUT, overlap = 0.1)\n",
    "        high_overlap_chunks = chunker(DETERMINISTIC_TEST_INPUT, overlap = math.ceil(DETERMINISTIC_TEST_CHUNK_SIZE * 0.9))\n",
    "        \n",
    "        if name == 'word':\n",
    "            assert len(high_overlap_chunks) == len(low_overlap_chunks)\n",
    "        \n",
    "        else:\n",
    "            assert len(high_overlap_chunks) > len(low_overlap_chunks)\n",
    "        \n",
    "        if TEST_OFFSETS:\n",
    "            low_overlap_chunks, low_overlap_offsets = chunker(DETERMINISTIC_TEST_INPUT, overlap = 0.1, offsets = True)\n",
    "            high_overlap_chunks, high_overlap_offsets = chunker(DETERMINISTIC_TEST_INPUT, overlap = math.ceil(DETERMINISTIC_TEST_CHUNK_SIZE * 0.9), offsets = True)\n",
    "            \n",
    "            if name == 'word':\n",
    "                assert len(high_overlap_chunks) == len(low_overlap_chunks)\n",
    "                assert len(high_overlap_offsets) == len(low_overlap_offsets)\n",
    "\n",
    "            else:\n",
    "                assert len(high_overlap_chunks) > len(low_overlap_chunks)\n",
    "                assert len(high_overlap_offsets) > len(low_overlap_offsets)\n",
    "    \n",
    "            assert high_overlap_chunks == [DETERMINISTIC_TEST_INPUT[start:end] for start, end in high_overlap_offsets]\n",
    "        \n",
    "        # Verify deterministic behavior.\n",
    "        chunker = semchunk.chunkerify(token_counter, DETERMINISTIC_TEST_CHUNK_SIZE)\n",
    "        chunks = chunker(DETERMINISTIC_TEST_INPUT)\n",
    "        assert chunks == DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]\n",
    "        \n",
    "        if TEST_OFFSETS:\n",
    "            chunks, offsets = chunker(DETERMINISTIC_TEST_INPUT, offsets = True)\n",
    "            assert chunks == DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]\n",
    "            assert offsets == DETERMINISTIC_TEST_OUTPUT_OFFSETS[name]\n",
    "            \n",
    "        # Test using semchunk directly with memoization enabled.\n",
    "        chunks = semchunk.chunk(DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_CHUNK_SIZE, token_counter, memoize = True)\n",
    "        assert chunks == DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]\n",
    "        \n",
    "        if TEST_OFFSETS:\n",
    "            chunks, offsets = semchunk.chunk(DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_CHUNK_SIZE, token_counter, offsets = True, memoize = True)\n",
    "            assert chunks == DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]\n",
    "            assert offsets == DETERMINISTIC_TEST_OUTPUT_OFFSETS[name]\n",
    "        \n",
    "        # Test chunking multiple texts.\n",
    "        chunker = semchunk.chunkerify(token_counter, DETERMINISTIC_TEST_CHUNK_SIZE)\n",
    "        chunks = chunker([DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_INPUT])\n",
    "        assert chunks == [DETERMINISTIC_TEST_OUTPUT_CHUNKS[name], DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]]\n",
    "        \n",
    "        if TEST_OFFSETS:\n",
    "            chunks, offsets = chunker([DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_INPUT], offsets = True)\n",
    "            assert chunks == [DETERMINISTIC_TEST_OUTPUT_CHUNKS[name], DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]]\n",
    "        \n",
    "        # Test chunking multiple texts with multiple processes.\n",
    "        chunker = semchunk.chunkerify(token_counter, DETERMINISTIC_TEST_CHUNK_SIZE)\n",
    "        chunks = chunker([DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_INPUT], processes = 2)\n",
    "        assert chunks == [DETERMINISTIC_TEST_OUTPUT_CHUNKS[name], DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]]\n",
    "        \n",
    "        if TEST_OFFSETS:\n",
    "            chunks, offsets = chunker([DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_INPUT], offsets = True, processes = 2)\n",
    "            assert chunks == [DETERMINISTIC_TEST_OUTPUT_CHUNKS[name], DETERMINISTIC_TEST_OUTPUT_CHUNKS[name]]\n",
    "            assert offsets == [DETERMINISTIC_TEST_OUTPUT_OFFSETS[name], DETERMINISTIC_TEST_OUTPUT_OFFSETS[name]]\n",
    "    \n",
    "    # Test causing a `ValueError` by passing a token counter without a chunk size.\n",
    "    try:\n",
    "        chunker = semchunk.chunkerify(list(token_counters.values())[0], None)\n",
    "        error_raised = False\n",
    "    \n",
    "    except ValueError:\n",
    "        error_raised = True\n",
    "    \n",
    "    assert error_raised\n",
    "    \n",
    "    # Test using `tiktoken` tokenizers, encodings and a `transformers` tokenizer by name with `chunkerify()`.\n",
    "    for name in ['cl100k_base', 'gpt-4', 'isaacus/kanon-tokenizer']:\n",
    "        chunker = semchunk.chunkerify(name, 1)\n",
    "        chunker(DETERMINISTIC_TEST_INPUT)\n",
    "        if TEST_OFFSETS: chunker(DETERMINISTIC_TEST_INPUT, offsets = True)\n",
    "\n",
    "    # Test causing a `ValueError` by passing a tokenizer by name that should not exist.\n",
    "    try:\n",
    "        chunker = semchunk.chunkerify('\\n\\f\\rع\\n\\f\\r', 1)\n",
    "        error_raised = False\n",
    "    \n",
    "    except ValueError:\n",
    "        error_raised = True\n",
    "    \n",
    "    assert error_raised\n",
    "    \n",
    "    # Test using a `transformers` tokenizer directly.\n",
    "    tokenizer = AutoTokenizer.from_pretrained('isaacus/kanon-tokenizer')\n",
    "    chunker = semchunk.chunkerify(tokenizer, 1)\n",
    "    \n",
    "    # Test using a `tiktoken` tokenizer directly.\n",
    "    tokenizer = tiktoken.encoding_for_model('gpt-4')\n",
    "    chunker = semchunk.chunkerify(tokenizer, 1)\n",
    "    \n",
    "    # Try enabling a progress bar.\n",
    "    chunker([DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_INPUT], progress = True)\n",
    "    chunker([DETERMINISTIC_TEST_INPUT, DETERMINISTIC_TEST_INPUT], offsets = True, progress = True)\n",
    "    \n",
    "    # Test chunking nothing to ensure no errors are raised.\n",
    "    semchunk.chunk('', 512, lambda *args: 0)\n",
    "    \n",
    "    # Test chunking whitespace to ensure no errors are raised.\n",
    "    semchunk.chunk('\\n\\n', 512, lambda *args: 0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_semchunk()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
