{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cohere modeli hfden upload etmeyi deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "hf_token = os.getenv(\"HF_API_KEY\")  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\", \n",
    "    token=hf_token\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\",\n",
    "    token=hf_token,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "# √ñrnek metinler\n",
    "texts = [\"Bu bir test c√ºmlesidir.\", \"Mahkeme kararƒ±nƒ± verdi.\"]\n",
    "\n",
    "# Tokenize et\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Modeli kullanarak embedding al\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Burada last_hidden_state'den mean pooling yapƒ±yoruz\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(\"Embedding boyutu:\", embeddings.shape)\n",
    "print(\"ƒ∞lk embedding √∂rneƒüi:\", embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "# Hugging Face API key ortam deƒüi≈ükeninden √ßek\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli token ile y√ºkle\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token  # API key burada\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlik hesapla\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasƒ±nƒ± y√ºkle\n",
    "load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\")\n",
    "\n",
    "# Hugging Face API key\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli y√ºkle (gated model ise token ver)\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# Embeddingleri al\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlikleri cosine similarity ile hesapla\n",
    "similarities = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Benzerlik matrisi:\\n\", similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "vec = model.encode([\"test\"])\n",
    "print(vec.shape)  # (1, 784) olmalƒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk ederken cpu mu gpu mu kullanƒ±lƒ±yor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA kullanƒ±labilir mi? \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU adƒ±:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU kullanƒ±lƒ±yor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarƒ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 768  # embeddinggemma-300m embedding boyutu\n",
    "\n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 256\n",
    "\n",
    "    # Cihaz ayarƒ±\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli (GPU/CPU se√ßimi burada yapƒ±lƒ±yor)\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        print(f\"‚úÖ Model ≈üu cihazda √ßalƒ±≈üƒ±yor: {config.DEVICE}\")\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"‚úÖ SemChunk hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ Google EmbeddingGemma modeli hazƒ±r\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bir batch in embeddingleri olu≈ütuktan sonra reduction yapƒ±yo yani par√ßa par√ßa yapmƒ±≈ü oluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            start_embed = time.time()\n",
    "\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "\n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding s√ºresi: {end_embed - start_embed:.2f} saniye\")\n",
    "\n",
    "            pca=PCA(n_components=512)\n",
    "            batch_embeddings_reduced = pca.fit_transform(batch_embeddings) #default 768 olan embedding boyutunu 512ye d√º≈ü√ºr√ºyoruz -> PCA\n",
    "            all_embeddings.extend(batch_embeddings_reduced.tolist())\n",
    "\n",
    "            print(f\"  üîπ Embedding olu≈üturuldu: {i + len(batch_texts)}/{total}\")\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Modeli y√ºkle\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# K√º√ß√ºk dataset\n",
    "train_examples = [\n",
    "    InputExample(texts=[\"araba\", \"otomobil\"], label=1.0),\n",
    "    InputExample(texts=[\"kedi\", \"k√∂pek\"], label=1.0),\n",
    "    InputExample(texts=[\"masa\", \"sandalye\"], label=1.0),\n",
    "    InputExample(texts=[\"araba\", \"u√ßak\"], label=0.0),\n",
    "    InputExample(texts=[\"kedi\", \"araba\"], label=0.0),\n",
    "    InputExample(texts=[\"masa\", \"araba\"], label=0.0),\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "\n",
    "# Loss fonksiyonu\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Eƒüitim\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10\n",
    ")\n",
    "\n",
    "# Test\n",
    "emb1 = model.encode(\"araba\")\n",
    "emb2 = model.encode(\"otomobil\")\n",
    "emb3 = model.encode(\"kedi\")\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "print(\"Benzerlik (araba - otomobil):\", cos_sim(emb1, emb2).item())\n",
    "print(\"Benzerlik (araba - kedi):\", cos_sim(emb1, emb3).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge 512 dimension with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# √ñrnek 1024 boyutlu embedding\n",
    "embedding_1024 = torch.randn(1, 1024)  # batch size 1\n",
    "\n",
    "# Linear layer ile 1024 -> 512 boyuta d√º≈ü√ºrme\n",
    "reduce_dim = nn.Linear(1024, 512)\n",
    "\n",
    "# Boyut indirgeme i≈ülemi\n",
    "embedding_512 = reduce_dim(embedding_1024)\n",
    "\n",
    "print(\"Orijinal boyut:\", embedding_1024.shape)\n",
    "print(\"Yeni boyut:\", embedding_512.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Normalize edilmi≈ü, reducer uyumlu, query_points kullanƒ±yor)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    # t: (N, D)\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Embed reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim: int = 1024, output_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "# -------------------------\n",
    "# Processor\n",
    "# -------------------------\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model & reducer\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(input_dim=1024, output_dim=self.config.EMBEDDING_DIM).to(config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense dim\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0]\n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ± - Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {'colbert_vecs' in emb_res}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (dim={self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings: List[List[float]] = []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ BGE-M3 ile {total} metin i≈üleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # 1) embed (BGE-M3) -> dense (1024)\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "                if isinstance(emb_res, dict) and 'dense_vecs' in emb_res:\n",
    "                    dense = emb_res['dense_vecs']\n",
    "                else:\n",
    "                    dense = emb_res\n",
    "\n",
    "                # 2) to tensor, device\n",
    "                if not isinstance(dense, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense.to(self.config.DEVICE)\n",
    "\n",
    "                # 3) reducer -> 512\n",
    "                with torch.no_grad():\n",
    "                    reduced = self.reducer(dense_t)\n",
    "\n",
    "                # 4) normalize L2 (important for cosine)\n",
    "                reduced = l2_normalize_tensor(reduced)\n",
    "\n",
    "                # 5) append as python lists (cpu)\n",
    "                all_embeddings.extend([v.cpu().tolist() for v in reduced])\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå BGE-M3 Embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # fallback zero vectors\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "\n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for chunk, emb in zip(chunks, embeddings):\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    # ---------- SEARCH (query_points + reducer + normalize) ----------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "            print(f\"üîç Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "\n",
    "            # use query_points (recommended)\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            # prepare reduced + normalized query vector same as above\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline + main\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        # recreate collection to ensure clean 512-dim index\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana men√º\")\n",
    "            ch = input(\"Se√ßiminiz (1-3): \").strip()\n",
    "            if ch == \"3\":\n",
    "                break\n",
    "            if ch not in {\"1\", \"2\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch == \"1\":\n",
    "                # try with low threshold first for debugging\n",
    "                results = self.processor.search_semantic(q, limit=limit, score_threshold=None)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (√∂rn: '6.HukukDairesi', bo≈ü = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit, score_threshold=None)\n",
    "\n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_normal_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding bulunamadƒ± ‚Äî pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# Yargƒ±tay Kararlarƒ± i√ßin Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # L2 normalization ekledik\n",
    "        x = self.linear(x)\n",
    "        return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "\n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarƒ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512  #collection boyutu\n",
    "\n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE: int = 256\n",
    "\n",
    "    \n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargƒ±tay kararlarƒ± i√ßin semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        # Reducer modeli - daha iyi initialization ile\n",
    "        self.reducer = EmbedReducer(768, 512)\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.reducer.to(self.device)\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"‚úÖ SemChunk hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ Google EmbeddingGemma modeli hazƒ±r\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n",
    "        print(f\"‚úÖ Device: {self.device}\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        self.reducer.eval()  # Eval moduna al\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_embed = time.time()\n",
    "            \n",
    "            # Embedding olu≈ütur\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = self.model.encode(\n",
    "                    batch_texts, \n",
    "                    show_progress_bar=True, \n",
    "                    convert_to_tensor=True, \n",
    "                    normalize_embeddings=True  # Bu √∂nemli!\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Boyut d√º≈ü√ºr ve normalize et\n",
    "                reduced_vector = self.reducer(batch_embeddings)\n",
    "                \n",
    "                print(\"*\" * 40)\n",
    "                print(f\"Original embedding shape: {batch_embeddings.shape}\")\n",
    "                print(f\"Reduced vector shape: {reduced_vector.shape}\")\n",
    "                print(f\"Vector norm check (should be ~1.0): {torch.norm(reduced_vector[0]).item():.4f}\")\n",
    "                print(\"*\" * 40)\n",
    "                \n",
    "                # CPU'ya ta≈üƒ± ve listeye √ßevir\n",
    "                all_embeddings.extend(reduced_vector.cpu().tolist())\n",
    "            \n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding s√ºresi: {end_embed - start_embed:.2f} saniye\")\n",
    "            print(f\"  üîπ Embedding olu≈üturuldu: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"üìÑ Toplam {total_rows} satƒ±r i≈ülenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"üîÆ Embedding'ler olu≈üturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"üöÄ {total_points} chunk Qdrant'a y√ºkleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                start_upload = time.time()\n",
    "\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "\n",
    "                end_upload = time.time()\n",
    "                print(f\"batch Qdrant y√ºkleme s√ºresi: {end_upload - start_upload:.2f} saniye\")\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(f\"üéâ {total_points} chunk Qdrant'a y√ºklendi!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"D√ºzeltilmi≈ü search metodu - daha d√º≈ü√ºk threshold ile\"\"\"\n",
    "        \n",
    "        self.reducer.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Query embedding √ßƒ±kar (normalize edilmi≈ü)\n",
    "            query_embedding = self.model.encode(\n",
    "                [query], \n",
    "                convert_to_tensor=True, \n",
    "                normalize_embeddings=True  # Bu √∂nemli!\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Boyutu 512'ye d√º≈ü√ºr ve normalize et\n",
    "            reduced_query_embedding = self.reducer(query_embedding)\n",
    "            \n",
    "            print(f\"üîç Query vector boyutu: {reduced_query_embedding.shape}\")\n",
    "            print(f\"üîç Query vector norm: {torch.norm(reduced_query_embedding[0]).item():.4f}\")\n",
    "            \n",
    "            # CPU'ya ta≈üƒ± ve numpy array'e √ßevir\n",
    "            query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "        \n",
    "        print(f\"üîç Final query vector boyutu: {len(query_vector)} (hedef: {self.config.DIMENSION})\")\n",
    "        \n",
    "        try:\n",
    "            # Qdrant aramasƒ± - daha d√º≈ü√ºk threshold ile\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"üîç Ham arama sonucu sayƒ±sƒ±: {len(search_results)}\")\n",
    "            \n",
    "            # Eƒüer hi√ß sonu√ß yoksa, threshold'suz deneyelim\n",
    "            if not search_results:\n",
    "                print(\"‚ö†Ô∏è Threshold ile sonu√ß yok, threshold'suz deneniyor...\")\n",
    "                search_results = self.qdrant_client.search(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    query_vector=query_vector,\n",
    "                    limit=limit\n",
    "                )\n",
    "                print(f\"üîç Threshold'suz sonu√ß sayƒ±sƒ±: {len(search_results)}\")\n",
    "                \n",
    "                if search_results:\n",
    "                    print(f\"üîç En y√ºksek score: {search_results[0].score:.4f}\")\n",
    "                    print(f\"üîç En d√º≈ü√ºk score: {search_results[-1].score:.4f}\")\n",
    "            \n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def debug_search_issue(self, query: str):\n",
    "        \"\"\"Arama problemini debug et\"\"\"\n",
    "        print(f\"üîß Debug ba≈ülƒ±yor - Query: '{query}'\")\n",
    "        \n",
    "        # 1. Koleksiyon durumunu kontrol et\n",
    "        info = self.get_collection_info()\n",
    "        print(f\"üìä Koleksiyon durumu: {info}\")\n",
    "        \n",
    "        if info.get('points_count', 0) == 0:\n",
    "            print(\"‚ùå Koleksiyonda hi√ß point yok!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Query embedding olu≈ütur\n",
    "        self.reducer.eval()\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model.encode([query], convert_to_tensor=True, normalize_embeddings=True).to(self.device)\n",
    "            reduced_query = self.reducer(query_embedding)\n",
    "            query_vector = reduced_query[0].cpu().numpy().tolist()\n",
    "            \n",
    "        print(f\"üîç Query vector shape: {len(query_vector)}\")\n",
    "        print(f\"üîç Query vector norm: {np.linalg.norm(query_vector):.4f}\")\n",
    "        print(f\"üîç Query vector sample: {query_vector[:5]}\")\n",
    "        \n",
    "        # 3. √áok d√º≈ü√ºk threshold ile ara\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=5,\n",
    "                score_threshold=0.0  # √áok d√º≈ü√ºk threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"üîç Score threshold 0.0 ile bulunan sonu√ß: {len(results)}\")\n",
    "            if results:\n",
    "                for i, r in enumerate(results):\n",
    "                    print(f\"  {i+1}. Score: {r.score:.6f}\")\n",
    "                    text = r.payload.get('text', '')[:100]\n",
    "                    print(f\"     Text preview: {text}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Debug search hatasƒ±: {e}\")\n",
    "    \n",
    "    def fix_dimension_mismatch(self):\n",
    "        \"\"\"Dimension uyumsuzluƒüunu d√ºzelt\"\"\"\n",
    "        print(\"üîß Dimension uyumsuzluƒüu d√ºzeltiliyor...\")\n",
    "        response = input(\"‚ö†Ô∏è Bu i≈ülem koleksiyonu silecek. Devam? (y/N): \")\n",
    "        if response.lower() != 'y':\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.config.COLLECTION_NAME)\n",
    "            print(\"‚úÖ Koleksiyon silindi\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Silme hatasƒ±: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Pipeline sƒ±nƒ±fƒ±\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Yargƒ±tay Semantic Pipeline Ba≈ülƒ±yor\")\n",
    "\n",
    "        total_start = time.time()\n",
    "\n",
    "        # Koleksiyon olu≈üturma\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        # CSV i≈ülemleri ve chunk olu≈üturma\n",
    "        chunk_start = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        chunk_end = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"‚ùå ƒ∞≈ülenecek chunk bulunamadƒ±\")\n",
    "            return False\n",
    "\n",
    "        # Embedding olu≈üturma ve Qdrant y√ºkleme\n",
    "        upload_start = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        upload_end = time.time()\n",
    "\n",
    "        total_end = time.time()\n",
    "\n",
    "        # Toplam istatistikler\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Pipeline S√ºreleri ve ƒ∞statistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(chunk_end - chunk_start, 2),\n",
    "            \"embedding_and_upload_time_s\": round(upload_end - upload_start, 2),\n",
    "            \"total_pipeline_time_s\": round(total_end - total_start, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"üîç ƒ∞nteraktif Arama Ba≈ülatƒ±ldƒ±\")\n",
    "        \n",
    "        # √ñnce koleksiyon durumunu kontrol et\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(f\"üìä Koleksiyon Durumu: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nüîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                print(\"‚ùå Bo≈ü sorgu, tekrar deneyin\")\n",
    "                continue\n",
    "            \n",
    "            limit_input = input(\"Ka√ß sonu√ß? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "                limit = max(1, min(limit, 50))\n",
    "            except ValueError:\n",
    "                print(\"‚ùå Ge√ßersiz sayƒ±, varsayƒ±lan 5 kullanƒ±lƒ±yor\")\n",
    "                limit = 5\n",
    "\n",
    "            score_input = input(\"Minimum score? (default 0.3): \").strip()\n",
    "            try:\n",
    "                score_threshold = float(score_input) if score_input else 0.3\n",
    "                score_threshold = max(0.0, min(score_threshold, 1.0))\n",
    "            except ValueError:\n",
    "                print(\"‚ùå Ge√ßersiz score, varsayƒ±lan 0.3 kullanƒ±lƒ±yor\")\n",
    "                score_threshold = 0.3\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            results = self.processor.search_semantic(query, limit=limit, score_threshold=score_threshold)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå Hi√ß sonu√ß bulunamadƒ±.\")\n",
    "                print(\"üí° √ñneriler:\")\n",
    "                print(\"   - Score threshold'u d√º≈ü√ºr√ºn (0.03 veya 0.05)\")\n",
    "                print(\"   - Debug modunu √ßalƒ±≈ütƒ±rƒ±n (ana men√ºden 4)\")\n",
    "                print(\"   - Daha spesifik kelimeler kullanƒ±n\")\n",
    "            else:\n",
    "                print(f\"\\nüìã {len(results)} Sonu√ß Bulundu:\")\n",
    "                for i, r in enumerate(results, 1):\n",
    "                    payload = r['payload']\n",
    "                    text_preview = payload.get('text', '')[:200] + \"...\" if len(payload.get('text', '')) > 200 else payload.get('text', '')\n",
    "                    print(f\"\\n{i}. üìä Score: {r['score']:.4f}\")\n",
    "                    print(f\"   üìã Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                    print(f\"   üìã Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                    print(f\"   üìã Daire: {payload.get('daire', 'N/A')}\")\n",
    "                    print(f\"   üìã Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                    print(f\"   üìÑ Metin: {text_preview}\")\n",
    "                    print(f\"   {'‚îÄ'*50}\")\n",
    "            \n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_gemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"1. Full pipeline √ßalƒ±≈ütƒ±r\")\n",
    "        print(\"2. Arama yap\") \n",
    "        print(\"3. Koleksiyon bilgisi\")\n",
    "        print(\"4. üîß Debug arama sorunu\")\n",
    "        print(\"5. üóëÔ∏è Dimension uyumsuzluƒüunu d√ºzelt\")\n",
    "        print(\"6. √áƒ±kƒ±≈ü\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        choice = input(\"Se√ßim: \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "            \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "            \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "            \n",
    "        elif choice == \"4\":\n",
    "            query = input(\"Debug i√ßin test query (Enter: 'test'): \").strip() or \"test\"\n",
    "            pipeline.processor.debug_search_issue(query)\n",
    "            \n",
    "        elif choice == \"5\":\n",
    "            if pipeline.processor.fix_dimension_mismatch():\n",
    "                print(\"‚úÖ Koleksiyon sƒ±fƒ±rlandƒ±. ≈ûimdi '1' se√ßeneƒüi ile veriyi yeniden y√ºkleyin.\")\n",
    "            else:\n",
    "                print(\"‚ùå ƒ∞≈ülem iptal edildi\")\n",
    "                \n",
    "        elif choice == \"6\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bge no reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (1024->512 via slice, L2 normalize, no reducer)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:#normlarƒ± 1 yapƒ±lƒ±yo √ß√ºnk√º cosine'da y√∂n √∂nemli b√ºy√ºkl√ºk deƒüil\n",
    "    # t: (N, D) or (D,)\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model (no reducer)\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense dim\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ± - Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {'colbert_vecs' in emb_res}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (dim={self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings: List[List[float]] = []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ BGE-M3 ile {total} metin i≈üleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # 1) embed (BGE-M3) -> dense (1024)\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "                if isinstance(emb_res, dict) and 'dense_vecs' in emb_res:\n",
    "                    dense = emb_res['dense_vecs']\n",
    "                else:\n",
    "                    dense = emb_res\n",
    "\n",
    "                # 2) to tensor, device\n",
    "                if not isinstance(dense, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense.to(self.config.DEVICE)\n",
    "\n",
    "                # 3) slice first 512 dims and normalize\n",
    "                with torch.no_grad():\n",
    "                    sliced = dense_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    normed = l2_normalize_tensor(sliced)\n",
    "\n",
    "                # 4) append as python lists (cpu)\n",
    "                all_embeddings.extend([v.cpu().tolist() for v in normed])\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå BGE-M3 Embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # fallback zero vectors\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "\n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for chunk, emb in zip(chunks, embeddings):\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    # ---------- SEARCH (query_points + slice + normalize) ----------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            # to tensor, slice first 512 dims and normalize\n",
    "            q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                q_sliced = q_t[0, :self.config.EMBEDDING_DIM]\n",
    "                q_norm = l2_normalize_tensor(q_sliced)\n",
    "\n",
    "            query_vector = q_norm.cpu().tolist()\n",
    "            print(f\"üîç Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "\n",
    "            # use query_points (recommended)\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                q_sliced = q_t[0, :self.config.EMBEDDING_DIM]\n",
    "                q_norm = l2_normalize_tensor(q_sliced)\n",
    "\n",
    "            query_vector = q_norm.cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline + main\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        # recreate collection to ensure clean 512-dim index\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana men√º\")\n",
    "            ch = input(\"Se√ßiminiz (1-3): \").strip()\n",
    "            if ch == \"3\":\n",
    "                break\n",
    "            if ch not in {\"1\", \"2\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch == \"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit, score_threshold=None)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (√∂rn: '6.HukukDairesi', bo≈ü = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit, score_threshold=None)\n",
    "\n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_test_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding bulunamadƒ± ‚Äî pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model se√ßme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# Dynamic Model Selection + SemChunk + Qdrant Entegrasyon\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Model yapƒ±landƒ±rmalarƒ±\n",
    "MODEL_CONFIGS = {\n",
    "    \"bge-m3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"model_type\": \"bge\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 8192,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE-M3 - √áok dilli, dense+sparse embedding destekli\"\n",
    "    },\n",
    "    \"bge-large\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE Large - Sadece dense embedding\"\n",
    "    },\n",
    "    \"multilingual-e5\": {\n",
    "        \"model_name\": \"intfloat/multilingual-e5-large\",\n",
    "        \"model_type\": \"sentence_transformer\", \n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"E5 Multilingual Large - √áok dilli dense embedding\"\n",
    "    },\n",
    "    \"turkish-bert\": {\n",
    "        \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"Turkish BERT - T√ºrk√ße √∂zelle≈ütirilmi≈ü\"\n",
    "    },\n",
    "    \"all-mpnet\": {\n",
    "        \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 384,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"All-MiniLM - Genel ama√ßlƒ±, hƒ±zlƒ±\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    MODEL_TYPE: str = \"bge\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"dynamic_model_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "    SUPPORTS_SPARSE: bool = True\n",
    "    SUPPORTS_DENSE: bool = True\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Dinamik model y√∂netimi i√ßin sƒ±nƒ±f\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, config: Config):\n",
    "        self.model_key = model_key\n",
    "        self.config = config\n",
    "        self.model_config = MODEL_CONFIGS.get(model_key)\n",
    "        \n",
    "        if not self.model_config:\n",
    "            raise ValueError(f\"Desteklenmeyen model: {model_key}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.vectorizer = None  # Sparse embedding i√ßin\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Se√ßilen modeli y√ºkle\"\"\"\n",
    "        try:\n",
    "            model_name = self.model_config[\"model_name\"]\n",
    "            model_type = self.model_config[\"model_type\"]\n",
    "            \n",
    "            print(f\"üîÆ Model y√ºkleniyor: {model_name} (tip: {model_type})\")\n",
    "            \n",
    "            if model_type == \"bge\":\n",
    "                self.model = BGEM3FlagModel(\n",
    "                    model_name, \n",
    "                    use_fp16=self.config.USE_FP16, \n",
    "                    device=self.config.DEVICE\n",
    "                )\n",
    "            elif model_type == \"sentence_transformer\":\n",
    "                self.model = SentenceTransformer(model_name, device=self.config.DEVICE)\n",
    "                # Sparse embedding i√ßin TF-IDF hazƒ±rla\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "            else:\n",
    "                raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "                \n",
    "            print(f\"‚úÖ Model y√ºklendi: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model y√ºkleme hatasƒ±: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Metinleri encode et - model tipine g√∂re\"\"\"\n",
    "        dense_embeddings = []\n",
    "        sparse_embeddings = []\n",
    "        \n",
    "        try:\n",
    "            if self.model_config[\"model_type\"] == \"bge\" and hasattr(self.model, 'encode'):\n",
    "                # BGE-M3 i√ßin\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    result = self.model.encode(\n",
    "                        texts, \n",
    "                        return_dense=True, \n",
    "                        return_sparse=True\n",
    "                    )\n",
    "                    dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "                    sparse_raw = result.get(\"sparse_vecs\", [])\n",
    "                    sparse_embeddings = [\n",
    "                        {\"indices\": s.get(\"indices\", []), \"values\": s.get(\"values\", [])} \n",
    "                        for s in sparse_raw\n",
    "                    ] if sparse_raw else [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                else:\n",
    "                    dense_embeddings = self.model.encode(texts, return_dense=True)\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                    \n",
    "            else:\n",
    "                # Sentence Transformer i√ßin\n",
    "                dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "                \n",
    "                # Sparse embedding TF-IDF ile\n",
    "                if self.vectorizer:\n",
    "                    X_sparse = self.vectorizer.fit_transform(texts)\n",
    "                    sparse_embeddings = []\n",
    "                    for i in range(X_sparse.shape[0]):\n",
    "                        row = X_sparse[i].tocoo()\n",
    "                        sparse_embeddings.append({\n",
    "                            \"indices\": row.col.tolist(),\n",
    "                            \"values\": row.data.tolist()\n",
    "                        })\n",
    "                else:\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            \n",
    "            # Embedding boyutunu ayarla\n",
    "            target_dim = self.config.EMBEDDING_DIM\n",
    "            dense_clean = []\n",
    "            for vec in dense_embeddings:\n",
    "                if vec is None:\n",
    "                    dense_clean.append([0.0] * target_dim)\n",
    "                elif len(vec) < target_dim:\n",
    "                    dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "                else:\n",
    "                    dense_clean.append(vec[:target_dim])\n",
    "            \n",
    "            return dense_clean, sparse_embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Encoding hatasƒ±: {e}\")\n",
    "            # Fallback\n",
    "            return (\n",
    "                [[0.0] * self.config.EMBEDDING_DIM for _ in texts],\n",
    "                [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            )\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Model bilgilerini d√∂nd√ºr\"\"\"\n",
    "        info = self.model_config.copy()\n",
    "        info[\"loaded\"] = self.model is not None\n",
    "        info[\"current_embedding_dim\"] = self.config.EMBEDDING_DIM\n",
    "        return info\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.config = config\n",
    "        self.model_manager = ModelManager(model_key, config)\n",
    "        \n",
    "        # Model bilgilerini config'e aktar\n",
    "        model_config = self.model_manager.model_config\n",
    "        self.config.BGE_MODEL_NAME = model_config[\"model_name\"]\n",
    "        self.config.MODEL_TYPE = model_config[\"model_type\"]\n",
    "        self.config.SUPPORTS_SPARSE = model_config[\"supports_sparse\"]\n",
    "        self.config.SUPPORTS_DENSE = model_config[\"supports_dense\"]\n",
    "        \n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Model y√ºkle\n",
    "        if not self.model_manager.load_model():\n",
    "            raise RuntimeError(\"Model y√ºklenemedi!\")\n",
    "        \n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Model: {model_config['model_name']} | Cihaz: {device_name}\")\n",
    "\n",
    "    def test_model_connection(self):\n",
    "        \"\"\"Model baƒülantƒ±sƒ±nƒ± test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts(test_text)\n",
    "            \n",
    "            print(f\"‚úÖ Dense embedding boyutu: {len(dense_emb[0])}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {len(sparse_emb[0]['indices']) > 0}\")\n",
    "            return len(dense_emb[0])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense vector config\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM, \n",
    "                        distance=models.Distance.COSINE\n",
    "                    ),\n",
    "                }\n",
    "                \n",
    "                \n",
    "                sparse_config = {}\n",
    "                if self.config.SUPPORTS_SPARSE:\n",
    "                    sparse_config = {\n",
    "                        \"sparse_vec\": models.SparseVectorParams(\n",
    "                            index=models.SparseIndexParams(on_disk=False)\n",
    "                        )\n",
    "                    }\n",
    "                \n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config=sparse_config if sparse_config else None\n",
    "                )\n",
    "                \n",
    "                support_info = \"Dense+Sparse\" if sparse_config else \"Dense only\"\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} ({support_info})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding olu≈ütur\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (model: {self.config.BGE_MODEL_NAME})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "                'model_used': self.config.BGE_MODEL_NAME  # Hangi model kullanƒ±ldƒ±ƒüƒ±nƒ± kaydet\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings(texts)\n",
    "\n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vector_dict = {\"dense_vec\": d}\n",
    "            \n",
    "            # Sparse vector sadece destekleniyorsa ekle\n",
    "            if self.config.SUPPORTS_SPARSE and s[\"indices\"]:\n",
    "                vector_dict[\"sparse_vec\"] = SparseVector(\n",
    "                    indices=s[\"indices\"],\n",
    "                    values=s[\"values\"]\n",
    "                )\n",
    "            \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector_dict,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "        try:\n",
    "            dense_emb, _ = self.model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0]\n",
    "            \n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=NamedVector(name=\"dense_vec\", vector=query_vector),\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Semantic search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Hybrid search (sadece destekleniyorsa)\"\"\"\n",
    "        if not self.config.SUPPORTS_SPARSE:\n",
    "            print(\"‚ö†Ô∏è Bu model sparse embedding desteklemiyor, dense search yapƒ±lƒ±yor...\")\n",
    "            return self.search_semantic(query, limit, score_threshold)\n",
    "        \n",
    "        try:\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts([query])\n",
    "            \n",
    "            requests = [\n",
    "                SearchRequest(\n",
    "                    vector=NamedVector(name=\"dense_vec\", vector=dense_emb[0]),\n",
    "                    limit=limit,\n",
    "                    with_payload=True,\n",
    "                    score_threshold=score_threshold\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Sparse varsa ekle\n",
    "            if sparse_emb[0][\"indices\"]:\n",
    "                requests.append(\n",
    "                    SearchRequest(\n",
    "                        vector=NamedSparseVector(\n",
    "                            name=\"sparse_vec\",\n",
    "                            vector=SparseVector(\n",
    "                                indices=sparse_emb[0][\"indices\"],\n",
    "                                values=sparse_emb[0][\"values\"]\n",
    "                            )\n",
    "                        ),\n",
    "                        limit=limit,\n",
    "                        score_threshold=score_threshold\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            results = []\n",
    "            for request_result in qr:\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Hybrid)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Hybrid search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            model_info = self.model_manager.get_model_info()\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"model_info\": model_info,\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM,\n",
    "                \"supports_sparse\": self.config.SUPPORTS_SPARSE\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.processor = YargitaySemanticProcessor(config, model_key)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_model_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        model_info = self.processor.model_manager.get_model_info()\n",
    "        print(f\"üì± Aktif model: {model_info['model_name']}\")\n",
    "        print(f\"üîß √ñzellikler: Dense‚úÖ, Sparse{'‚úÖ' if model_info['supports_sparse'] else '‚ùå'}\")\n",
    "        \n",
    "        while True:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"üîç ARAMA SE√áENEKLERƒ∞\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(\"1) Dense arama (Semantic)\")\n",
    "            if model_info['supports_sparse']:\n",
    "                print(\"2) Hybrid arama (Dense + Sparse)\")\n",
    "                print(\"3) Kar≈üƒ±la≈ütƒ±rmalƒ± arama (Her iki y√∂ntem)\")\n",
    "            else:\n",
    "                print(\"2) ‚ùå Hybrid arama (Bu model desteklemiyor)\")\n",
    "                print(\"3) ‚ùå Kar≈üƒ±la≈ütƒ±rma (Sparse desteklenmiyor)\")\n",
    "            print(\"4) Ana men√º\")\n",
    "            \n",
    "            ch = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "            if ch==\"4\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\",\"3\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "                \n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                print(\"\\nüéØ Dense Semantic Search...\")\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(results, \"Dense\")\n",
    "                \n",
    "            elif ch==\"2\" and model_info['supports_sparse']:\n",
    "                print(\"\\nüîÄ Hybrid Search...\")\n",
    "                results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(results, \"Hybrid\")\n",
    "                \n",
    "            elif ch==\"3\" and model_info['supports_sparse']:\n",
    "                print(\"\\nüìä Kar≈üƒ±la≈ütƒ±rmalƒ± Arama...\")\n",
    "                print(\"üéØ Dense sonu√ßlar:\")\n",
    "                dense_results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(dense_results, \"Dense\", show_comparison=True)\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"üîÄ Hybrid sonu√ßlar:\")\n",
    "                hybrid_results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(hybrid_results, \"Hybrid\", show_comparison=True)\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Bu √∂zellik se√ßilen model tarafƒ±ndan desteklenmiyor.\")\n",
    "\n",
    "    def _display_results(self, results: List[Dict], search_type: str, show_comparison: bool = False):\n",
    "        \"\"\"Sonu√ßlarƒ± g√∂r√ºnt√ºle\"\"\"\n",
    "        if not results:\n",
    "            print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nüìã {len(results)} {search_type} sonu√ß:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            p = r.get(\"payload\") or {}\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            \n",
    "            # Skor rengine g√∂re emoji\n",
    "            if score > 0.8:\n",
    "                score_icon = \"üü¢\"\n",
    "            elif score > 0.6:\n",
    "                score_icon = \"üü°\"\n",
    "            else:\n",
    "                score_icon = \"üî¥\"\n",
    "                \n",
    "            print(f\"\\n{i}. {score_icon} Skor: {score:.4f}\")\n",
    "            print(f\"   üìÑ Model: {p.get('model_used','N/A')}\")\n",
    "            print(f\"   üèõÔ∏è Daire: {p.get('daire','N/A')} | üìÖ Tarih: {p.get('tarih','N/A')}\")\n",
    "            print(f\"   üìã Esas: {p.get('esas_no','N/A')} | üî¢ Karar: {p.get('karar_no','N/A')}\")\n",
    "            \n",
    "            text = p.get('text', '')\n",
    "            if len(text) > 200:\n",
    "                text_preview = text[:200] + \"...\"\n",
    "            else:\n",
    "                text_preview = text\n",
    "            print(f\"   üìù Metin: {text_preview}\")\n",
    "            \n",
    "            if show_comparison:\n",
    "                print(f\"   üè∑Ô∏è Tip: {search_type}\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "def select_model() -> str:\n",
    "    \"\"\"Kullanƒ±cƒ±dan model se√ßimi al\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ MODEL SE√áƒ∞Mƒ∞\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Modelleri kategorilere ayƒ±r\n",
    "    categories = {\n",
    "        \"üåç √áok Dilli Modeller\": [\"bge-m3\", \"multilingual-e5\"],\n",
    "        \"üáπüá∑ T√ºrk√ße √ñzel\": [\"turkish-bert\", \"distilbert-turkish\"],\n",
    "        \"‚ö° Hƒ±zlƒ± & Genel\": [\"bge-large\", \"all-mpnet\"]\n",
    "    }\n",
    "    \n",
    "    for category, models in categories.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for model_key in models:\n",
    "            config = MODEL_CONFIGS[model_key]\n",
    "            sparse_type = config.get('sparse_type', 'none')\n",
    "            sparse_icon = f\"‚úÖ({sparse_type.upper()})\" if config['supports_sparse'] else \"‚ùå\"\n",
    "            print(f\"  {model_key}: {config['description']}\")\n",
    "            print(f\"    ‚îî‚îÄ Boyut: {config['embedding_dim']}, Sparse: {sparse_icon}, Max Token: {config['max_seq_length']}\")\n",
    "    \n",
    "    print(f\"\\nüí° √ñneri:\")\n",
    "    print(\"  ‚Ä¢ T√ºrk√ße aƒüƒ±rlƒ±klƒ±: turkish-bert (TF-IDF sparse)\")\n",
    "    print(\"  ‚Ä¢ En iyi performans: bge-m3 (Native sparse)\")\n",
    "    print(\"  ‚Ä¢ Hƒ±zlƒ± T√ºrk√ße: distilbert-turkish (TF-IDF sparse)\")\n",
    "    print(\"  ‚Ä¢ √áok dilli: multilingual-e5 (TF-IDF sparse)\")\n",
    "    \n",
    "    print(f\"\\nüîç Sparse Embedding T√ºrleri:\")\n",
    "    print(\"  ‚Ä¢ NATIVE: Model'in kendi sparse sistemi (sadece BGE-M3)\")\n",
    "    print(\"  ‚Ä¢ TFIDF: TF-IDF tabanlƒ± sparse embedding (t√ºm diƒüer modeller)\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nModel se√ßin (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            selected_config = MODEL_CONFIGS[choice]\n",
    "            sparse_method = selected_config.get('sparse_type', 'none')\n",
    "            print(f\"‚úÖ Se√ßilen model: {selected_config['model_name']}\")\n",
    "            print(f\"üìä Sparse method: {sparse_method.upper()}\")\n",
    "            return choice\n",
    "        print(\"‚ùå Ge√ßersiz model! Mevcut:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"{key}: {config['description']}\")\n",
    "        print(f\"  ‚îî‚îÄ Boyut: {config['embedding_dim']}, Sparse: {config['supports_sparse']}\")\n",
    "    \n",
    "    print(\"\\nMevcut modeller:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nModel se√ßin (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            print(f\"‚úÖ Se√ßilen model: {MODEL_CONFIGS[choice]['model_name']}\")\n",
    "            return choice\n",
    "        print(\"‚ùå Ge√ßersiz model! Tekrar deneyin.\")\n",
    "\n",
    "def main():\n",
    "    print(\"üèõÔ∏è YARGITAY Dƒ∞NAMƒ∞K MODEL Sƒ∞STEMƒ∞\")\n",
    "    \n",
    "    # Model se√ßimi\n",
    "    selected_model = select_model()\n",
    "    selected_config = MODEL_CONFIGS[selected_model]\n",
    "    \n",
    "    # Config olu≈ütur\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=f\"{selected_model}_chunks\",  # Model adƒ±na g√∂re koleksiyon\n",
    "        EMBEDDING_DIM=min(512, selected_config[\"embedding_dim\"]),  # Boyutu ayarla\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pipeline = YargitayPipeline(config, selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pipeline olu≈üturma hatasƒ±: {e}\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üèõÔ∏è YARGITAY SEMANTƒ∞K Sƒ∞STEM - Model: {selected_config['model_name']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) Model deƒüi≈ütir\")\n",
    "        print(\"5) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-5): \").strip()\n",
    "        \n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            # Model deƒüi≈ütir ve sistemi yeniden ba≈ülat\n",
    "            selected_model = select_model()\n",
    "            selected_config = MODEL_CONFIGS[selected_model]\n",
    "            config.COLLECTION_NAME = f\"{selected_model}_chunks\"\n",
    "            config.EMBEDDING_DIM = min(512, selected_config[\"embedding_dim\"])\n",
    "            try:\n",
    "                pipeline = YargitayPipeline(config, selected_model)\n",
    "                print(\"‚úÖ Model deƒüi≈ütirildi!\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Model deƒüi≈ütirme hatasƒ±: {e}\")\n",
    "        elif choice==\"5\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "  üîπ dense_vec (Dense): boyut 512\n",
      "     √∂rnek -> [0.051721986, -0.03350699, -0.02643429, 0.05381775, -0.044412125]\n",
      "  üî∏ sparse_vec (Sparse): 101 non-zero eleman\n",
      "     √∂rnek -> indices[:5]: [209, 210, 218, 239, 303], values[:5]: [0.06741171, 0.06388491, 0.08513636, 0.121806055, 0.071728185]\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# Dynamic Model Selection + SemChunk + Qdrant Entegrasyon\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Model yapƒ±landƒ±rmalarƒ±\n",
    "MODEL_CONFIGS = {\n",
    "    \"bge-m3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"model_type\": \"bge\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 8192,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE-M3 - √áok dilli, dense+sparse embedding destekli\"\n",
    "    },\n",
    "    \"bge-large\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE Large - Sadece dense embedding\"\n",
    "    },\n",
    "    \"multilingual-e5\": {\n",
    "        \"model_name\": \"intfloat/multilingual-e5-large\",\n",
    "        \"model_type\": \"sentence_transformer\", \n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"E5 Multilingual Large - √áok dilli dense embedding\"\n",
    "    },\n",
    "    \"turkish-bert\": {\n",
    "        \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"Turkish BERT - T√ºrk√ße √∂zelle≈ütirilmi≈ü\"\n",
    "    },\n",
    "    \"all-mpnet\": {\n",
    "        \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 384,\n",
    "        \"supports_sparse\":True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"All-MiniLM - Genel ama√ßlƒ±, hƒ±zlƒ±\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    MODEL_TYPE: str = \"bge\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"dynamic_model_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "    SUPPORTS_SPARSE: bool = True\n",
    "    SUPPORTS_DENSE: bool = True\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Dinamik model y√∂netimi i√ßin sƒ±nƒ±f\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, config: Config):\n",
    "        self.model_key = model_key\n",
    "        self.config = config\n",
    "        self.model_config = MODEL_CONFIGS.get(model_key)\n",
    "        \n",
    "        if not self.model_config:\n",
    "            raise ValueError(f\"Desteklenmeyen model: {model_key}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.vectorizer = None  # Sparse embedding i√ßin\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Se√ßilen modeli y√ºkle\"\"\"\n",
    "        try:\n",
    "            model_name = self.model_config[\"model_name\"]\n",
    "            model_type = self.model_config[\"model_type\"]\n",
    "            \n",
    "            print(f\"üîÆ Model y√ºkleniyor: {model_name} (tip: {model_type})\")\n",
    "            \n",
    "            if model_type == \"bge\":\n",
    "                self.model = BGEM3FlagModel(\n",
    "                    model_name, \n",
    "                    use_fp16=self.config.USE_FP16, \n",
    "                    device=self.config.DEVICE\n",
    "                )\n",
    "            elif model_type == \"sentence_transformer\":\n",
    "                self.model = SentenceTransformer(model_name, device=self.config.DEVICE)\n",
    "                # Sparse embedding i√ßin TF-IDF hazƒ±rla\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "            else:\n",
    "                raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "                \n",
    "            print(f\"‚úÖ Model y√ºklendi: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model y√ºkleme hatasƒ±: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Metinleri encode et - model tipine g√∂re\"\"\"\n",
    "        dense_embeddings = []\n",
    "        sparse_embeddings = []\n",
    "        \n",
    "        try:\n",
    "            if self.model_config[\"model_type\"] == \"bge\" and hasattr(self.model, 'encode'):\n",
    "                # BGE-M3 i√ßin\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    result = self.model.encode(\n",
    "                        texts, \n",
    "                        return_dense=True, \n",
    "                        return_sparse=True\n",
    "                    )\n",
    "                    dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "                    sparse_raw = result.get(\"sparse_vecs\", [])\n",
    "                    sparse_embeddings = [\n",
    "                        {\"indices\": s.get(\"indices\", []), \"values\": s.get(\"values\", [])} \n",
    "                        for s in sparse_raw\n",
    "                    ] if sparse_raw else [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                else:\n",
    "                    dense_embeddings = self.model.encode(texts, return_dense=True)\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                    \n",
    "            else:\n",
    "                # Sentence Transformer i√ßin\n",
    "                dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "                \n",
    "                # Sparse embedding TF-IDF ile\n",
    "                if self.vectorizer:\n",
    "                    X_sparse = self.vectorizer.fit_transform(texts)\n",
    "                    sparse_embeddings = []\n",
    "                    for i in range(X_sparse.shape[0]):\n",
    "                        row = X_sparse[i].tocoo()\n",
    "                        sparse_embeddings.append({\n",
    "                            \"indices\": row.col.tolist(),\n",
    "                            \"values\": row.data.tolist()\n",
    "                        })\n",
    "                else:\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            \n",
    "            # Embedding boyutunu ayarla\n",
    "            target_dim = self.config.EMBEDDING_DIM\n",
    "            dense_clean = []\n",
    "            for vec in dense_embeddings:\n",
    "                if vec is None:\n",
    "                    dense_clean.append([0.0] * target_dim)\n",
    "                elif len(vec) < target_dim:\n",
    "                    dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "                else:\n",
    "                    dense_clean.append(vec[:target_dim])\n",
    "            \n",
    "            return dense_clean, sparse_embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Encoding hatasƒ±: {e}\")\n",
    "            # Fallback\n",
    "            return (\n",
    "                [[0.0] * self.config.EMBEDDING_DIM for _ in texts],\n",
    "                [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            )\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Model bilgilerini d√∂nd√ºr\"\"\"\n",
    "        info = self.model_config.copy()\n",
    "        info[\"loaded\"] = self.model is not None\n",
    "        info[\"current_embedding_dim\"] = self.config.EMBEDDING_DIM\n",
    "        return info\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.config = config\n",
    "        self.model_manager = ModelManager(model_key, config)\n",
    "        \n",
    "        # Model bilgilerini config'e aktar\n",
    "        model_config = self.model_manager.model_config\n",
    "        self.config.BGE_MODEL_NAME = model_config[\"model_name\"]\n",
    "        self.config.MODEL_TYPE = model_config[\"model_type\"]\n",
    "        self.config.SUPPORTS_SPARSE = model_config[\"supports_sparse\"]\n",
    "        self.config.SUPPORTS_DENSE = model_config[\"supports_dense\"]\n",
    "        \n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Model y√ºkle\n",
    "        if not self.model_manager.load_model():\n",
    "            raise RuntimeError(\"Model y√ºklenemedi!\")\n",
    "        \n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Model: {model_config['model_name']} | Cihaz: {device_name}\")\n",
    "\n",
    "    def test_model_connection(self):\n",
    "        \"\"\"Model baƒülantƒ±sƒ±nƒ± test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts(test_text)\n",
    "            \n",
    "            print(f\"‚úÖ Dense embedding boyutu: {len(dense_emb[0])}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {len(sparse_emb[0]['indices']) > 0}\")\n",
    "            return len(dense_emb[0])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense vector config\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM, \n",
    "                        distance=models.Distance.COSINE\n",
    "                    ),\n",
    "                }\n",
    "                \n",
    "                # Sparse config (eƒüer destekleniyorsa)\n",
    "                sparse_config = {}\n",
    "                if self.config.SUPPORTS_SPARSE:\n",
    "                    sparse_config = {\n",
    "                        \"sparse_vec\": models.SparseVectorParams(\n",
    "                            index=models.SparseIndexParams(on_disk=False)\n",
    "                        )\n",
    "                    }\n",
    "                \n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config=sparse_config if sparse_config else None\n",
    "                )\n",
    "                \n",
    "                support_info = \"Dense+Sparse\" if sparse_config else \"Dense only\"\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} ({support_info})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding olu≈ütur\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (model: {self.config.BGE_MODEL_NAME})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "                'model_used': self.config.BGE_MODEL_NAME  # Hangi model kullanƒ±ldƒ±ƒüƒ±nƒ± kaydet\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings(texts)\n",
    "\n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vector_dict = {\"dense_vec\": d}\n",
    "            \n",
    "            # Sparse vector sadece destekleniyorsa ekle\n",
    "            if self.config.SUPPORTS_SPARSE and s[\"indices\"]:\n",
    "                vector_dict[\"sparse_vec\"] = SparseVector(\n",
    "                    indices=s[\"indices\"],\n",
    "                    values=s[\"values\"]\n",
    "                )\n",
    "            \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector_dict,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "        try:\n",
    "            dense_emb, _ = self.model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0]\n",
    "            \n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=NamedVector(name=\"dense_vec\", vector=query_vector),\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Semantic search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Hybrid search (sadece destekleniyorsa)\"\"\"\n",
    "        if not self.config.SUPPORTS_SPARSE:\n",
    "            print(\"‚ö†Ô∏è Bu model sparse embedding desteklemiyor, dense search yapƒ±lƒ±yor...\")\n",
    "            return self.search_semantic(query, limit, score_threshold)\n",
    "        \n",
    "        try:\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts([query])\n",
    "            \n",
    "            requests = [\n",
    "                SearchRequest(\n",
    "                    vector=NamedVector(name=\"dense_vec\", vector=dense_emb[0]),\n",
    "                    limit=limit,\n",
    "                    with_payload=True,\n",
    "                    score_threshold=score_threshold\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Sparse varsa ekle\n",
    "            if sparse_emb[0][\"indices\"]:\n",
    "                requests.append(\n",
    "                    SearchRequest(\n",
    "                        vector=NamedSparseVector(\n",
    "                            name=\"sparse_vec\",\n",
    "                            vector=SparseVector(\n",
    "                                indices=sparse_emb[0][\"indices\"],\n",
    "                                values=sparse_emb[0][\"values\"]\n",
    "                            )\n",
    "                        ),\n",
    "                        limit=limit,\n",
    "                        score_threshold=score_threshold\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            results = []\n",
    "            for request_result in qr:\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Hybrid)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Hybrid search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            model_info = self.model_manager.get_model_info()\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"model_info\": model_info,\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM,\n",
    "                \"supports_sparse\": self.config.SUPPORTS_SPARSE\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.processor = YargitaySemanticProcessor(config, model_key)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_model_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        model_info = self.processor.model_manager.get_model_info()\n",
    "        print(f\"üì± Aktif model: {model_info['model_name']}\")\n",
    "        print(f\"üîß √ñzellikler: Dense‚úÖ, Sparse{'‚úÖ' if model_info['supports_sparse'] else '‚ùå'}\")\n",
    "        \n",
    "        while True:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"üîç ARAMA SE√áENEKLERƒ∞\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(\"1) Dense arama (Semantic)\")\n",
    "            if model_info['supports_sparse']:\n",
    "                print(\"2) Hybrid arama (Dense + Sparse)\")\n",
    "                print(\"3) Kar≈üƒ±la≈ütƒ±rmalƒ± arama (Her iki y√∂ntem)\")\n",
    "            else:\n",
    "                print(\"2) ‚ùå Hybrid arama (Bu model desteklemiyor)\")\n",
    "                print(\"3) ‚ùå Kar≈üƒ±la≈ütƒ±rma (Sparse desteklenmiyor)\")\n",
    "            print(\"4) Ana men√º\")\n",
    "            \n",
    "            ch = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "            if ch==\"4\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\",\"3\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "                \n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                print(\"\\nüéØ Dense Semantic Search...\")\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(results, \"Dense\")\n",
    "                \n",
    "            elif ch==\"2\" and model_info['supports_sparse']:\n",
    "                print(\"\\nüîÄ Hybrid Search...\")\n",
    "                results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(results, \"Hybrid\")\n",
    "                \n",
    "            elif ch==\"3\" and model_info['supports_sparse']:\n",
    "                print(\"\\nüìä Kar≈üƒ±la≈ütƒ±rmalƒ± Arama...\")\n",
    "                print(\"üéØ Dense sonu√ßlar:\")\n",
    "                dense_results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(dense_results, \"Dense\", show_comparison=True)\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"üîÄ Hybrid sonu√ßlar:\")\n",
    "                hybrid_results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(hybrid_results, \"Hybrid\", show_comparison=True)\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Bu √∂zellik se√ßilen model tarafƒ±ndan desteklenmiyor.\")\n",
    "\n",
    "    def _display_results(self, results: List[Dict], search_type: str, show_comparison: bool = False):\n",
    "        \"\"\"Sonu√ßlarƒ± g√∂r√ºnt√ºle\"\"\"\n",
    "        if not results:\n",
    "            print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nüìã {len(results)} {search_type} sonu√ß:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            p = r.get(\"payload\") or {}\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            \n",
    "            # Skor rengine g√∂re emoji\n",
    "            if score > 0.8:\n",
    "                score_icon = \"üü¢\"\n",
    "            elif score > 0.6:\n",
    "                score_icon = \"üü°\"\n",
    "            else:\n",
    "                score_icon = \"üî¥\"\n",
    "                \n",
    "            print(f\"\\n{i}. {score_icon} Skor: {score:.4f}\")\n",
    "            print(f\"   üìÑ Model: {p.get('model_used','N/A')}\")\n",
    "            print(f\"   üèõÔ∏è Daire: {p.get('daire','N/A')} | üìÖ Tarih: {p.get('tarih','N/A')}\")\n",
    "            print(f\"   üìã Esas: {p.get('esas_no','N/A')} | üî¢ Karar: {p.get('karar_no','N/A')}\")\n",
    "            \n",
    "            text = p.get('text', '')\n",
    "            if len(text) > 200:\n",
    "                text_preview = text[:200] + \"...\"\n",
    "            else:\n",
    "                text_preview = text\n",
    "            print(f\"   üìù Metin: {text_preview}\")\n",
    "            \n",
    "            if show_comparison:\n",
    "                print(f\"   üè∑Ô∏è Tip: {search_type}\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "\n",
    "\n",
    "def check_sparse_vectors(collection_name, point_id):\n",
    "    res = client.retrieve(\n",
    "        collection_name=collection_name,\n",
    "        ids=[point_id],   # UUID veya int doƒürudan verilebilir\n",
    "        with_vectors=True\n",
    "    )\n",
    "\n",
    "    if not res:\n",
    "        print(\"‚ùå Veri bulunamadƒ±\")\n",
    "        return\n",
    "\n",
    "    point = res[0]\n",
    "    vectors = point.vector\n",
    "\n",
    "    for k, v in vectors.items():\n",
    "        if hasattr(v, \"indices\") and hasattr(v, \"values\"):  \n",
    "            # SparseVector\n",
    "            print(f\"  üî∏ {k} (Sparse): {len(v.indices)} non-zero eleman\")\n",
    "            print(f\"     √∂rnek -> indices[:5]: {v.indices[:5]}, values[:5]: {v.values[:5]}\")\n",
    "        else:\n",
    "            # DenseVector\n",
    "            dense_data = v.vector if hasattr(v, \"vector\") else v\n",
    "            print(f\"  üîπ {k} (Dense): boyut {len(dense_data)}\")\n",
    "            print(f\"     √∂rnek -> {dense_data[:5]}\")\n",
    "\n",
    "\n",
    "def select_model() -> str:\n",
    "    \"\"\"Kullanƒ±cƒ±dan model se√ßimi al\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ MODEL SE√áƒ∞Mƒ∞\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"{key}: {config['description']}\")\n",
    "        print(f\"  ‚îî‚îÄ Boyut: {config['embedding_dim']}, Sparse: {config['supports_sparse']}\")\n",
    "    \n",
    "    print(\"\\nMevcut modeller:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nModel se√ßin (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            print(f\"‚úÖ Se√ßilen model: {MODEL_CONFIGS[choice]['model_name']}\")\n",
    "            return choice\n",
    "        print(\"‚ùå Ge√ßersiz model! Tekrar deneyin.\")\n",
    "\n",
    "# def main():\n",
    "#     check_sparse_vectors(\"all-mpnet_chunks\", \"01967bd6-5735-43b6-8e81-c0508c12f257\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"üèõÔ∏è YARGITAY Dƒ∞NAMƒ∞K MODEL Sƒ∞STEMƒ∞\")\n",
    "    \n",
    "    # Model se√ßimi\n",
    "    selected_model = select_model()\n",
    "    selected_config = MODEL_CONFIGS[selected_model]\n",
    "    \n",
    "    # Config olu≈ütur\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=f\"{selected_model}_chunks\",  # Model adƒ±na g√∂re koleksiyon\n",
    "        EMBEDDING_DIM=min(512, selected_config[\"embedding_dim\"]),  # Boyutu ayarla\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pipeline = YargitayPipeline(config, selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pipeline olu≈üturma hatasƒ±: {e}\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üèõÔ∏è YARGITAY SEMANTƒ∞K Sƒ∞STEM - Model: {selected_config['model_name']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) Model deƒüi≈ütir\")\n",
    "        print(\"5) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-5): \").strip()\n",
    "        \n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            # Model deƒüi≈ütir ve sistemi yeniden ba≈ülat\n",
    "            selected_model = select_model()\n",
    "            selected_config = MODEL_CONFIGS[selected_model]\n",
    "            config.COLLECTION_NAME = f\"{selected_model}_chunks\"\n",
    "            config.EMBEDDING_DIM = min(512, selected_config[\"embedding_dim\"])\n",
    "            try:\n",
    "                pipeline = YargitayPipeline(config, selected_model)\n",
    "                print(\"‚úÖ Model deƒüi≈ütirildi!\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Model deƒüi≈ütirme hatasƒ±: {e}\")\n",
    "        elif choice==\"5\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
