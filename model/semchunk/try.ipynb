{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cohere modeli hfden upload etmeyi deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "hf_token = os.getenv(\"HF_API_KEY\")  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\", \n",
    "    token=hf_token\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\",\n",
    "    token=hf_token,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "# √ñrnek metinler\n",
    "texts = [\"Bu bir test c√ºmlesidir.\", \"Mahkeme kararƒ±nƒ± verdi.\"]\n",
    "\n",
    "# Tokenize et\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Modeli kullanarak embedding al\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Burada last_hidden_state'den mean pooling yapƒ±yoruz\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(\"Embedding boyutu:\", embeddings.shape)\n",
    "print(\"ƒ∞lk embedding √∂rneƒüi:\", embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "# Hugging Face API key ortam deƒüi≈ükeninden √ßek\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli token ile y√ºkle\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token  # API key burada\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlik hesapla\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (4, 768)\n",
      "Benzerlik matrisi:\n",
      " [[1.         0.91585195 0.98131907 0.7847327 ]\n",
      " [0.91585195 1.0000001  0.8955096  0.77843153]\n",
      " [0.98131907 0.8955096  0.99999976 0.7499244 ]\n",
      " [0.7847327  0.77843153 0.7499244  1.0000002 ]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasƒ±nƒ± y√ºkle\n",
    "load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\")\n",
    "\n",
    "# Hugging Face API key\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli y√ºkle (gated model ise token ver)\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# Embeddingleri al\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlikleri cosine similarity ile hesapla\n",
    "similarities = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Benzerlik matrisi:\\n\", similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "vec = model.encode([\"test\"])\n",
    "print(vec.shape)  # (1, 784) olmalƒ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk ederken cpu mu gpu mu kullanƒ±lƒ±yor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA kullanƒ±labilir mi?  True\n",
      "GPU adƒ±: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA kullanƒ±labilir mi? \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU adƒ±:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU kullanƒ±lƒ±yor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarƒ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 768  # embeddinggemma-300m embedding boyutu\n",
    "\n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 256\n",
    "\n",
    "    # Cihaz ayarƒ±\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli (GPU/CPU se√ßimi burada yapƒ±lƒ±yor)\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        print(f\"‚úÖ Model ≈üu cihazda √ßalƒ±≈üƒ±yor: {config.DEVICE}\")\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"‚úÖ SemChunk hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ Google EmbeddingGemma modeli hazƒ±r\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bir batch in embeddingleri olu≈ütuktan sonra reduction yapƒ±yo yani par√ßa par√ßa yapmƒ±≈ü oluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            start_embed = time.time()\n",
    "\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "\n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding s√ºresi: {end_embed - start_embed:.2f} saniye\")\n",
    "\n",
    "            pca=PCA(n_components=512)\n",
    "            batch_embeddings_reduced = pca.fit_transform(batch_embeddings) #default 768 olan embedding boyutunu 512ye d√º≈ü√ºr√ºyoruz -> PCA\n",
    "            all_embeddings.extend(batch_embeddings_reduced.tolist())\n",
    "\n",
    "            print(f\"  üîπ Embedding olu≈üturuldu: {i + len(batch_texts)}/{total}\")\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benzerlik (araba - otomobil): 0.547791063785553\n",
      "Benzerlik (araba - kedi): -0.0018093292601406574\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Modeli y√ºkle\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# K√º√ß√ºk dataset\n",
    "train_examples = [\n",
    "    InputExample(texts=[\"araba\", \"otomobil\"], label=1.0),\n",
    "    InputExample(texts=[\"kedi\", \"k√∂pek\"], label=1.0),\n",
    "    InputExample(texts=[\"masa\", \"sandalye\"], label=1.0),\n",
    "    InputExample(texts=[\"araba\", \"u√ßak\"], label=0.0),\n",
    "    InputExample(texts=[\"kedi\", \"araba\"], label=0.0),\n",
    "    InputExample(texts=[\"masa\", \"araba\"], label=0.0),\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "\n",
    "# Loss fonksiyonu\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Eƒüitim\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10\n",
    ")\n",
    "\n",
    "# Test\n",
    "emb1 = model.encode(\"araba\")\n",
    "emb2 = model.encode(\"otomobil\")\n",
    "emb3 = model.encode(\"kedi\")\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "print(\"Benzerlik (araba - otomobil):\", cos_sim(emb1, emb2).item())\n",
    "print(\"Benzerlik (araba - kedi):\", cos_sim(emb1, emb3).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge 512 dimension with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal boyut: torch.Size([1, 1024])\n",
      "Yeni boyut: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# √ñrnek 1024 boyutlu embedding\n",
    "embedding_1024 = torch.randn(1, 1024)  # batch size 1\n",
    "\n",
    "# Linear layer ile 1024 -> 512 boyuta d√º≈ü√ºrme\n",
    "reduce_dim = nn.Linear(1024, 512)\n",
    "\n",
    "# Boyut indirgeme i≈ülemi\n",
    "embedding_512 = reduce_dim(embedding_1024)\n",
    "\n",
    "print(\"Orijinal boyut:\", embedding_1024.shape)\n",
    "print(\"Yeni boyut:\", embedding_512.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Normalize edilmi≈ü, reducer uyumlu, query_points kullanƒ±yor)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    # t: (N, D)\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Embed reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim: int = 1024, output_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "# -------------------------\n",
    "# Processor\n",
    "# -------------------------\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model & reducer\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(input_dim=1024, output_dim=self.config.EMBEDDING_DIM).to(config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense dim\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0]\n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ± - Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {'colbert_vecs' in emb_res}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (dim={self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings: List[List[float]] = []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ BGE-M3 ile {total} metin i≈üleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # 1) embed (BGE-M3) -> dense (1024)\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "                if isinstance(emb_res, dict) and 'dense_vecs' in emb_res:\n",
    "                    dense = emb_res['dense_vecs']\n",
    "                else:\n",
    "                    dense = emb_res\n",
    "\n",
    "                # 2) to tensor, device\n",
    "                if not isinstance(dense, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense.to(self.config.DEVICE)\n",
    "\n",
    "                # 3) reducer -> 512\n",
    "                with torch.no_grad():\n",
    "                    reduced = self.reducer(dense_t)\n",
    "\n",
    "                # 4) normalize L2 (important for cosine)\n",
    "                reduced = l2_normalize_tensor(reduced)\n",
    "\n",
    "                # 5) append as python lists (cpu)\n",
    "                all_embeddings.extend([v.cpu().tolist() for v in reduced])\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå BGE-M3 Embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # fallback zero vectors\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "\n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for chunk, emb in zip(chunks, embeddings):\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    # ---------- SEARCH (query_points + reducer + normalize) ----------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "            print(f\"üîç Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "\n",
    "            # use query_points (recommended)\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            # prepare reduced + normalized query vector same as above\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline + main\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        # recreate collection to ensure clean 512-dim index\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana men√º\")\n",
    "            ch = input(\"Se√ßiminiz (1-3): \").strip()\n",
    "            if ch == \"3\":\n",
    "                break\n",
    "            if ch not in {\"1\", \"2\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch == \"1\":\n",
    "                # try with low threshold first for debugging\n",
    "                results = self.processor.search_semantic(q, limit=limit, score_threshold=None)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (√∂rn: '6.HukukDairesi', bo≈ü = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit, score_threshold=None)\n",
    "\n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_normal_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding bulunamadƒ± ‚Äî pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
