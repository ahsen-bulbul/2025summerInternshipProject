{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cohere modeli hfden upload etmeyi deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "hf_token = os.getenv(\"HF_API_KEY\")  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\", \n",
    "    token=hf_token\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\",\n",
    "    token=hf_token,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "# Örnek metinler\n",
    "texts = [\"Bu bir test cümlesidir.\", \"Mahkeme kararını verdi.\"]\n",
    "\n",
    "# Tokenize et\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Modeli kullanarak embedding al\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Burada last_hidden_state'den mean pooling yapıyoruz\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(\"Embedding boyutu:\", embeddings.shape)\n",
    "print(\"İlk embedding örneği:\", embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "# Hugging Face API key ortam değişkeninden çek\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli token ile yükle\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token  # API key burada\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlik hesapla\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (4, 768)\n",
      "Benzerlik matrisi:\n",
      " [[1.         0.91585195 0.98131907 0.7847327 ]\n",
      " [0.91585195 1.0000001  0.8955096  0.77843153]\n",
      " [0.98131907 0.8955096  0.99999976 0.7499244 ]\n",
      " [0.7847327  0.77843153 0.7499244  1.0000002 ]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasını yükle\n",
    "load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\")\n",
    "\n",
    "# Hugging Face API key\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli yükle (gated model ise token ver)\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# Embeddingleri al\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlikleri cosine similarity ile hesapla\n",
    "similarities = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Benzerlik matrisi:\\n\", similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "vec = model.encode([\"test\"])\n",
    "print(vec.shape)  # (1, 784) olmalı"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk ederken cpu mu gpu mu kullanılıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA kullanılabilir mi?  True\n",
      "GPU adı: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA kullanılabilir mi? \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU adı:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU kullanılıyor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Konfigürasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarları\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarları\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarları\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 768  # embeddinggemma-300m embedding boyutu\n",
    "\n",
    "    # Dosya ayarları\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 256\n",
    "\n",
    "    # Cihaz ayarı\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli (GPU/CPU seçimi burada yapılıyor)\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        print(f\"✅ Model şu cihazda çalışıyor: {config.DEVICE}\")\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"✅ SemChunk hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ Google EmbeddingGemma modeli hazır\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bir batch in embeddingleri oluştuktan sonra reduction yapıyo yani parça parça yapmış oluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            start_embed = time.time()\n",
    "\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "\n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding süresi: {end_embed - start_embed:.2f} saniye\")\n",
    "\n",
    "            pca=PCA(n_components=512)\n",
    "            batch_embeddings_reduced = pca.fit_transform(batch_embeddings) #default 768 olan embedding boyutunu 512ye düşürüyoruz -> PCA\n",
    "            all_embeddings.extend(batch_embeddings_reduced.tolist())\n",
    "\n",
    "            print(f\"  🔹 Embedding oluşturuldu: {i + len(batch_texts)}/{total}\")\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benzerlik (araba - otomobil): 0.547791063785553\n",
      "Benzerlik (araba - kedi): -0.0018093292601406574\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Modeli yükle\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# Küçük dataset\n",
    "train_examples = [\n",
    "    InputExample(texts=[\"araba\", \"otomobil\"], label=1.0),\n",
    "    InputExample(texts=[\"kedi\", \"köpek\"], label=1.0),\n",
    "    InputExample(texts=[\"masa\", \"sandalye\"], label=1.0),\n",
    "    InputExample(texts=[\"araba\", \"uçak\"], label=0.0),\n",
    "    InputExample(texts=[\"kedi\", \"araba\"], label=0.0),\n",
    "    InputExample(texts=[\"masa\", \"araba\"], label=0.0),\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "\n",
    "# Loss fonksiyonu\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Eğitim\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10\n",
    ")\n",
    "\n",
    "# Test\n",
    "emb1 = model.encode(\"araba\")\n",
    "emb2 = model.encode(\"otomobil\")\n",
    "emb3 = model.encode(\"kedi\")\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "print(\"Benzerlik (araba - otomobil):\", cos_sim(emb1, emb2).item())\n",
    "print(\"Benzerlik (araba - kedi):\", cos_sim(emb1, emb3).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge 512 dimension with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal boyut: torch.Size([1, 1024])\n",
      "Yeni boyut: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Örnek 1024 boyutlu embedding\n",
    "embedding_1024 = torch.randn(1, 1024)  # batch size 1\n",
    "\n",
    "# Linear layer ile 1024 -> 512 boyuta düşürme\n",
    "reduce_dim = nn.Linear(1024, 512)\n",
    "\n",
    "# Boyut indirgeme işlemi\n",
    "embedding_512 = reduce_dim(embedding_1024)\n",
    "\n",
    "print(\"Orijinal boyut:\", embedding_1024.shape)\n",
    "print(\"Yeni boyut:\", embedding_512.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Normalize edilmiş, reducer uyumlu, query_points kullanıyor)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    # t: (N, D)\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Embed reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim: int = 1024, output_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "# -------------------------\n",
    "# Processor\n",
    "# -------------------------\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model & reducer\n",
    "        print(f\"🔮 BGE-M3 yükleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(input_dim=1024, output_dim=self.config.EMBEDDING_DIM).to(config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"✅ Hazır - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense dim\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0]\n",
    "            print(f\"✅ BGE-M3 test başarılı - Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"🔍 Sparse embedding mevcut: {'colbert_vecs' in emb_res}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} (dim={self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings: List[List[float]] = []\n",
    "        total = len(texts)\n",
    "        print(f\"🔮 BGE-M3 ile {total} metin işleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # 1) embed (BGE-M3) -> dense (1024)\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "                if isinstance(emb_res, dict) and 'dense_vecs' in emb_res:\n",
    "                    dense = emb_res['dense_vecs']\n",
    "                else:\n",
    "                    dense = emb_res\n",
    "\n",
    "                # 2) to tensor, device\n",
    "                if not isinstance(dense, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense.to(self.config.DEVICE)\n",
    "\n",
    "                # 3) reducer -> 512\n",
    "                with torch.no_grad():\n",
    "                    reduced = self.reducer(dense_t)\n",
    "\n",
    "                # 4) normalize L2 (important for cosine)\n",
    "                reduced = l2_normalize_tensor(reduced)\n",
    "\n",
    "                # 5) append as python lists (cpu)\n",
    "                all_embeddings.extend([v.cpu().tolist() for v in reduced])\n",
    "\n",
    "                print(f\"  📊 Batch işlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ BGE-M3 Embedding hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                # fallback zero vectors\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"📄 CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"❌ Ana metin sütunu bulunamadı\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "\n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"❌ Embedding sayısı uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for chunk, emb in zip(chunks, embeddings):\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i+batch, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "\n",
    "    # ---------- SEARCH (query_points + reducer + normalize) ----------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "            print(f\"🔍 Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "\n",
    "            # use query_points (recommended)\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            # prepare reduced + normalized query vector same as above\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"📊 {len(results)} filtreli sonuç bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Filtreli arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline + main\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Full pipeline başlıyor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        # recreate collection to ensure clean 512-dim index\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ Chunk bulunamadı\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\n🔎 İnteraktif arama başlatıldı\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana menü\")\n",
    "            ch = input(\"Seçiminiz (1-3): \").strip()\n",
    "            if ch == \"3\":\n",
    "                break\n",
    "            if ch not in {\"1\", \"2\"}:\n",
    "                print(\"❌ Geçersiz seçim\")\n",
    "                continue\n",
    "            q = input(\"🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if q.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Kaç sonuç? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch == \"1\":\n",
    "                # try with low threshold first for debugging\n",
    "                results = self.processor.search_semantic(q, limit=limit, score_threshold=None)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (örn: '6.HukukDairesi', boş = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit, score_threshold=None)\n",
    "\n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n📋 {len(results)} sonuç:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_normal_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK SİSTEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) İnteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini göster\")\n",
    "        print(\"4) Çıkış\")\n",
    "        choice = input(\"Seçiminiz (1-4): \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"✅ Tamamlandı\" if ok else \"❌ Hata çıktı\")\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 Görüşürüz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"✅ FlagEmbedding yüklü\")\n",
    "    except ImportError:\n",
    "        print(\"❌ FlagEmbedding bulunamadı — pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
