{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cohere modeli hfden upload etmeyi deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "hf_token = os.getenv(\"HF_API_KEY\")  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\", \n",
    "    token=hf_token\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\",\n",
    "    token=hf_token,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "# Ã–rnek metinler\n",
    "texts = [\"Bu bir test cÃ¼mlesidir.\", \"Mahkeme kararÄ±nÄ± verdi.\"]\n",
    "\n",
    "# Tokenize et\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Modeli kullanarak embedding al\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Burada last_hidden_state'den mean pooling yapÄ±yoruz\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(\"Embedding boyutu:\", embeddings.shape)\n",
    "print(\"Ä°lk embedding Ã¶rneÄŸi:\", embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "# Hugging Face API key ortam deÄŸiÅŸkeninden Ã§ek\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli token ile yÃ¼kle\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token  # API key burada\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlik hesapla\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasÄ±nÄ± yÃ¼kle\n",
    "load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\")\n",
    "\n",
    "# Hugging Face API key\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli yÃ¼kle (gated model ise token ver)\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# Embeddingleri al\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlikleri cosine similarity ile hesapla\n",
    "similarities = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Benzerlik matrisi:\\n\", similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "vec = model.encode([\"test\"])\n",
    "print(vec.shape)  # (1, 784) olmalÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk ederken cpu mu gpu mu kullanÄ±lÄ±yor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA kullanÄ±labilir mi? \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU adÄ±:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU kullanÄ±lÄ±yor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarÄ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarÄ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarÄ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 768  # embeddinggemma-300m embedding boyutu\n",
    "\n",
    "    # Dosya ayarlarÄ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 256\n",
    "\n",
    "    # Cihaz ayarÄ±\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli (GPU/CPU seÃ§imi burada yapÄ±lÄ±yor)\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        print(f\"âœ… Model ÅŸu cihazda Ã§alÄ±ÅŸÄ±yor: {config.DEVICE}\")\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"âœ… SemChunk hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… Google EmbeddingGemma modeli hazÄ±r\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bir batch in embeddingleri oluÅŸtuktan sonra reduction yapÄ±yo yani parÃ§a parÃ§a yapmÄ±ÅŸ oluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            start_embed = time.time()\n",
    "\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "\n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding sÃ¼resi: {end_embed - start_embed:.2f} saniye\")\n",
    "\n",
    "            pca=PCA(n_components=512)\n",
    "            batch_embeddings_reduced = pca.fit_transform(batch_embeddings) #default 768 olan embedding boyutunu 512ye dÃ¼ÅŸÃ¼rÃ¼yoruz -> PCA\n",
    "            all_embeddings.extend(batch_embeddings_reduced.tolist())\n",
    "\n",
    "            print(f\"  ğŸ”¹ Embedding oluÅŸturuldu: {i + len(batch_texts)}/{total}\")\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Modeli yÃ¼kle\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# KÃ¼Ã§Ã¼k dataset\n",
    "train_examples = [\n",
    "    InputExample(texts=[\"araba\", \"otomobil\"], label=1.0),\n",
    "    InputExample(texts=[\"kedi\", \"kÃ¶pek\"], label=1.0),\n",
    "    InputExample(texts=[\"masa\", \"sandalye\"], label=1.0),\n",
    "    InputExample(texts=[\"araba\", \"uÃ§ak\"], label=0.0),\n",
    "    InputExample(texts=[\"kedi\", \"araba\"], label=0.0),\n",
    "    InputExample(texts=[\"masa\", \"araba\"], label=0.0),\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "\n",
    "# Loss fonksiyonu\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# EÄŸitim\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10\n",
    ")\n",
    "\n",
    "# Test\n",
    "emb1 = model.encode(\"araba\")\n",
    "emb2 = model.encode(\"otomobil\")\n",
    "emb3 = model.encode(\"kedi\")\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "print(\"Benzerlik (araba - otomobil):\", cos_sim(emb1, emb2).item())\n",
    "print(\"Benzerlik (araba - kedi):\", cos_sim(emb1, emb3).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge 512 dimension with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ã–rnek 1024 boyutlu embedding\n",
    "embedding_1024 = torch.randn(1, 1024)  # batch size 1\n",
    "\n",
    "# Linear layer ile 1024 -> 512 boyuta dÃ¼ÅŸÃ¼rme\n",
    "reduce_dim = nn.Linear(1024, 512)\n",
    "\n",
    "# Boyut indirgeme iÅŸlemi\n",
    "embedding_512 = reduce_dim(embedding_1024)\n",
    "\n",
    "print(\"Orijinal boyut:\", embedding_1024.shape)\n",
    "print(\"Yeni boyut:\", embedding_512.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Normalize edilmiÅŸ, reducer uyumlu, query_points kullanÄ±yor)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    # t: (N, D)\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Embed reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim: int = 1024, output_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "# -------------------------\n",
    "# Processor\n",
    "# -------------------------\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model & reducer\n",
    "        print(f\"ğŸ”® BGE-M3 yÃ¼kleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(input_dim=1024, output_dim=self.config.EMBEDDING_DIM).to(config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"âœ… HazÄ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense dim\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"YargÄ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararÄ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0]\n",
    "            print(f\"âœ… BGE-M3 test baÅŸarÄ±lÄ± - Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"ğŸ” Sparse embedding mevcut: {'colbert_vecs' in emb_res}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ BGE-M3 baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ğŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name} (dim={self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunking hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings: List[List[float]] = []\n",
    "        total = len(texts)\n",
    "        print(f\"ğŸ”® BGE-M3 ile {total} metin iÅŸleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # 1) embed (BGE-M3) -> dense (1024)\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "                if isinstance(emb_res, dict) and 'dense_vecs' in emb_res:\n",
    "                    dense = emb_res['dense_vecs']\n",
    "                else:\n",
    "                    dense = emb_res\n",
    "\n",
    "                # 2) to tensor, device\n",
    "                if not isinstance(dense, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense.to(self.config.DEVICE)\n",
    "\n",
    "                # 3) reducer -> 512\n",
    "                with torch.no_grad():\n",
    "                    reduced = self.reducer(dense_t)\n",
    "\n",
    "                # 4) normalize L2 (important for cosine)\n",
    "                reduced = l2_normalize_tensor(reduced)\n",
    "\n",
    "                # 5) append as python lists (cpu)\n",
    "                all_embeddings.extend([v.cpu().tolist() for v in reduced])\n",
    "\n",
    "                print(f\"  ğŸ“Š Batch iÅŸlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ BGE-M3 Embedding hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # fallback zero vectors\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"ğŸ“„ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"ğŸ“Š {len(df)} satÄ±r yÃ¼klendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"âŒ Ana metin sÃ¼tunu bulunamadÄ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ğŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"ğŸš€ {len(chunks)} chunk Qdrant'a yÃ¼kleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "\n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"âŒ Embedding sayÄ±sÄ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for chunk, emb in zip(chunks, embeddings):\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i+batch, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(\"ğŸ‰ YÃ¼kleme tamamlandÄ±!\")\n",
    "\n",
    "    # ---------- SEARCH (query_points + reducer + normalize) ----------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "            print(f\"ğŸ” Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "\n",
    "            # use query_points (recommended)\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"ğŸ“Š {len(results)} sonuÃ§ bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            # prepare reduced + normalized query vector same as above\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"ğŸ“Š {len(results)} filtreli sonuÃ§ bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Filtreli arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline + main\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ğŸš€ Full pipeline baÅŸlÄ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        # recreate collection to ensure clean 512-dim index\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"âŒ Chunk bulunamadÄ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nğŸ“Š Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nğŸ” Ä°nteraktif arama baÅŸlatÄ±ldÄ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana menÃ¼\")\n",
    "            ch = input(\"SeÃ§iminiz (1-3): \").strip()\n",
    "            if ch == \"3\":\n",
    "                break\n",
    "            if ch not in {\"1\", \"2\"}:\n",
    "                print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "                continue\n",
    "            q = input(\"ğŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if q.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"KaÃ§ sonuÃ§? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch == \"1\":\n",
    "                # try with low threshold first for debugging\n",
    "                results = self.processor.search_semantic(q, limit=limit, score_threshold=None)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (Ã¶rn: '6.HukukDairesi', boÅŸ = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit, score_threshold=None)\n",
    "\n",
    "            if not results:\n",
    "                print(\"âŒ SonuÃ§ bulunamadÄ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nğŸ“‹ {len(results)} sonuÃ§:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_normal_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K SÄ°STEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) Ä°nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini gÃ¶ster\")\n",
    "        print(\"4) Ã‡Ä±kÄ±ÅŸ\")\n",
    "        choice = input(\"SeÃ§iminiz (1-4): \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"âœ… TamamlandÄ±\" if ok else \"âŒ Hata Ã§Ä±ktÄ±\")\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"âœ… FlagEmbedding yÃ¼klÃ¼\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ FlagEmbedding bulunamadÄ± â€” pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# YargÄ±tay KararlarÄ± iÃ§in Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # L2 normalization ekledik\n",
    "        x = self.linear(x)\n",
    "        return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarÄ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarÄ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarÄ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512  #collection boyutu\n",
    "\n",
    "    # Dosya ayarlarÄ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE: int = 256\n",
    "\n",
    "    \n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"YargÄ±tay kararlarÄ± iÃ§in semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        # Reducer modeli - daha iyi initialization ile\n",
    "        self.reducer = EmbedReducer(768, 512)\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.reducer.to(self.device)\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"âœ… SemChunk hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… Google EmbeddingGemma modeli hazÄ±r\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n",
    "        print(f\"âœ… Device: {self.device}\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ğŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        self.reducer.eval()  # Eval moduna al\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_embed = time.time()\n",
    "            \n",
    "            # Embedding oluÅŸtur\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = self.model.encode(\n",
    "                    batch_texts, \n",
    "                    show_progress_bar=True, \n",
    "                    convert_to_tensor=True, \n",
    "                    normalize_embeddings=True  # Bu Ã¶nemli!\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Boyut dÃ¼ÅŸÃ¼r ve normalize et\n",
    "                reduced_vector = self.reducer(batch_embeddings)\n",
    "                \n",
    "                print(\"*\" * 40)\n",
    "                print(f\"Original embedding shape: {batch_embeddings.shape}\")\n",
    "                print(f\"Reduced vector shape: {reduced_vector.shape}\")\n",
    "                print(f\"Vector norm check (should be ~1.0): {torch.norm(reduced_vector[0]).item():.4f}\")\n",
    "                print(\"*\" * 40)\n",
    "                \n",
    "                # CPU'ya taÅŸÄ± ve listeye Ã§evir\n",
    "                all_embeddings.extend(reduced_vector.cpu().tolist())\n",
    "            \n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding sÃ¼resi: {end_embed - start_embed:.2f} saniye\")\n",
    "            print(f\"  ğŸ”¹ Embedding oluÅŸturuldu: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"ğŸ“„ Toplam {total_rows} satÄ±r iÅŸlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ğŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"ğŸ”® Embedding'ler oluÅŸturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"ğŸš€ {total_points} chunk Qdrant'a yÃ¼kleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                start_upload = time.time()\n",
    "\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "\n",
    "                end_upload = time.time()\n",
    "                print(f\"batch Qdrant yÃ¼kleme sÃ¼resi: {end_upload - start_upload:.2f} saniye\")\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(f\"ğŸ‰ {total_points} chunk Qdrant'a yÃ¼klendi!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"DÃ¼zeltilmiÅŸ search metodu - daha dÃ¼ÅŸÃ¼k threshold ile\"\"\"\n",
    "        \n",
    "        self.reducer.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Query embedding Ã§Ä±kar (normalize edilmiÅŸ)\n",
    "            query_embedding = self.model.encode(\n",
    "                [query], \n",
    "                convert_to_tensor=True, \n",
    "                normalize_embeddings=True  # Bu Ã¶nemli!\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Boyutu 512'ye dÃ¼ÅŸÃ¼r ve normalize et\n",
    "            reduced_query_embedding = self.reducer(query_embedding)\n",
    "            \n",
    "            print(f\"ğŸ” Query vector boyutu: {reduced_query_embedding.shape}\")\n",
    "            print(f\"ğŸ” Query vector norm: {torch.norm(reduced_query_embedding[0]).item():.4f}\")\n",
    "            \n",
    "            # CPU'ya taÅŸÄ± ve numpy array'e Ã§evir\n",
    "            query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "        \n",
    "        print(f\"ğŸ” Final query vector boyutu: {len(query_vector)} (hedef: {self.config.DIMENSION})\")\n",
    "        \n",
    "        try:\n",
    "            # Qdrant aramasÄ± - daha dÃ¼ÅŸÃ¼k threshold ile\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"ğŸ” Ham arama sonucu sayÄ±sÄ±: {len(search_results)}\")\n",
    "            \n",
    "            # EÄŸer hiÃ§ sonuÃ§ yoksa, threshold'suz deneyelim\n",
    "            if not search_results:\n",
    "                print(\"âš ï¸ Threshold ile sonuÃ§ yok, threshold'suz deneniyor...\")\n",
    "                search_results = self.qdrant_client.search(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    query_vector=query_vector,\n",
    "                    limit=limit\n",
    "                )\n",
    "                print(f\"ğŸ” Threshold'suz sonuÃ§ sayÄ±sÄ±: {len(search_results)}\")\n",
    "                \n",
    "                if search_results:\n",
    "                    print(f\"ğŸ” En yÃ¼ksek score: {search_results[0].score:.4f}\")\n",
    "                    print(f\"ğŸ” En dÃ¼ÅŸÃ¼k score: {search_results[-1].score:.4f}\")\n",
    "            \n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def debug_search_issue(self, query: str):\n",
    "        \"\"\"Arama problemini debug et\"\"\"\n",
    "        print(f\"ğŸ”§ Debug baÅŸlÄ±yor - Query: '{query}'\")\n",
    "        \n",
    "        # 1. Koleksiyon durumunu kontrol et\n",
    "        info = self.get_collection_info()\n",
    "        print(f\"ğŸ“Š Koleksiyon durumu: {info}\")\n",
    "        \n",
    "        if info.get('points_count', 0) == 0:\n",
    "            print(\"âŒ Koleksiyonda hiÃ§ point yok!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Query embedding oluÅŸtur\n",
    "        self.reducer.eval()\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model.encode([query], convert_to_tensor=True, normalize_embeddings=True).to(self.device)\n",
    "            reduced_query = self.reducer(query_embedding)\n",
    "            query_vector = reduced_query[0].cpu().numpy().tolist()\n",
    "            \n",
    "        print(f\"ğŸ” Query vector shape: {len(query_vector)}\")\n",
    "        print(f\"ğŸ” Query vector norm: {np.linalg.norm(query_vector):.4f}\")\n",
    "        print(f\"ğŸ” Query vector sample: {query_vector[:5]}\")\n",
    "        \n",
    "        # 3. Ã‡ok dÃ¼ÅŸÃ¼k threshold ile ara\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=5,\n",
    "                score_threshold=0.0  # Ã‡ok dÃ¼ÅŸÃ¼k threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"ğŸ” Score threshold 0.0 ile bulunan sonuÃ§: {len(results)}\")\n",
    "            if results:\n",
    "                for i, r in enumerate(results):\n",
    "                    print(f\"  {i+1}. Score: {r.score:.6f}\")\n",
    "                    text = r.payload.get('text', '')[:100]\n",
    "                    print(f\"     Text preview: {text}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Debug search hatasÄ±: {e}\")\n",
    "    \n",
    "    def fix_dimension_mismatch(self):\n",
    "        \"\"\"Dimension uyumsuzluÄŸunu dÃ¼zelt\"\"\"\n",
    "        print(\"ğŸ”§ Dimension uyumsuzluÄŸu dÃ¼zeltiliyor...\")\n",
    "        response = input(\"âš ï¸ Bu iÅŸlem koleksiyonu silecek. Devam? (y/N): \")\n",
    "        if response.lower() != 'y':\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.config.COLLECTION_NAME)\n",
    "            print(\"âœ… Koleksiyon silindi\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Silme hatasÄ±: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Pipeline sÄ±nÄ±fÄ±\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ğŸš€ YargÄ±tay Semantic Pipeline BaÅŸlÄ±yor\")\n",
    "\n",
    "        total_start = time.time()\n",
    "\n",
    "        # Koleksiyon oluÅŸturma\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        # CSV iÅŸlemleri ve chunk oluÅŸturma\n",
    "        chunk_start = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        chunk_end = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"âŒ Ä°ÅŸlenecek chunk bulunamadÄ±\")\n",
    "            return False\n",
    "\n",
    "        # Embedding oluÅŸturma ve Qdrant yÃ¼kleme\n",
    "        upload_start = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        upload_end = time.time()\n",
    "\n",
    "        total_end = time.time()\n",
    "\n",
    "        # Toplam istatistikler\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nğŸ“Š Pipeline SÃ¼releri ve Ä°statistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(chunk_end - chunk_start, 2),\n",
    "            \"embedding_and_upload_time_s\": round(upload_end - upload_start, 2),\n",
    "            \"total_pipeline_time_s\": round(total_end - total_start, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"ğŸ” Ä°nteraktif Arama BaÅŸlatÄ±ldÄ±\")\n",
    "        \n",
    "        # Ã–nce koleksiyon durumunu kontrol et\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(f\"ğŸ“Š Koleksiyon Durumu: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nğŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                print(\"âŒ BoÅŸ sorgu, tekrar deneyin\")\n",
    "                continue\n",
    "            \n",
    "            limit_input = input(\"KaÃ§ sonuÃ§? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "                limit = max(1, min(limit, 50))\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz sayÄ±, varsayÄ±lan 5 kullanÄ±lÄ±yor\")\n",
    "                limit = 5\n",
    "\n",
    "            score_input = input(\"Minimum score? (default 0.3): \").strip()\n",
    "            try:\n",
    "                score_threshold = float(score_input) if score_input else 0.3\n",
    "                score_threshold = max(0.0, min(score_threshold, 1.0))\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz score, varsayÄ±lan 0.3 kullanÄ±lÄ±yor\")\n",
    "                score_threshold = 0.3\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            results = self.processor.search_semantic(query, limit=limit, score_threshold=score_threshold)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"âŒ HiÃ§ sonuÃ§ bulunamadÄ±.\")\n",
    "                print(\"ğŸ’¡ Ã–neriler:\")\n",
    "                print(\"   - Score threshold'u dÃ¼ÅŸÃ¼rÃ¼n (0.03 veya 0.05)\")\n",
    "                print(\"   - Debug modunu Ã§alÄ±ÅŸtÄ±rÄ±n (ana menÃ¼den 4)\")\n",
    "                print(\"   - Daha spesifik kelimeler kullanÄ±n\")\n",
    "            else:\n",
    "                print(f\"\\nğŸ“‹ {len(results)} SonuÃ§ Bulundu:\")\n",
    "                for i, r in enumerate(results, 1):\n",
    "                    payload = r['payload']\n",
    "                    text_preview = payload.get('text', '')[:200] + \"...\" if len(payload.get('text', '')) > 200 else payload.get('text', '')\n",
    "                    print(f\"\\n{i}. ğŸ“Š Score: {r['score']:.4f}\")\n",
    "                    print(f\"   ğŸ“‹ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“‹ Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“‹ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“‹ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“„ Metin: {text_preview}\")\n",
    "                    print(f\"   {'â”€'*50}\")\n",
    "            \n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_gemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"1. Full pipeline Ã§alÄ±ÅŸtÄ±r\")\n",
    "        print(\"2. Arama yap\") \n",
    "        print(\"3. Koleksiyon bilgisi\")\n",
    "        print(\"4. ğŸ”§ Debug arama sorunu\")\n",
    "        print(\"5. ğŸ—‘ï¸ Dimension uyumsuzluÄŸunu dÃ¼zelt\")\n",
    "        print(\"6. Ã‡Ä±kÄ±ÅŸ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        choice = input(\"SeÃ§im: \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "            \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "            \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nğŸ“Š Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "            \n",
    "        elif choice == \"4\":\n",
    "            query = input(\"Debug iÃ§in test query (Enter: 'test'): \").strip() or \"test\"\n",
    "            pipeline.processor.debug_search_issue(query)\n",
    "            \n",
    "        elif choice == \"5\":\n",
    "            if pipeline.processor.fix_dimension_mismatch():\n",
    "                print(\"âœ… Koleksiyon sÄ±fÄ±rlandÄ±. Åimdi '1' seÃ§eneÄŸi ile veriyi yeniden yÃ¼kleyin.\")\n",
    "            else:\n",
    "                print(\"âŒ Ä°ÅŸlem iptal edildi\")\n",
    "                \n",
    "        elif choice == \"6\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bge no reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (1024->512 via slice, L2 normalize, no reducer)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:#normlarÄ± 1 yapÄ±lÄ±yo Ã§Ã¼nkÃ¼ cosine'da yÃ¶n Ã¶nemli bÃ¼yÃ¼klÃ¼k deÄŸil\n",
    "    # t: (N, D) or (D,)\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model (no reducer)\n",
    "        print(f\"ğŸ”® BGE-M3 yÃ¼kleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"âœ… HazÄ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense dim\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"YargÄ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararÄ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            print(f\"âœ… BGE-M3 test baÅŸarÄ±lÄ± - Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"ğŸ” Sparse embedding mevcut: {'colbert_vecs' in emb_res}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ BGE-M3 baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ğŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name} (dim={self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunking hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings: List[List[float]] = []\n",
    "        total = len(texts)\n",
    "        print(f\"ğŸ”® BGE-M3 ile {total} metin iÅŸleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # 1) embed (BGE-M3) -> dense (1024)\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "                if isinstance(emb_res, dict) and 'dense_vecs' in emb_res:\n",
    "                    dense = emb_res['dense_vecs']\n",
    "                else:\n",
    "                    dense = emb_res\n",
    "\n",
    "                # 2) to tensor, device\n",
    "                if not isinstance(dense, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense.to(self.config.DEVICE)\n",
    "\n",
    "                # 3) slice first 512 dims and normalize\n",
    "                with torch.no_grad():\n",
    "                    sliced = dense_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    normed = l2_normalize_tensor(sliced)\n",
    "\n",
    "                # 4) append as python lists (cpu)\n",
    "                all_embeddings.extend([v.cpu().tolist() for v in normed])\n",
    "\n",
    "                print(f\"  ğŸ“Š Batch iÅŸlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ BGE-M3 Embedding hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # fallback zero vectors\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"ğŸ“„ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"ğŸ“Š {len(df)} satÄ±r yÃ¼klendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"âŒ Ana metin sÃ¼tunu bulunamadÄ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ğŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"ğŸš€ {len(chunks)} chunk Qdrant'a yÃ¼kleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "\n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"âŒ Embedding sayÄ±sÄ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for chunk, emb in zip(chunks, embeddings):\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i+batch, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(\"ğŸ‰ YÃ¼kleme tamamlandÄ±!\")\n",
    "\n",
    "    # ---------- SEARCH (query_points + slice + normalize) ----------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            # to tensor, slice first 512 dims and normalize\n",
    "            q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                q_sliced = q_t[0, :self.config.EMBEDDING_DIM]\n",
    "                q_norm = l2_normalize_tensor(q_sliced)\n",
    "\n",
    "            query_vector = q_norm.cpu().tolist()\n",
    "            print(f\"ğŸ” Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "\n",
    "            # use query_points (recommended)\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"ğŸ“Š {len(results)} sonuÃ§ bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                q_sliced = q_t[0, :self.config.EMBEDDING_DIM]\n",
    "                q_norm = l2_normalize_tensor(q_sliced)\n",
    "\n",
    "            query_vector = q_norm.cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"ğŸ“Š {len(results)} filtreli sonuÃ§ bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Filtreli arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline + main\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ğŸš€ Full pipeline baÅŸlÄ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        # recreate collection to ensure clean 512-dim index\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"âŒ Chunk bulunamadÄ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nğŸ“Š Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nğŸ” Ä°nteraktif arama baÅŸlatÄ±ldÄ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana menÃ¼\")\n",
    "            ch = input(\"SeÃ§iminiz (1-3): \").strip()\n",
    "            if ch == \"3\":\n",
    "                break\n",
    "            if ch not in {\"1\", \"2\"}:\n",
    "                print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "                continue\n",
    "            q = input(\"ğŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if q.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"KaÃ§ sonuÃ§? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch == \"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit, score_threshold=None)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (Ã¶rn: '6.HukukDairesi', boÅŸ = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit, score_threshold=None)\n",
    "\n",
    "            if not results:\n",
    "                print(\"âŒ SonuÃ§ bulunamadÄ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nğŸ“‹ {len(results)} sonuÃ§:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_test_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K SÄ°STEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) Ä°nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini gÃ¶ster\")\n",
    "        print(\"4) Ã‡Ä±kÄ±ÅŸ\")\n",
    "        choice = input(\"SeÃ§iminiz (1-4): \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"âœ… TamamlandÄ±\" if ok else \"âŒ Hata Ã§Ä±ktÄ±\")\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"âœ… FlagEmbedding yÃ¼klÃ¼\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ FlagEmbedding bulunamadÄ± â€” pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model seÃ§me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# Dynamic Model Selection + SemChunk + Qdrant Entegrasyon\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Model yapÄ±landÄ±rmalarÄ±\n",
    "MODEL_CONFIGS = {\n",
    "    \"bge-m3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"model_type\": \"bge\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 8192,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE-M3 - Ã‡ok dilli, dense+sparse embedding destekli\"\n",
    "    },\n",
    "    \"bge-large\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE Large - Sadece dense embedding\"\n",
    "    },\n",
    "    \"multilingual-e5\": {\n",
    "        \"model_name\": \"intfloat/multilingual-e5-large\",\n",
    "        \"model_type\": \"sentence_transformer\", \n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"E5 Multilingual Large - Ã‡ok dilli dense embedding\"\n",
    "    },\n",
    "    \"turkish-bert\": {\n",
    "        \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"Turkish BERT - TÃ¼rkÃ§e Ã¶zelleÅŸtirilmiÅŸ\"\n",
    "    },\n",
    "    \"all-mpnet\": {\n",
    "        \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 384,\n",
    "        \"supports_sparse\": False,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"All-MiniLM - Genel amaÃ§lÄ±, hÄ±zlÄ±\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    MODEL_TYPE: str = \"bge\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"dynamic_model_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "    SUPPORTS_SPARSE: bool = True\n",
    "    SUPPORTS_DENSE: bool = True\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Dinamik model yÃ¶netimi iÃ§in sÄ±nÄ±f\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, config: Config):\n",
    "        self.model_key = model_key\n",
    "        self.config = config\n",
    "        self.model_config = MODEL_CONFIGS.get(model_key)\n",
    "        \n",
    "        if not self.model_config:\n",
    "            raise ValueError(f\"Desteklenmeyen model: {model_key}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.vectorizer = None  # Sparse embedding iÃ§in\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"SeÃ§ilen modeli yÃ¼kle\"\"\"\n",
    "        try:\n",
    "            model_name = self.model_config[\"model_name\"]\n",
    "            model_type = self.model_config[\"model_type\"]\n",
    "            \n",
    "            print(f\"ğŸ”® Model yÃ¼kleniyor: {model_name} (tip: {model_type})\")\n",
    "            \n",
    "            if model_type == \"bge\":\n",
    "                self.model = BGEM3FlagModel(\n",
    "                    model_name, \n",
    "                    use_fp16=self.config.USE_FP16, \n",
    "                    device=self.config.DEVICE\n",
    "                )\n",
    "            elif model_type == \"sentence_transformer\":\n",
    "                self.model = SentenceTransformer(model_name, device=self.config.DEVICE)\n",
    "                # Sparse embedding iÃ§in TF-IDF hazÄ±rla\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "            else:\n",
    "                raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "                \n",
    "            print(f\"âœ… Model yÃ¼klendi: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Model yÃ¼kleme hatasÄ±: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Metinleri encode et - model tipine gÃ¶re\"\"\"\n",
    "        dense_embeddings = []\n",
    "        sparse_embeddings = []\n",
    "        \n",
    "        try:\n",
    "            if self.model_config[\"model_type\"] == \"bge\" and hasattr(self.model, 'encode'):\n",
    "                # BGE-M3 iÃ§in\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    result = self.model.encode(\n",
    "                        texts, \n",
    "                        return_dense=True, \n",
    "                        return_sparse=True\n",
    "                    )\n",
    "                    dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "                    sparse_raw = result.get(\"sparse_vecs\", [])\n",
    "                    sparse_embeddings = [\n",
    "                        {\"indices\": s.get(\"indices\", []), \"values\": s.get(\"values\", [])} \n",
    "                        for s in sparse_raw\n",
    "                    ] if sparse_raw else [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                else:\n",
    "                    dense_embeddings = self.model.encode(texts, return_dense=True)\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                    \n",
    "            else:\n",
    "                # Sentence Transformer iÃ§in\n",
    "                dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "                \n",
    "                # Sparse embedding TF-IDF ile\n",
    "                if self.vectorizer:\n",
    "                    X_sparse = self.vectorizer.fit_transform(texts)\n",
    "                    sparse_embeddings = []\n",
    "                    for i in range(X_sparse.shape[0]):\n",
    "                        row = X_sparse[i].tocoo()\n",
    "                        sparse_embeddings.append({\n",
    "                            \"indices\": row.col.tolist(),\n",
    "                            \"values\": row.data.tolist()\n",
    "                        })\n",
    "                else:\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            \n",
    "            # Embedding boyutunu ayarla\n",
    "            target_dim = self.config.EMBEDDING_DIM\n",
    "            dense_clean = []\n",
    "            for vec in dense_embeddings:\n",
    "                if vec is None:\n",
    "                    dense_clean.append([0.0] * target_dim)\n",
    "                elif len(vec) < target_dim:\n",
    "                    dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "                else:\n",
    "                    dense_clean.append(vec[:target_dim])\n",
    "            \n",
    "            return dense_clean, sparse_embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Encoding hatasÄ±: {e}\")\n",
    "            # Fallback\n",
    "            return (\n",
    "                [[0.0] * self.config.EMBEDDING_DIM for _ in texts],\n",
    "                [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            )\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Model bilgilerini dÃ¶ndÃ¼r\"\"\"\n",
    "        info = self.model_config.copy()\n",
    "        info[\"loaded\"] = self.model is not None\n",
    "        info[\"current_embedding_dim\"] = self.config.EMBEDDING_DIM\n",
    "        return info\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.config = config\n",
    "        self.model_manager = ModelManager(model_key, config)\n",
    "        \n",
    "        # Model bilgilerini config'e aktar\n",
    "        model_config = self.model_manager.model_config\n",
    "        self.config.BGE_MODEL_NAME = model_config[\"model_name\"]\n",
    "        self.config.MODEL_TYPE = model_config[\"model_type\"]\n",
    "        self.config.SUPPORTS_SPARSE = model_config[\"supports_sparse\"]\n",
    "        self.config.SUPPORTS_DENSE = model_config[\"supports_dense\"]\n",
    "        \n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Model yÃ¼kle\n",
    "        if not self.model_manager.load_model():\n",
    "            raise RuntimeError(\"Model yÃ¼klenemedi!\")\n",
    "        \n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"âœ… HazÄ±r - Model: {model_config['model_name']} | Cihaz: {device_name}\")\n",
    "\n",
    "    def test_model_connection(self):\n",
    "        \"\"\"Model baÄŸlantÄ±sÄ±nÄ± test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"YargÄ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararÄ±\"]\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts(test_text)\n",
    "            \n",
    "            print(f\"âœ… Dense embedding boyutu: {len(dense_emb[0])}\")\n",
    "            print(f\"ğŸ” Sparse embedding mevcut: {len(sparse_emb[0]['indices']) > 0}\")\n",
    "            return len(dense_emb[0])\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Model baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ğŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense vector config\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM, \n",
    "                        distance=models.Distance.COSINE\n",
    "                    ),\n",
    "                }\n",
    "                \n",
    "                \n",
    "                sparse_config = {}\n",
    "                if self.config.SUPPORTS_SPARSE:\n",
    "                    sparse_config = {\n",
    "                        \"sparse_vec\": models.SparseVectorParams(\n",
    "                            index=models.SparseIndexParams(on_disk=False)\n",
    "                        )\n",
    "                    }\n",
    "                \n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config=sparse_config if sparse_config else None\n",
    "                )\n",
    "                \n",
    "                support_info = \"Dense+Sparse\" if sparse_config else \"Dense only\"\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name} ({support_info})\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunking hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding oluÅŸtur\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"ğŸ”® {total} metin iÅŸleniyor (model: {self.config.BGE_MODEL_NAME})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  ğŸ“Š Batch iÅŸlendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Embedding hatasÄ± (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"ğŸ“„ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"ğŸ“Š {len(df)} satÄ±r yÃ¼klendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"âŒ Ana metin sÃ¼tunu bulunamadÄ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "                'model_used': self.config.BGE_MODEL_NAME  # Hangi model kullanÄ±ldÄ±ÄŸÄ±nÄ± kaydet\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ğŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"ğŸš€ {len(chunks)} chunk Qdrant'a yÃ¼kleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings(texts)\n",
    "\n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vector_dict = {\"dense_vec\": d}\n",
    "            \n",
    "            # Sparse vector sadece destekleniyorsa ekle\n",
    "            if self.config.SUPPORTS_SPARSE and s[\"indices\"]:\n",
    "                vector_dict[\"sparse_vec\"] = SparseVector(\n",
    "                    indices=s[\"indices\"],\n",
    "                    values=s[\"values\"]\n",
    "                )\n",
    "            \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector_dict,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(\"ğŸ‰ YÃ¼kleme tamamlandÄ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "        try:\n",
    "            dense_emb, _ = self.model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0]\n",
    "            \n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=NamedVector(name=\"dense_vec\", vector=query_vector),\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"ğŸ“Š {len(results)} sonuÃ§ bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Semantic search hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Hybrid search (sadece destekleniyorsa)\"\"\"\n",
    "        if not self.config.SUPPORTS_SPARSE:\n",
    "            print(\"âš ï¸ Bu model sparse embedding desteklemiyor, dense search yapÄ±lÄ±yor...\")\n",
    "            return self.search_semantic(query, limit, score_threshold)\n",
    "        \n",
    "        try:\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts([query])\n",
    "            \n",
    "            requests = [\n",
    "                SearchRequest(\n",
    "                    vector=NamedVector(name=\"dense_vec\", vector=dense_emb[0]),\n",
    "                    limit=limit,\n",
    "                    with_payload=True,\n",
    "                    score_threshold=score_threshold\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Sparse varsa ekle\n",
    "            if sparse_emb[0][\"indices\"]:\n",
    "                requests.append(\n",
    "                    SearchRequest(\n",
    "                        vector=NamedSparseVector(\n",
    "                            name=\"sparse_vec\",\n",
    "                            vector=SparseVector(\n",
    "                                indices=sparse_emb[0][\"indices\"],\n",
    "                                values=sparse_emb[0][\"values\"]\n",
    "                            )\n",
    "                        ),\n",
    "                        limit=limit,\n",
    "                        score_threshold=score_threshold\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            results = []\n",
    "            for request_result in qr:\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"ğŸ“Š {len(results)} sonuÃ§ bulundu (Hybrid)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Hybrid search hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            model_info = self.model_manager.get_model_info()\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"model_info\": model_info,\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM,\n",
    "                \"supports_sparse\": self.config.SUPPORTS_SPARSE\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.processor = YargitaySemanticProcessor(config, model_key)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ğŸš€ Full pipeline baÅŸlÄ±yor\")\n",
    "        emb_dim = self.processor.test_model_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"âŒ Chunk bulunamadÄ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nğŸ“Š Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nğŸ” Ä°nteraktif arama baÅŸlatÄ±ldÄ±\")\n",
    "        model_info = self.processor.model_manager.get_model_info()\n",
    "        print(f\"ğŸ“± Aktif model: {model_info['model_name']}\")\n",
    "        print(f\"ğŸ”§ Ã–zellikler: Denseâœ…, Sparse{'âœ…' if model_info['supports_sparse'] else 'âŒ'}\")\n",
    "        \n",
    "        while True:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"ğŸ” ARAMA SEÃ‡ENEKLERÄ°\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(\"1) Dense arama (Semantic)\")\n",
    "            if model_info['supports_sparse']:\n",
    "                print(\"2) Hybrid arama (Dense + Sparse)\")\n",
    "                print(\"3) KarÅŸÄ±laÅŸtÄ±rmalÄ± arama (Her iki yÃ¶ntem)\")\n",
    "            else:\n",
    "                print(\"2) âŒ Hybrid arama (Bu model desteklemiyor)\")\n",
    "                print(\"3) âŒ KarÅŸÄ±laÅŸtÄ±rma (Sparse desteklenmiyor)\")\n",
    "            print(\"4) Ana menÃ¼\")\n",
    "            \n",
    "            ch = input(\"SeÃ§iminiz (1-4): \").strip()\n",
    "            if ch==\"4\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\",\"3\"}:\n",
    "                print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "                continue\n",
    "                \n",
    "            q = input(\"ğŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                limit = int(input(\"KaÃ§ sonuÃ§? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                print(\"\\nğŸ¯ Dense Semantic Search...\")\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(results, \"Dense\")\n",
    "                \n",
    "            elif ch==\"2\" and model_info['supports_sparse']:\n",
    "                print(\"\\nğŸ”€ Hybrid Search...\")\n",
    "                results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(results, \"Hybrid\")\n",
    "                \n",
    "            elif ch==\"3\" and model_info['supports_sparse']:\n",
    "                print(\"\\nğŸ“Š KarÅŸÄ±laÅŸtÄ±rmalÄ± Arama...\")\n",
    "                print(\"ğŸ¯ Dense sonuÃ§lar:\")\n",
    "                dense_results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(dense_results, \"Dense\", show_comparison=True)\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"ğŸ”€ Hybrid sonuÃ§lar:\")\n",
    "                hybrid_results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(hybrid_results, \"Hybrid\", show_comparison=True)\n",
    "                \n",
    "            else:\n",
    "                print(\"âš ï¸ Bu Ã¶zellik seÃ§ilen model tarafÄ±ndan desteklenmiyor.\")\n",
    "\n",
    "    def _display_results(self, results: List[Dict], search_type: str, show_comparison: bool = False):\n",
    "        \"\"\"SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼le\"\"\"\n",
    "        if not results:\n",
    "            print(\"âŒ SonuÃ§ bulunamadÄ±\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nğŸ“‹ {len(results)} {search_type} sonuÃ§:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            p = r.get(\"payload\") or {}\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            \n",
    "            # Skor rengine gÃ¶re emoji\n",
    "            if score > 0.8:\n",
    "                score_icon = \"ğŸŸ¢\"\n",
    "            elif score > 0.6:\n",
    "                score_icon = \"ğŸŸ¡\"\n",
    "            else:\n",
    "                score_icon = \"ğŸ”´\"\n",
    "                \n",
    "            print(f\"\\n{i}. {score_icon} Skor: {score:.4f}\")\n",
    "            print(f\"   ğŸ“„ Model: {p.get('model_used','N/A')}\")\n",
    "            print(f\"   ğŸ›ï¸ Daire: {p.get('daire','N/A')} | ğŸ“… Tarih: {p.get('tarih','N/A')}\")\n",
    "            print(f\"   ğŸ“‹ Esas: {p.get('esas_no','N/A')} | ğŸ”¢ Karar: {p.get('karar_no','N/A')}\")\n",
    "            \n",
    "            text = p.get('text', '')\n",
    "            if len(text) > 200:\n",
    "                text_preview = text[:200] + \"...\"\n",
    "            else:\n",
    "                text_preview = text\n",
    "            print(f\"   ğŸ“ Metin: {text_preview}\")\n",
    "            \n",
    "            if show_comparison:\n",
    "                print(f\"   ğŸ·ï¸ Tip: {search_type}\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "def select_model() -> str:\n",
    "    \"\"\"KullanÄ±cÄ±dan model seÃ§imi al\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¤– MODEL SEÃ‡Ä°MÄ°\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Modelleri kategorilere ayÄ±r\n",
    "    categories = {\n",
    "        \"ğŸŒ Ã‡ok Dilli Modeller\": [\"bge-m3\", \"multilingual-e5\"],\n",
    "        \"ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Ã–zel\": [\"turkish-bert\", \"distilbert-turkish\"],\n",
    "        \"âš¡ HÄ±zlÄ± & Genel\": [\"bge-large\", \"all-mpnet\"]\n",
    "    }\n",
    "    \n",
    "    for category, models in categories.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for model_key in models:\n",
    "            config = MODEL_CONFIGS[model_key]\n",
    "            sparse_type = config.get('sparse_type', 'none')\n",
    "            sparse_icon = f\"âœ…({sparse_type.upper()})\" if config['supports_sparse'] else \"âŒ\"\n",
    "            print(f\"  {model_key}: {config['description']}\")\n",
    "            print(f\"    â””â”€ Boyut: {config['embedding_dim']}, Sparse: {sparse_icon}, Max Token: {config['max_seq_length']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ Ã–neri:\")\n",
    "    print(\"  â€¢ TÃ¼rkÃ§e aÄŸÄ±rlÄ±klÄ±: turkish-bert (TF-IDF sparse)\")\n",
    "    print(\"  â€¢ En iyi performans: bge-m3 (Native sparse)\")\n",
    "    print(\"  â€¢ HÄ±zlÄ± TÃ¼rkÃ§e: distilbert-turkish (TF-IDF sparse)\")\n",
    "    print(\"  â€¢ Ã‡ok dilli: multilingual-e5 (TF-IDF sparse)\")\n",
    "    \n",
    "    print(f\"\\nğŸ” Sparse Embedding TÃ¼rleri:\")\n",
    "    print(\"  â€¢ NATIVE: Model'in kendi sparse sistemi (sadece BGE-M3)\")\n",
    "    print(\"  â€¢ TFIDF: TF-IDF tabanlÄ± sparse embedding (tÃ¼m diÄŸer modeller)\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nModel seÃ§in (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            selected_config = MODEL_CONFIGS[choice]\n",
    "            sparse_method = selected_config.get('sparse_type', 'none')\n",
    "            print(f\"âœ… SeÃ§ilen model: {selected_config['model_name']}\")\n",
    "            print(f\"ğŸ“Š Sparse method: {sparse_method.upper()}\")\n",
    "            return choice\n",
    "        print(\"âŒ GeÃ§ersiz model! Mevcut:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"{key}: {config['description']}\")\n",
    "        print(f\"  â””â”€ Boyut: {config['embedding_dim']}, Sparse: {config['supports_sparse']}\")\n",
    "    \n",
    "    print(\"\\nMevcut modeller:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nModel seÃ§in (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            print(f\"âœ… SeÃ§ilen model: {MODEL_CONFIGS[choice]['model_name']}\")\n",
    "            return choice\n",
    "        print(\"âŒ GeÃ§ersiz model! Tekrar deneyin.\")\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ›ï¸ YARGITAY DÄ°NAMÄ°K MODEL SÄ°STEMÄ°\")\n",
    "    \n",
    "    # Model seÃ§imi\n",
    "    selected_model = select_model()\n",
    "    selected_config = MODEL_CONFIGS[selected_model]\n",
    "    \n",
    "    # Config oluÅŸtur\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=f\"{selected_model}_chunks\",  # Model adÄ±na gÃ¶re koleksiyon\n",
    "        EMBEDDING_DIM=min(512, selected_config[\"embedding_dim\"]),  # Boyutu ayarla\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pipeline = YargitayPipeline(config, selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pipeline oluÅŸturma hatasÄ±: {e}\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"ğŸ›ï¸ YARGITAY SEMANTÄ°K SÄ°STEM - Model: {selected_config['model_name']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) Ä°nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini gÃ¶ster\")\n",
    "        print(\"4) Model deÄŸiÅŸtir\")\n",
    "        print(\"5) Ã‡Ä±kÄ±ÅŸ\")\n",
    "        choice = input(\"SeÃ§iminiz (1-5): \").strip()\n",
    "        \n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"âœ… TamamlandÄ±\" if ok else \"âŒ Hata Ã§Ä±ktÄ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            # Model deÄŸiÅŸtir ve sistemi yeniden baÅŸlat\n",
    "            selected_model = select_model()\n",
    "            selected_config = MODEL_CONFIGS[selected_model]\n",
    "            config.COLLECTION_NAME = f\"{selected_model}_chunks\"\n",
    "            config.EMBEDDING_DIM = min(512, selected_config[\"embedding_dim\"])\n",
    "            try:\n",
    "                pipeline = YargitayPipeline(config, selected_model)\n",
    "                print(\"âœ… Model deÄŸiÅŸtirildi!\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Model deÄŸiÅŸtirme hatasÄ±: {e}\")\n",
    "        elif choice==\"5\":\n",
    "            print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "  ğŸ”¹ dense_vec (Dense): boyut 512\n",
      "     Ã¶rnek -> [0.051721986, -0.03350699, -0.02643429, 0.05381775, -0.044412125]\n",
      "  ğŸ”¸ sparse_vec (Sparse): 101 non-zero eleman\n",
      "     Ã¶rnek -> indices[:5]: [209, 210, 218, 239, 303], values[:5]: [0.06741171, 0.06388491, 0.08513636, 0.121806055, 0.071728185]\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# Dynamic Model Selection + SemChunk + Qdrant Entegrasyon\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Model yapÄ±landÄ±rmalarÄ±\n",
    "MODEL_CONFIGS = {\n",
    "    \"bge-m3\": {\n",
    "        \"model_name\": \"BAAI/bge-m3\",\n",
    "        \"model_type\": \"bge\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 8192,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE-M3 - Ã‡ok dilli, dense+sparse embedding destekli\"\n",
    "    },\n",
    "    \"bge-large\": {\n",
    "        \"model_name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"BGE Large - Sadece dense embedding\"\n",
    "    },\n",
    "    \"multilingual-e5\": {\n",
    "        \"model_name\": \"intfloat/multilingual-e5-large\",\n",
    "        \"model_type\": \"sentence_transformer\", \n",
    "        \"embedding_dim\": 1024,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"E5 Multilingual Large - Ã‡ok dilli dense embedding\"\n",
    "    },\n",
    "    \"turkish-bert\": {\n",
    "        \"model_name\": \"dbmdz/bert-base-turkish-cased\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"supports_sparse\": True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"Turkish BERT - TÃ¼rkÃ§e Ã¶zelleÅŸtirilmiÅŸ\"\n",
    "    },\n",
    "    \"all-mpnet\": {\n",
    "        \"model_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"model_type\": \"sentence_transformer\",\n",
    "        \"embedding_dim\": 768,\n",
    "        \"max_seq_length\": 384,\n",
    "        \"supports_sparse\":True,\n",
    "        \"supports_dense\": True,\n",
    "        \"description\": \"All-MiniLM - Genel amaÃ§lÄ±, hÄ±zlÄ±\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    MODEL_TYPE: str = \"bge\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"dynamic_model_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "    SUPPORTS_SPARSE: bool = True\n",
    "    SUPPORTS_DENSE: bool = True\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Dinamik model yÃ¶netimi iÃ§in sÄ±nÄ±f\"\"\"\n",
    "    \n",
    "    def __init__(self, model_key: str, config: Config):\n",
    "        self.model_key = model_key\n",
    "        self.config = config\n",
    "        self.model_config = MODEL_CONFIGS.get(model_key)\n",
    "        \n",
    "        if not self.model_config:\n",
    "            raise ValueError(f\"Desteklenmeyen model: {model_key}\")\n",
    "        \n",
    "        self.model = None\n",
    "        self.vectorizer = None  # Sparse embedding iÃ§in\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"SeÃ§ilen modeli yÃ¼kle\"\"\"\n",
    "        try:\n",
    "            model_name = self.model_config[\"model_name\"]\n",
    "            model_type = self.model_config[\"model_type\"]\n",
    "            \n",
    "            print(f\"ğŸ”® Model yÃ¼kleniyor: {model_name} (tip: {model_type})\")\n",
    "            \n",
    "            if model_type == \"bge\":\n",
    "                self.model = BGEM3FlagModel(\n",
    "                    model_name, \n",
    "                    use_fp16=self.config.USE_FP16, \n",
    "                    device=self.config.DEVICE\n",
    "                )\n",
    "            elif model_type == \"sentence_transformer\":\n",
    "                self.model = SentenceTransformer(model_name, device=self.config.DEVICE)\n",
    "                # Sparse embedding iÃ§in TF-IDF hazÄ±rla\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "            else:\n",
    "                raise ValueError(f\"Desteklenmeyen model tipi: {model_type}\")\n",
    "                \n",
    "            print(f\"âœ… Model yÃ¼klendi: {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Model yÃ¼kleme hatasÄ±: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def encode_texts(self, texts: List[str]) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Metinleri encode et - model tipine gÃ¶re\"\"\"\n",
    "        dense_embeddings = []\n",
    "        sparse_embeddings = []\n",
    "        \n",
    "        try:\n",
    "            if self.model_config[\"model_type\"] == \"bge\" and hasattr(self.model, 'encode'):\n",
    "                # BGE-M3 iÃ§in\n",
    "                if self.model_config[\"supports_sparse\"]:\n",
    "                    result = self.model.encode(\n",
    "                        texts, \n",
    "                        return_dense=True, \n",
    "                        return_sparse=True\n",
    "                    )\n",
    "                    dense_embeddings = result.get(\"dense_vecs\", [])\n",
    "                    sparse_raw = result.get(\"sparse_vecs\", [])\n",
    "                    sparse_embeddings = [\n",
    "                        {\"indices\": s.get(\"indices\", []), \"values\": s.get(\"values\", [])} \n",
    "                        for s in sparse_raw\n",
    "                    ] if sparse_raw else [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                else:\n",
    "                    dense_embeddings = self.model.encode(texts, return_dense=True)\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "                    \n",
    "            else:\n",
    "                # Sentence Transformer iÃ§in\n",
    "                dense_embeddings = self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "                \n",
    "                # Sparse embedding TF-IDF ile\n",
    "                if self.vectorizer:\n",
    "                    X_sparse = self.vectorizer.fit_transform(texts)\n",
    "                    sparse_embeddings = []\n",
    "                    for i in range(X_sparse.shape[0]):\n",
    "                        row = X_sparse[i].tocoo()\n",
    "                        sparse_embeddings.append({\n",
    "                            \"indices\": row.col.tolist(),\n",
    "                            \"values\": row.data.tolist()\n",
    "                        })\n",
    "                else:\n",
    "                    sparse_embeddings = [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            \n",
    "            # Embedding boyutunu ayarla\n",
    "            target_dim = self.config.EMBEDDING_DIM\n",
    "            dense_clean = []\n",
    "            for vec in dense_embeddings:\n",
    "                if vec is None:\n",
    "                    dense_clean.append([0.0] * target_dim)\n",
    "                elif len(vec) < target_dim:\n",
    "                    dense_clean.append(vec + [0.0] * (target_dim - len(vec)))\n",
    "                else:\n",
    "                    dense_clean.append(vec[:target_dim])\n",
    "            \n",
    "            return dense_clean, sparse_embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Encoding hatasÄ±: {e}\")\n",
    "            # Fallback\n",
    "            return (\n",
    "                [[0.0] * self.config.EMBEDDING_DIM for _ in texts],\n",
    "                [{\"indices\": [], \"values\": []} for _ in texts]\n",
    "            )\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Model bilgilerini dÃ¶ndÃ¼r\"\"\"\n",
    "        info = self.model_config.copy()\n",
    "        info[\"loaded\"] = self.model is not None\n",
    "        info[\"current_embedding_dim\"] = self.config.EMBEDDING_DIM\n",
    "        return info\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.config = config\n",
    "        self.model_manager = ModelManager(model_key, config)\n",
    "        \n",
    "        # Model bilgilerini config'e aktar\n",
    "        model_config = self.model_manager.model_config\n",
    "        self.config.BGE_MODEL_NAME = model_config[\"model_name\"]\n",
    "        self.config.MODEL_TYPE = model_config[\"model_type\"]\n",
    "        self.config.SUPPORTS_SPARSE = model_config[\"supports_sparse\"]\n",
    "        self.config.SUPPORTS_DENSE = model_config[\"supports_dense\"]\n",
    "        \n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Model yÃ¼kle\n",
    "        if not self.model_manager.load_model():\n",
    "            raise RuntimeError(\"Model yÃ¼klenemedi!\")\n",
    "        \n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"âœ… HazÄ±r - Model: {model_config['model_name']} | Cihaz: {device_name}\")\n",
    "\n",
    "    def test_model_connection(self):\n",
    "        \"\"\"Model baÄŸlantÄ±sÄ±nÄ± test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"YargÄ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararÄ±\"]\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts(test_text)\n",
    "            \n",
    "            print(f\"âœ… Dense embedding boyutu: {len(dense_emb[0])}\")\n",
    "            print(f\"ğŸ” Sparse embedding mevcut: {len(sparse_emb[0]['indices']) > 0}\")\n",
    "            return len(dense_emb[0])\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Model baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ğŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense vector config\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM, \n",
    "                        distance=models.Distance.COSINE\n",
    "                    ),\n",
    "                }\n",
    "                \n",
    "                # Sparse config (eÄŸer destekleniyorsa)\n",
    "                sparse_config = {}\n",
    "                if self.config.SUPPORTS_SPARSE:\n",
    "                    sparse_config = {\n",
    "                        \"sparse_vec\": models.SparseVectorParams(\n",
    "                            index=models.SparseIndexParams(on_disk=False)\n",
    "                        )\n",
    "                    }\n",
    "                \n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config=sparse_config if sparse_config else None\n",
    "                )\n",
    "                \n",
    "                support_info = \"Dense+Sparse\" if sparse_config else \"Dense only\"\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name} ({support_info})\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunking hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = None):\n",
    "        \"\"\"Dinamik model ile embedding oluÅŸtur\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"ğŸ”® {total} metin iÅŸleniyor (model: {self.config.BGE_MODEL_NAME})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                dense, sparse = self.model_manager.encode_texts(batch_texts)\n",
    "                all_embeddings_dense.extend(dense)\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "                \n",
    "                print(f\"  ğŸ“Š Batch iÅŸlendi: {i + len(batch_texts)}/{total}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Embedding hatasÄ± (batch {i//batch_size+1}): {e}\")\n",
    "                # Fallback\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"ğŸ“„ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"ğŸ“Š {len(df)} satÄ±r yÃ¼klendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"âŒ Ana metin sÃ¼tunu bulunamadÄ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "                'model_used': self.config.BGE_MODEL_NAME  # Hangi model kullanÄ±ldÄ±ÄŸÄ±nÄ± kaydet\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ğŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"ğŸš€ {len(chunks)} chunk Qdrant'a yÃ¼kleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings(texts)\n",
    "\n",
    "        points = []\n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vector_dict = {\"dense_vec\": d}\n",
    "            \n",
    "            # Sparse vector sadece destekleniyorsa ekle\n",
    "            if self.config.SUPPORTS_SPARSE and s[\"indices\"]:\n",
    "                vector_dict[\"sparse_vec\"] = SparseVector(\n",
    "                    indices=s[\"indices\"],\n",
    "                    values=s[\"values\"]\n",
    "                )\n",
    "            \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vector_dict,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(\"ğŸ‰ YÃ¼kleme tamamlandÄ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Dense semantic search\"\"\"\n",
    "        try:\n",
    "            dense_emb, _ = self.model_manager.encode_texts([query])\n",
    "            query_vector = dense_emb[0]\n",
    "            \n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=NamedVector(name=\"dense_vec\", vector=query_vector),\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"ğŸ“Š {len(results)} sonuÃ§ bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Semantic search hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Hybrid search (sadece destekleniyorsa)\"\"\"\n",
    "        if not self.config.SUPPORTS_SPARSE:\n",
    "            print(\"âš ï¸ Bu model sparse embedding desteklemiyor, dense search yapÄ±lÄ±yor...\")\n",
    "            return self.search_semantic(query, limit, score_threshold)\n",
    "        \n",
    "        try:\n",
    "            dense_emb, sparse_emb = self.model_manager.encode_texts([query])\n",
    "            \n",
    "            requests = [\n",
    "                SearchRequest(\n",
    "                    vector=NamedVector(name=\"dense_vec\", vector=dense_emb[0]),\n",
    "                    limit=limit,\n",
    "                    with_payload=True,\n",
    "                    score_threshold=score_threshold\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Sparse varsa ekle\n",
    "            if sparse_emb[0][\"indices\"]:\n",
    "                requests.append(\n",
    "                    SearchRequest(\n",
    "                        vector=NamedSparseVector(\n",
    "                            name=\"sparse_vec\",\n",
    "                            vector=SparseVector(\n",
    "                                indices=sparse_emb[0][\"indices\"],\n",
    "                                values=sparse_emb[0][\"values\"]\n",
    "                            )\n",
    "                        ),\n",
    "                        limit=limit,\n",
    "                        score_threshold=score_threshold\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            results = []\n",
    "            for request_result in qr:\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"ğŸ“Š {len(results)} sonuÃ§ bulundu (Hybrid)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Hybrid search hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            model_info = self.model_manager.get_model_info()\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"model_info\": model_info,\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM,\n",
    "                \"supports_sparse\": self.config.SUPPORTS_SPARSE\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config, model_key: str = \"bge-m3\"):\n",
    "        self.processor = YargitaySemanticProcessor(config, model_key)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ğŸš€ Full pipeline baÅŸlÄ±yor\")\n",
    "        emb_dim = self.processor.test_model_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"âŒ Chunk bulunamadÄ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nğŸ“Š Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nğŸ” Ä°nteraktif arama baÅŸlatÄ±ldÄ±\")\n",
    "        model_info = self.processor.model_manager.get_model_info()\n",
    "        print(f\"ğŸ“± Aktif model: {model_info['model_name']}\")\n",
    "        print(f\"ğŸ”§ Ã–zellikler: Denseâœ…, Sparse{'âœ…' if model_info['supports_sparse'] else 'âŒ'}\")\n",
    "        \n",
    "        while True:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(\"ğŸ” ARAMA SEÃ‡ENEKLERÄ°\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(\"1) Dense arama (Semantic)\")\n",
    "            if model_info['supports_sparse']:\n",
    "                print(\"2) Hybrid arama (Dense + Sparse)\")\n",
    "                print(\"3) KarÅŸÄ±laÅŸtÄ±rmalÄ± arama (Her iki yÃ¶ntem)\")\n",
    "            else:\n",
    "                print(\"2) âŒ Hybrid arama (Bu model desteklemiyor)\")\n",
    "                print(\"3) âŒ KarÅŸÄ±laÅŸtÄ±rma (Sparse desteklenmiyor)\")\n",
    "            print(\"4) Ana menÃ¼\")\n",
    "            \n",
    "            ch = input(\"SeÃ§iminiz (1-4): \").strip()\n",
    "            if ch==\"4\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\",\"3\"}:\n",
    "                print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "                continue\n",
    "                \n",
    "            q = input(\"ğŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                limit = int(input(\"KaÃ§ sonuÃ§? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                print(\"\\nğŸ¯ Dense Semantic Search...\")\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(results, \"Dense\")\n",
    "                \n",
    "            elif ch==\"2\" and model_info['supports_sparse']:\n",
    "                print(\"\\nğŸ”€ Hybrid Search...\")\n",
    "                results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(results, \"Hybrid\")\n",
    "                \n",
    "            elif ch==\"3\" and model_info['supports_sparse']:\n",
    "                print(\"\\nğŸ“Š KarÅŸÄ±laÅŸtÄ±rmalÄ± Arama...\")\n",
    "                print(\"ğŸ¯ Dense sonuÃ§lar:\")\n",
    "                dense_results = self.processor.search_semantic(q, limit=limit)\n",
    "                self._display_results(dense_results, \"Dense\", show_comparison=True)\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"ğŸ”€ Hybrid sonuÃ§lar:\")\n",
    "                hybrid_results = self.processor.search_hybrid(q, limit=limit)\n",
    "                self._display_results(hybrid_results, \"Hybrid\", show_comparison=True)\n",
    "                \n",
    "            else:\n",
    "                print(\"âš ï¸ Bu Ã¶zellik seÃ§ilen model tarafÄ±ndan desteklenmiyor.\")\n",
    "\n",
    "    def _display_results(self, results: List[Dict], search_type: str, show_comparison: bool = False):\n",
    "        \"\"\"SonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼le\"\"\"\n",
    "        if not results:\n",
    "            print(\"âŒ SonuÃ§ bulunamadÄ±\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nğŸ“‹ {len(results)} {search_type} sonuÃ§:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            p = r.get(\"payload\") or {}\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            \n",
    "            # Skor rengine gÃ¶re emoji\n",
    "            if score > 0.8:\n",
    "                score_icon = \"ğŸŸ¢\"\n",
    "            elif score > 0.6:\n",
    "                score_icon = \"ğŸŸ¡\"\n",
    "            else:\n",
    "                score_icon = \"ğŸ”´\"\n",
    "                \n",
    "            print(f\"\\n{i}. {score_icon} Skor: {score:.4f}\")\n",
    "            print(f\"   ğŸ“„ Model: {p.get('model_used','N/A')}\")\n",
    "            print(f\"   ğŸ›ï¸ Daire: {p.get('daire','N/A')} | ğŸ“… Tarih: {p.get('tarih','N/A')}\")\n",
    "            print(f\"   ğŸ“‹ Esas: {p.get('esas_no','N/A')} | ğŸ”¢ Karar: {p.get('karar_no','N/A')}\")\n",
    "            \n",
    "            text = p.get('text', '')\n",
    "            if len(text) > 200:\n",
    "                text_preview = text[:200] + \"...\"\n",
    "            else:\n",
    "                text_preview = text\n",
    "            print(f\"   ğŸ“ Metin: {text_preview}\")\n",
    "            \n",
    "            if show_comparison:\n",
    "                print(f\"   ğŸ·ï¸ Tip: {search_type}\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "\n",
    "\n",
    "def check_sparse_vectors(collection_name, point_id):\n",
    "    res = client.retrieve(\n",
    "        collection_name=collection_name,\n",
    "        ids=[point_id],   # UUID veya int doÄŸrudan verilebilir\n",
    "        with_vectors=True\n",
    "    )\n",
    "\n",
    "    if not res:\n",
    "        print(\"âŒ Veri bulunamadÄ±\")\n",
    "        return\n",
    "\n",
    "    point = res[0]\n",
    "    vectors = point.vector\n",
    "\n",
    "    for k, v in vectors.items():\n",
    "        if hasattr(v, \"indices\") and hasattr(v, \"values\"):  \n",
    "            # SparseVector\n",
    "            print(f\"  ğŸ”¸ {k} (Sparse): {len(v.indices)} non-zero eleman\")\n",
    "            print(f\"     Ã¶rnek -> indices[:5]: {v.indices[:5]}, values[:5]: {v.values[:5]}\")\n",
    "        else:\n",
    "            # DenseVector\n",
    "            dense_data = v.vector if hasattr(v, \"vector\") else v\n",
    "            print(f\"  ğŸ”¹ {k} (Dense): boyut {len(dense_data)}\")\n",
    "            print(f\"     Ã¶rnek -> {dense_data[:5]}\")\n",
    "\n",
    "\n",
    "def select_model() -> str:\n",
    "    \"\"\"KullanÄ±cÄ±dan model seÃ§imi al\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¤– MODEL SEÃ‡Ä°MÄ°\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for key, config in MODEL_CONFIGS.items():\n",
    "        print(f\"{key}: {config['description']}\")\n",
    "        print(f\"  â””â”€ Boyut: {config['embedding_dim']}, Sparse: {config['supports_sparse']}\")\n",
    "    \n",
    "    print(\"\\nMevcut modeller:\", \", \".join(MODEL_CONFIGS.keys()))\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nModel seÃ§in (default: bge-m3): \").strip().lower() or \"bge-m3\"\n",
    "        if choice in MODEL_CONFIGS:\n",
    "            print(f\"âœ… SeÃ§ilen model: {MODEL_CONFIGS[choice]['model_name']}\")\n",
    "            return choice\n",
    "        print(\"âŒ GeÃ§ersiz model! Tekrar deneyin.\")\n",
    "\n",
    "# def main():\n",
    "#     check_sparse_vectors(\"all-mpnet_chunks\", \"01967bd6-5735-43b6-8e81-c0508c12f257\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ›ï¸ YARGITAY DÄ°NAMÄ°K MODEL SÄ°STEMÄ°\")\n",
    "    \n",
    "    # Model seÃ§imi\n",
    "    selected_model = select_model()\n",
    "    selected_config = MODEL_CONFIGS[selected_model]\n",
    "    \n",
    "    # Config oluÅŸtur\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=f\"{selected_model}_chunks\",  # Model adÄ±na gÃ¶re koleksiyon\n",
    "        EMBEDDING_DIM=min(512, selected_config[\"embedding_dim\"]),  # Boyutu ayarla\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        pipeline = YargitayPipeline(config, selected_model)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pipeline oluÅŸturma hatasÄ±: {e}\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"ğŸ›ï¸ YARGITAY SEMANTÄ°K SÄ°STEM - Model: {selected_config['model_name']}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) Ä°nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini gÃ¶ster\")\n",
    "        print(\"4) Model deÄŸiÅŸtir\")\n",
    "        print(\"5) Ã‡Ä±kÄ±ÅŸ\")\n",
    "        choice = input(\"SeÃ§iminiz (1-5): \").strip()\n",
    "        \n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"âœ… TamamlandÄ±\" if ok else \"âŒ Hata Ã§Ä±ktÄ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            # Model deÄŸiÅŸtir ve sistemi yeniden baÅŸlat\n",
    "            selected_model = select_model()\n",
    "            selected_config = MODEL_CONFIGS[selected_model]\n",
    "            config.COLLECTION_NAME = f\"{selected_model}_chunks\"\n",
    "            config.EMBEDDING_DIM = min(512, selected_config[\"embedding_dim\"])\n",
    "            try:\n",
    "                pipeline = YargitayPipeline(config, selected_model)\n",
    "                print(\"âœ… Model deÄŸiÅŸtirildi!\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Model deÄŸiÅŸtirme hatasÄ±: {e}\")\n",
    "        elif choice==\"5\":\n",
    "            print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
