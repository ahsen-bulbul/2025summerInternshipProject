{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cohere modeli hfden upload etmeyi deniyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "hf_token = os.getenv(\"HF_API_KEY\")  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\", \n",
    "    token=hf_token\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Cohere/Cohere-embed-multilingual-v3.0\",\n",
    "    token=hf_token,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "# Ã–rnek metinler\n",
    "texts = [\"Bu bir test cÃ¼mlesidir.\", \"Mahkeme kararÄ±nÄ± verdi.\"]\n",
    "\n",
    "# Tokenize et\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Modeli kullanarak embedding al\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # Burada last_hidden_state'den mean pooling yapÄ±yoruz\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(\"Embedding boyutu:\", embeddings.shape)\n",
    "print(\"Ä°lk embedding Ã¶rneÄŸi:\", embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "# Hugging Face API key ortam deÄŸiÅŸkeninden Ã§ek\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli token ile yÃ¼kle\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token  # API key burada\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlik hesapla\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasÄ±nÄ± yÃ¼kle\n",
    "load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\")\n",
    "\n",
    "# Hugging Face API key\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# Modeli yÃ¼kle (gated model ise token ver)\n",
    "model = SentenceTransformer(\n",
    "    \"google/embeddinggemma-300m\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# Embeddingleri al\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Benzerlikleri cosine similarity ile hesapla\n",
    "similarities = cosine_similarity(embeddings, embeddings)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Benzerlik matrisi:\\n\", similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "vec = model.encode([\"test\"])\n",
    "print(vec.shape)  # (1, 784) olmalÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk ederken cpu mu gpu mu kullanÄ±lÄ±yor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA kullanÄ±labilir mi? \", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU adÄ±:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU kullanÄ±lÄ±yor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarÄ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarÄ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarÄ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 768  # embeddinggemma-300m embedding boyutu\n",
    "\n",
    "    # Dosya ayarlarÄ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 256\n",
    "\n",
    "    # Cihaz ayarÄ±\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli (GPU/CPU seÃ§imi burada yapÄ±lÄ±yor)\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        print(f\"âœ… Model ÅŸu cihazda Ã§alÄ±ÅŸÄ±yor: {config.DEVICE}\")\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"âœ… SemChunk hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… Google EmbeddingGemma modeli hazÄ±r\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bir batch in embeddingleri oluÅŸtuktan sonra reduction yapÄ±yo yani parÃ§a parÃ§a yapmÄ±ÅŸ oluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            start_embed = time.time()\n",
    "\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "\n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding sÃ¼resi: {end_embed - start_embed:.2f} saniye\")\n",
    "\n",
    "            pca=PCA(n_components=512)\n",
    "            batch_embeddings_reduced = pca.fit_transform(batch_embeddings) #default 768 olan embedding boyutunu 512ye dÃ¼ÅŸÃ¼rÃ¼yoruz -> PCA\n",
    "            all_embeddings.extend(batch_embeddings_reduced.tolist())\n",
    "\n",
    "            print(f\"  ğŸ”¹ Embedding oluÅŸturuldu: {i + len(batch_texts)}/{total}\")\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Modeli yÃ¼kle\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "# KÃ¼Ã§Ã¼k dataset\n",
    "train_examples = [\n",
    "    InputExample(texts=[\"araba\", \"otomobil\"], label=1.0),\n",
    "    InputExample(texts=[\"kedi\", \"kÃ¶pek\"], label=1.0),\n",
    "    InputExample(texts=[\"masa\", \"sandalye\"], label=1.0),\n",
    "    InputExample(texts=[\"araba\", \"uÃ§ak\"], label=0.0),\n",
    "    InputExample(texts=[\"kedi\", \"araba\"], label=0.0),\n",
    "    InputExample(texts=[\"masa\", \"araba\"], label=0.0),\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n",
    "\n",
    "# Loss fonksiyonu\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# EÄŸitim\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10\n",
    ")\n",
    "\n",
    "# Test\n",
    "emb1 = model.encode(\"araba\")\n",
    "emb2 = model.encode(\"otomobil\")\n",
    "emb3 = model.encode(\"kedi\")\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "print(\"Benzerlik (araba - otomobil):\", cos_sim(emb1, emb2).item())\n",
    "print(\"Benzerlik (araba - kedi):\", cos_sim(emb1, emb3).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge 512 dimension with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ã–rnek 1024 boyutlu embedding\n",
    "embedding_1024 = torch.randn(1, 1024)  # batch size 1\n",
    "\n",
    "# Linear layer ile 1024 -> 512 boyuta dÃ¼ÅŸÃ¼rme\n",
    "reduce_dim = nn.Linear(1024, 512)\n",
    "\n",
    "# Boyut indirgeme iÅŸlemi\n",
    "embedding_512 = reduce_dim(embedding_1024)\n",
    "\n",
    "print(\"Orijinal boyut:\", embedding_1024.shape)\n",
    "print(\"Yeni boyut:\", embedding_512.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Normalize edilmiÅŸ, reducer uyumlu, query_points kullanÄ±yor)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    # t: (N, D)\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "# Embed reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim: int = 1024, output_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "# -------------------------\n",
    "# Processor\n",
    "# -------------------------\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model & reducer\n",
    "        print(f\"ğŸ”® BGE-M3 yÃ¼kleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(input_dim=1024, output_dim=self.config.EMBEDDING_DIM).to(config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"âœ… HazÄ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense dim\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"YargÄ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararÄ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0]\n",
    "            print(f\"âœ… BGE-M3 test baÅŸarÄ±lÄ± - Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"ğŸ” Sparse embedding mevcut: {'colbert_vecs' in emb_res}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ BGE-M3 baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ğŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name} (dim={self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunking hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings: List[List[float]] = []\n",
    "        total = len(texts)\n",
    "        print(f\"ğŸ”® BGE-M3 ile {total} metin iÅŸleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # 1) embed (BGE-M3) -> dense (1024)\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "                if isinstance(emb_res, dict) and 'dense_vecs' in emb_res:\n",
    "                    dense = emb_res['dense_vecs']\n",
    "                else:\n",
    "                    dense = emb_res\n",
    "\n",
    "                # 2) to tensor, device\n",
    "                if not isinstance(dense, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense.to(self.config.DEVICE)\n",
    "\n",
    "                # 3) reducer -> 512\n",
    "                with torch.no_grad():\n",
    "                    reduced = self.reducer(dense_t)\n",
    "\n",
    "                # 4) normalize L2 (important for cosine)\n",
    "                reduced = l2_normalize_tensor(reduced)\n",
    "\n",
    "                # 5) append as python lists (cpu)\n",
    "                all_embeddings.extend([v.cpu().tolist() for v in reduced])\n",
    "\n",
    "                print(f\"  ğŸ“Š Batch iÅŸlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ BGE-M3 Embedding hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # fallback zero vectors\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"ğŸ“„ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"ğŸ“Š {len(df)} satÄ±r yÃ¼klendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"âŒ Ana metin sÃ¼tunu bulunamadÄ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ğŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"ğŸš€ {len(chunks)} chunk Qdrant'a yÃ¼kleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "\n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"âŒ Embedding sayÄ±sÄ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for chunk, emb in zip(chunks, embeddings):\n",
    "            points.append(PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i+batch, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(\"ğŸ‰ YÃ¼kleme tamamlandÄ±!\")\n",
    "\n",
    "    # ---------- SEARCH (query_points + reducer + normalize) ----------\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "            print(f\"ğŸ” Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "\n",
    "            # use query_points (recommended)\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"ğŸ“Š {len(results)} sonuÃ§ bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        try:\n",
    "            # prepare reduced + normalized query vector same as above\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "            if not isinstance(q_dense, torch.Tensor):\n",
    "                q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            else:\n",
    "                q_t = q_dense.to(self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reduced_q = self.reducer(q_t)\n",
    "                reduced_q = l2_normalize_tensor(reduced_q)\n",
    "\n",
    "            query_vector = reduced_q[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr.points]\n",
    "            print(f\"ğŸ“Š {len(results)} filtreli sonuÃ§ bulundu\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Filtreli arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline + main\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ğŸš€ Full pipeline baÅŸlÄ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        # recreate collection to ensure clean 512-dim index\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"âŒ Chunk bulunamadÄ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nğŸ“Š Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nğŸ” Ä°nteraktif arama baÅŸlatÄ±ldÄ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana menÃ¼\")\n",
    "            ch = input(\"SeÃ§iminiz (1-3): \").strip()\n",
    "            if ch == \"3\":\n",
    "                break\n",
    "            if ch not in {\"1\", \"2\"}:\n",
    "                print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "                continue\n",
    "            q = input(\"ğŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if q.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"KaÃ§ sonuÃ§? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch == \"1\":\n",
    "                # try with low threshold first for debugging\n",
    "                results = self.processor.search_semantic(q, limit=limit, score_threshold=None)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (Ã¶rn: '6.HukukDairesi', boÅŸ = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit, score_threshold=None)\n",
    "\n",
    "            if not results:\n",
    "                print(\"âŒ SonuÃ§ bulunamadÄ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nğŸ“‹ {len(results)} sonuÃ§:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_normal_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K SÄ°STEM\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) Ä°nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini gÃ¶ster\")\n",
    "        print(\"4) Ã‡Ä±kÄ±ÅŸ\")\n",
    "        choice = input(\"SeÃ§iminiz (1-4): \").strip()\n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"âœ… TamamlandÄ±\" if ok else \"âŒ Hata Ã§Ä±ktÄ±\")\n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice == \"4\":\n",
    "            print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"âœ… FlagEmbedding yÃ¼klÃ¼\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ FlagEmbedding bulunamadÄ± â€” pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "âœ… SemChunk hazÄ±r (Token boyutu: 512)\n",
      "âœ… Google EmbeddingGemma modeli hazÄ±r\n",
      "âœ… Qdrant client hazÄ±r (http://localhost:6333)\n",
      "âœ… Device: cuda:0\n",
      "\n",
      "============================================================\n",
      "1. Full pipeline Ã§alÄ±ÅŸtÄ±r\n",
      "2. Arama yap\n",
      "3. Koleksiyon bilgisi\n",
      "4. ğŸ”§ Debug arama sorunu\n",
      "5. ğŸ—‘ï¸ Dimension uyumsuzluÄŸunu dÃ¼zelt\n",
      "6. Ã‡Ä±kÄ±ÅŸ\n",
      "============================================================\n",
      "ğŸ” Ä°nteraktif Arama BaÅŸlatÄ±ldÄ±\n",
      "ğŸ“Š Koleksiyon Durumu: {\n",
      "  \"collection_name\": \"google_normal_chunks\",\n",
      "  \"points_count\": 80345,\n",
      "  \"vectors_count\": null,\n",
      "  \"status\": \"green\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "ğŸ” Query vector boyutu: torch.Size([1, 512])\n",
      "ğŸ” Query vector norm: 1.0000\n",
      "ğŸ” Final query vector boyutu: 512 (hedef: 512)\n",
      "ğŸ” Ham arama sonucu sayÄ±sÄ±: 0\n",
      "âš ï¸ Threshold ile sonuÃ§ yok, threshold'suz deneniyor...\n",
      "ğŸ” Threshold'suz sonuÃ§ sayÄ±sÄ±: 5\n",
      "ğŸ” En yÃ¼ksek score: 0.1165\n",
      "ğŸ” En dÃ¼ÅŸÃ¼k score: 0.1068\n",
      "\n",
      "ğŸ“‹ 5 SonuÃ§ Bulundu:\n",
      "\n",
      "1. ğŸ“Š Score: 0.1165\n",
      "   ğŸ“‹ Esas No: 2024/1564 E.\n",
      "   ğŸ“‹ Karar No: 2024/2633 K.\n",
      "   ğŸ“‹ Daire: 1.HukukDairesisi\n",
      "   ğŸ“‹ Tarih: 08.04.2008,20.09.2010,18.03.2014,27.11.1937,01.04.2024\n",
      "   ğŸ“„ Metin: Tapu kaydÄ±nÄ±n uygulanmasÄ±nda tÃ¼m komÅŸu parsel tutanak ve dayanaklarÄ±ndan yararlanÄ±larak tapu kayÄ±t uygulamasÄ± denetlenmemiÅŸ, tapu kaydÄ±nÄ±n miktar fazlasÄ± bulunup bulunmadÄ±ÄŸÄ± incelenmemiÅŸ, keÅŸif yapÄ±lm...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "2. ğŸ“Š Score: 0.1129\n",
      "   ğŸ“‹ Esas No: 2021/17062 E.\n",
      "   ğŸ“‹ Karar No: 2024/730 K.\n",
      "   ğŸ“‹ Daire: 4.HukukDairesisi\n",
      "   ğŸ“‹ Tarih: 24.12.2018,02.05.2018,02.05.2018,23.01.2008,22.01.2024\n",
      "   ğŸ“„ Metin: Ä°cra MÃ¼dÃ¼rlÃ¼ÄŸÃ¼'nÃ¼n 2008/465 (2017/3329 E.) sayÄ±lÄ± icra dosyasÄ±ndan takibe baÅŸlandÄ±ÄŸÄ±nÄ± ve aralarÄ±nda tatsÄ±zlÄ±klar yaÅŸandÄ±ÄŸÄ±nÄ±, davaya konu edilen taÅŸÄ±nmazlarÄ±n mÃ¼ÅŸterek ana-babadan adÄ± geÃ§en 4 kardeÅŸe...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "3. ğŸ“Š Score: 0.1105\n",
      "   ğŸ“‹ Esas No: 2023/159 E.\n",
      "   ğŸ“‹ Karar No: 2024/38 K.\n",
      "   ğŸ“‹ Daire: HukukGenelKurulu\n",
      "   ğŸ“‹ Tarih: 06.10.2021,24.07.2017,17.01.2019,04.03.2021,15.11.2019,23.10.2020,05.12.2018,24.05.2017,30.03.2021,05.12.2018,13.12.2017,13.12.2017,31.01.2024\n",
      "   ğŸ“„ Metin: Tapu kaydÄ±nda aile konutu ÅŸerhi bulunmasa dahi aile konutuna iliÅŸkin olarak; eÅŸlerden biri diÄŸer eÅŸin aÃ§Ä±k rÄ±zasÄ± bulunmadÄ±kÃ§a aile konutuyla ilgili kira sÃ¶zleÅŸmesini feshedemeyecek, aile konutunu dev...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "4. ğŸ“Š Score: 0.1071\n",
      "   ğŸ“‹ Esas No: 2021/9081 E.\n",
      "   ğŸ“‹ Karar No: 2024/328 K.\n",
      "   ğŸ“‹ Daire: 3.CezaDairesisi\n",
      "   ğŸ“‹ Tarih: 06.08.2016,04.08.2016,10.08.2016,25.08.2016,24.10.2019,24.04.2017,08.04.2008,12.05.2015,26.10.2015,29.09.2021,04.10.2021,26.09.2017,24.04.2017,04.06.2020,16.12.1992,29.09.2016,10.01.2024\n",
      "   ğŸ“„ Metin: Verilen bilginin Ã¶nemi cezanÄ±n belirlenmesinde dikkate alÄ±nmalÄ±dÄ±r (YargÄ±tay (KapatÄ±lan) 16. Ceza Dairesinin 12.05.2015 tarih, 2015/1426 Esas 2015/1292 Karar, 26.10.2015 tarih, 2015/1565 Esas 3464 say...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "5. ğŸ“Š Score: 0.1068\n",
      "   ğŸ“‹ Esas No: 2023/567 E.\n",
      "   ğŸ“‹ Karar No: 2024/42 K.\n",
      "   ğŸ“‹ Daire: HukukGenelKurulu\n",
      "   ğŸ“‹ Tarih: 20.06.2022,01.02.2009,01.10.2018,05.12.2019,24.06.2020,14.01.2021,11.11.2013,01.10.2018,11.11.2013,09.01.2018,30.11.2015,10.01.2018,12.10.2015,01.01.2016,10.01.2018,01.10.2018,29.03.2022,24.03.2012,01.03.2012,28.02.2014,01.03.2014,28.02.2017,20.03.2014,11.11.2013,01.03.2014,28.02.2017,19.10.2021,17.11.2020,16.03.2022,24.03.2012,01.03.2012,28.02.2014,20.03.2014,01.03.2014,28.02.2017,01.03.2017,28.02.2019,11.11.2013,14.01.2021,14.01.2021,14.01.2021,03.11.2021,11.11.2013,01.04.2016,09.01.2018,29.03.2022,31.01.2024,01.03.2014,01.10.2018,10.01.2018,01.10.2018,11.11.2013,01.03.2014,28.02.2017,28.02.2022,07.05.2014,11.11.2013,03.11.2021,11.11.2013,01.04.2016,09.01.2018\n",
      "   ğŸ“„ Metin: Bundan baÅŸka asÄ±l iÅŸverenin iÅŸÃ§ilerinin alt iÅŸveren tarafÄ±ndan iÅŸe alÄ±narak Ã§alÄ±ÅŸtÄ±rÄ±lmaya devam ettirilmesi suretiyle haklarÄ±nÄ±n kÄ±sÄ±tlanmasÄ± veya daha Ã¶nce asÄ±l iÅŸveren tarafÄ±ndan o iÅŸ yerinde Ã§alÄ±ÅŸ...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1560741/860841019.py:278: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.qdrant_client.search(\n",
      "/tmp/ipykernel_1560741/860841019.py:290: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ” Query vector boyutu: torch.Size([1, 512])\n",
      "ğŸ” Query vector norm: 1.0000\n",
      "ğŸ” Final query vector boyutu: 512 (hedef: 512)\n",
      "ğŸ” Ham arama sonucu sayÄ±sÄ±: 0\n",
      "âš ï¸ Threshold ile sonuÃ§ yok, threshold'suz deneniyor...\n",
      "ğŸ” Threshold'suz sonuÃ§ sayÄ±sÄ±: 5\n",
      "ğŸ” En yÃ¼ksek score: 0.1165\n",
      "ğŸ” En dÃ¼ÅŸÃ¼k score: 0.1068\n",
      "\n",
      "ğŸ“‹ 5 SonuÃ§ Bulundu:\n",
      "\n",
      "1. ğŸ“Š Score: 0.1165\n",
      "   ğŸ“‹ Esas No: 2024/1564 E.\n",
      "   ğŸ“‹ Karar No: 2024/2633 K.\n",
      "   ğŸ“‹ Daire: 1.HukukDairesisi\n",
      "   ğŸ“‹ Tarih: 08.04.2008,20.09.2010,18.03.2014,27.11.1937,01.04.2024\n",
      "   ğŸ“„ Metin: Tapu kaydÄ±nÄ±n uygulanmasÄ±nda tÃ¼m komÅŸu parsel tutanak ve dayanaklarÄ±ndan yararlanÄ±larak tapu kayÄ±t uygulamasÄ± denetlenmemiÅŸ, tapu kaydÄ±nÄ±n miktar fazlasÄ± bulunup bulunmadÄ±ÄŸÄ± incelenmemiÅŸ, keÅŸif yapÄ±lm...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "2. ğŸ“Š Score: 0.1129\n",
      "   ğŸ“‹ Esas No: 2021/17062 E.\n",
      "   ğŸ“‹ Karar No: 2024/730 K.\n",
      "   ğŸ“‹ Daire: 4.HukukDairesisi\n",
      "   ğŸ“‹ Tarih: 24.12.2018,02.05.2018,02.05.2018,23.01.2008,22.01.2024\n",
      "   ğŸ“„ Metin: Ä°cra MÃ¼dÃ¼rlÃ¼ÄŸÃ¼'nÃ¼n 2008/465 (2017/3329 E.) sayÄ±lÄ± icra dosyasÄ±ndan takibe baÅŸlandÄ±ÄŸÄ±nÄ± ve aralarÄ±nda tatsÄ±zlÄ±klar yaÅŸandÄ±ÄŸÄ±nÄ±, davaya konu edilen taÅŸÄ±nmazlarÄ±n mÃ¼ÅŸterek ana-babadan adÄ± geÃ§en 4 kardeÅŸe...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "3. ğŸ“Š Score: 0.1105\n",
      "   ğŸ“‹ Esas No: 2023/159 E.\n",
      "   ğŸ“‹ Karar No: 2024/38 K.\n",
      "   ğŸ“‹ Daire: HukukGenelKurulu\n",
      "   ğŸ“‹ Tarih: 06.10.2021,24.07.2017,17.01.2019,04.03.2021,15.11.2019,23.10.2020,05.12.2018,24.05.2017,30.03.2021,05.12.2018,13.12.2017,13.12.2017,31.01.2024\n",
      "   ğŸ“„ Metin: Tapu kaydÄ±nda aile konutu ÅŸerhi bulunmasa dahi aile konutuna iliÅŸkin olarak; eÅŸlerden biri diÄŸer eÅŸin aÃ§Ä±k rÄ±zasÄ± bulunmadÄ±kÃ§a aile konutuyla ilgili kira sÃ¶zleÅŸmesini feshedemeyecek, aile konutunu dev...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "4. ğŸ“Š Score: 0.1071\n",
      "   ğŸ“‹ Esas No: 2021/9081 E.\n",
      "   ğŸ“‹ Karar No: 2024/328 K.\n",
      "   ğŸ“‹ Daire: 3.CezaDairesisi\n",
      "   ğŸ“‹ Tarih: 06.08.2016,04.08.2016,10.08.2016,25.08.2016,24.10.2019,24.04.2017,08.04.2008,12.05.2015,26.10.2015,29.09.2021,04.10.2021,26.09.2017,24.04.2017,04.06.2020,16.12.1992,29.09.2016,10.01.2024\n",
      "   ğŸ“„ Metin: Verilen bilginin Ã¶nemi cezanÄ±n belirlenmesinde dikkate alÄ±nmalÄ±dÄ±r (YargÄ±tay (KapatÄ±lan) 16. Ceza Dairesinin 12.05.2015 tarih, 2015/1426 Esas 2015/1292 Karar, 26.10.2015 tarih, 2015/1565 Esas 3464 say...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "5. ğŸ“Š Score: 0.1068\n",
      "   ğŸ“‹ Esas No: 2023/567 E.\n",
      "   ğŸ“‹ Karar No: 2024/42 K.\n",
      "   ğŸ“‹ Daire: HukukGenelKurulu\n",
      "   ğŸ“‹ Tarih: 20.06.2022,01.02.2009,01.10.2018,05.12.2019,24.06.2020,14.01.2021,11.11.2013,01.10.2018,11.11.2013,09.01.2018,30.11.2015,10.01.2018,12.10.2015,01.01.2016,10.01.2018,01.10.2018,29.03.2022,24.03.2012,01.03.2012,28.02.2014,01.03.2014,28.02.2017,20.03.2014,11.11.2013,01.03.2014,28.02.2017,19.10.2021,17.11.2020,16.03.2022,24.03.2012,01.03.2012,28.02.2014,20.03.2014,01.03.2014,28.02.2017,01.03.2017,28.02.2019,11.11.2013,14.01.2021,14.01.2021,14.01.2021,03.11.2021,11.11.2013,01.04.2016,09.01.2018,29.03.2022,31.01.2024,01.03.2014,01.10.2018,10.01.2018,01.10.2018,11.11.2013,01.03.2014,28.02.2017,28.02.2022,07.05.2014,11.11.2013,03.11.2021,11.11.2013,01.04.2016,09.01.2018\n",
      "   ğŸ“„ Metin: Bundan baÅŸka asÄ±l iÅŸverenin iÅŸÃ§ilerinin alt iÅŸveren tarafÄ±ndan iÅŸe alÄ±narak Ã§alÄ±ÅŸtÄ±rÄ±lmaya devam ettirilmesi suretiyle haklarÄ±nÄ±n kÄ±sÄ±tlanmasÄ± veya daha Ã¶nce asÄ±l iÅŸveren tarafÄ±ndan o iÅŸ yerinde Ã§alÄ±ÅŸ...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ” Query vector boyutu: torch.Size([1, 512])\n",
      "ğŸ” Query vector norm: 1.0000\n",
      "ğŸ” Final query vector boyutu: 512 (hedef: 512)\n",
      "ğŸ” Ham arama sonucu sayÄ±sÄ±: 0\n",
      "âš ï¸ Threshold ile sonuÃ§ yok, threshold'suz deneniyor...\n",
      "ğŸ” Threshold'suz sonuÃ§ sayÄ±sÄ±: 5\n",
      "ğŸ” En yÃ¼ksek score: 0.1165\n",
      "ğŸ” En dÃ¼ÅŸÃ¼k score: 0.1068\n",
      "\n",
      "ğŸ“‹ 5 SonuÃ§ Bulundu:\n",
      "\n",
      "1. ğŸ“Š Score: 0.1165\n",
      "   ğŸ“‹ Esas No: 2024/1564 E.\n",
      "   ğŸ“‹ Karar No: 2024/2633 K.\n",
      "   ğŸ“‹ Daire: 1.HukukDairesisi\n",
      "   ğŸ“‹ Tarih: 08.04.2008,20.09.2010,18.03.2014,27.11.1937,01.04.2024\n",
      "   ğŸ“„ Metin: Tapu kaydÄ±nÄ±n uygulanmasÄ±nda tÃ¼m komÅŸu parsel tutanak ve dayanaklarÄ±ndan yararlanÄ±larak tapu kayÄ±t uygulamasÄ± denetlenmemiÅŸ, tapu kaydÄ±nÄ±n miktar fazlasÄ± bulunup bulunmadÄ±ÄŸÄ± incelenmemiÅŸ, keÅŸif yapÄ±lm...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "2. ğŸ“Š Score: 0.1129\n",
      "   ğŸ“‹ Esas No: 2021/17062 E.\n",
      "   ğŸ“‹ Karar No: 2024/730 K.\n",
      "   ğŸ“‹ Daire: 4.HukukDairesisi\n",
      "   ğŸ“‹ Tarih: 24.12.2018,02.05.2018,02.05.2018,23.01.2008,22.01.2024\n",
      "   ğŸ“„ Metin: Ä°cra MÃ¼dÃ¼rlÃ¼ÄŸÃ¼'nÃ¼n 2008/465 (2017/3329 E.) sayÄ±lÄ± icra dosyasÄ±ndan takibe baÅŸlandÄ±ÄŸÄ±nÄ± ve aralarÄ±nda tatsÄ±zlÄ±klar yaÅŸandÄ±ÄŸÄ±nÄ±, davaya konu edilen taÅŸÄ±nmazlarÄ±n mÃ¼ÅŸterek ana-babadan adÄ± geÃ§en 4 kardeÅŸe...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "3. ğŸ“Š Score: 0.1105\n",
      "   ğŸ“‹ Esas No: 2023/159 E.\n",
      "   ğŸ“‹ Karar No: 2024/38 K.\n",
      "   ğŸ“‹ Daire: HukukGenelKurulu\n",
      "   ğŸ“‹ Tarih: 06.10.2021,24.07.2017,17.01.2019,04.03.2021,15.11.2019,23.10.2020,05.12.2018,24.05.2017,30.03.2021,05.12.2018,13.12.2017,13.12.2017,31.01.2024\n",
      "   ğŸ“„ Metin: Tapu kaydÄ±nda aile konutu ÅŸerhi bulunmasa dahi aile konutuna iliÅŸkin olarak; eÅŸlerden biri diÄŸer eÅŸin aÃ§Ä±k rÄ±zasÄ± bulunmadÄ±kÃ§a aile konutuyla ilgili kira sÃ¶zleÅŸmesini feshedemeyecek, aile konutunu dev...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "4. ğŸ“Š Score: 0.1071\n",
      "   ğŸ“‹ Esas No: 2021/9081 E.\n",
      "   ğŸ“‹ Karar No: 2024/328 K.\n",
      "   ğŸ“‹ Daire: 3.CezaDairesisi\n",
      "   ğŸ“‹ Tarih: 06.08.2016,04.08.2016,10.08.2016,25.08.2016,24.10.2019,24.04.2017,08.04.2008,12.05.2015,26.10.2015,29.09.2021,04.10.2021,26.09.2017,24.04.2017,04.06.2020,16.12.1992,29.09.2016,10.01.2024\n",
      "   ğŸ“„ Metin: Verilen bilginin Ã¶nemi cezanÄ±n belirlenmesinde dikkate alÄ±nmalÄ±dÄ±r (YargÄ±tay (KapatÄ±lan) 16. Ceza Dairesinin 12.05.2015 tarih, 2015/1426 Esas 2015/1292 Karar, 26.10.2015 tarih, 2015/1565 Esas 3464 say...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "5. ğŸ“Š Score: 0.1068\n",
      "   ğŸ“‹ Esas No: 2023/567 E.\n",
      "   ğŸ“‹ Karar No: 2024/42 K.\n",
      "   ğŸ“‹ Daire: HukukGenelKurulu\n",
      "   ğŸ“‹ Tarih: 20.06.2022,01.02.2009,01.10.2018,05.12.2019,24.06.2020,14.01.2021,11.11.2013,01.10.2018,11.11.2013,09.01.2018,30.11.2015,10.01.2018,12.10.2015,01.01.2016,10.01.2018,01.10.2018,29.03.2022,24.03.2012,01.03.2012,28.02.2014,01.03.2014,28.02.2017,20.03.2014,11.11.2013,01.03.2014,28.02.2017,19.10.2021,17.11.2020,16.03.2022,24.03.2012,01.03.2012,28.02.2014,20.03.2014,01.03.2014,28.02.2017,01.03.2017,28.02.2019,11.11.2013,14.01.2021,14.01.2021,14.01.2021,03.11.2021,11.11.2013,01.04.2016,09.01.2018,29.03.2022,31.01.2024,01.03.2014,01.10.2018,10.01.2018,01.10.2018,11.11.2013,01.03.2014,28.02.2017,28.02.2022,07.05.2014,11.11.2013,03.11.2021,11.11.2013,01.04.2016,09.01.2018\n",
      "   ğŸ“„ Metin: Bundan baÅŸka asÄ±l iÅŸverenin iÅŸÃ§ilerinin alt iÅŸveren tarafÄ±ndan iÅŸe alÄ±narak Ã§alÄ±ÅŸtÄ±rÄ±lmaya devam ettirilmesi suretiyle haklarÄ±nÄ±n kÄ±sÄ±tlanmasÄ± veya daha Ã¶nce asÄ±l iÅŸveren tarafÄ±ndan o iÅŸ yerinde Ã§alÄ±ÅŸ...\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "1. Full pipeline Ã§alÄ±ÅŸtÄ±r\n",
      "2. Arama yap\n",
      "3. Koleksiyon bilgisi\n",
      "4. ğŸ”§ Debug arama sorunu\n",
      "5. ğŸ—‘ï¸ Dimension uyumsuzluÄŸunu dÃ¼zelt\n",
      "6. Ã‡Ä±kÄ±ÅŸ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + Google EmbeddingGemma + Qdrant Entegrasyon\n",
    "# YargÄ±tay KararlarÄ± iÃ§in Semantic Chunking Pipeline\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from sklearn.decomposition import PCA\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "hf_token = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # L2 normalization ekledik\n",
    "        x = self.linear(x)\n",
    "        return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "\n",
    "# KonfigÃ¼rasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Google EmbeddingGemma ayarlarÄ±\n",
    "    MODEL_NAME: str = \"google/embeddinggemma-300m\"\n",
    "    HF_TOKEN: str = hf_token\n",
    "\n",
    "    # SemChunk ayarlarÄ±\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "\n",
    "    # Qdrant ayarlarÄ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"gemma_semantic_chunks\"\n",
    "    DIMENSION: int = 512  #collection boyutu\n",
    "\n",
    "    # Dosya ayarlarÄ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH_SIZE: int = 256\n",
    "\n",
    "    \n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"YargÄ±tay kararlarÄ± iÃ§in semantic chunking ve vector search\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # SemChunk chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # SentenceTransformer modeli\n",
    "        self.model = SentenceTransformer(\n",
    "            config.MODEL_NAME,\n",
    "            token=config.HF_TOKEN\n",
    "        )\n",
    "\n",
    "        # Reducer modeli - daha iyi initialization ile\n",
    "        self.reducer = EmbedReducer(768, 512)\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.reducer.to(self.device)\n",
    "\n",
    "        # Qdrant client\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        print(f\"âœ… SemChunk hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… Google EmbeddingGemma modeli hazÄ±r\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n",
    "        print(f\"âœ… Device: {self.device}\")\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ğŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "\n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text),\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        self.reducer.eval()  # Eval moduna al\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            start_embed = time.time()\n",
    "            \n",
    "            # Embedding oluÅŸtur\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = self.model.encode(\n",
    "                    batch_texts, \n",
    "                    show_progress_bar=True, \n",
    "                    convert_to_tensor=True, \n",
    "                    normalize_embeddings=True  # Bu Ã¶nemli!\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Boyut dÃ¼ÅŸÃ¼r ve normalize et\n",
    "                reduced_vector = self.reducer(batch_embeddings)\n",
    "                \n",
    "                print(\"*\" * 40)\n",
    "                print(f\"Original embedding shape: {batch_embeddings.shape}\")\n",
    "                print(f\"Reduced vector shape: {reduced_vector.shape}\")\n",
    "                print(f\"Vector norm check (should be ~1.0): {torch.norm(reduced_vector[0]).item():.4f}\")\n",
    "                print(\"*\" * 40)\n",
    "                \n",
    "                # CPU'ya taÅŸÄ± ve listeye Ã§evir\n",
    "                all_embeddings.extend(reduced_vector.cpu().tolist())\n",
    "            \n",
    "            end_embed = time.time()\n",
    "            print(f\"Batch embedding sÃ¼resi: {end_embed - start_embed:.2f} saniye\")\n",
    "            print(f\"  ğŸ”¹ Embedding oluÅŸturuldu: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        total_rows = len(df)\n",
    "        print(f\"ğŸ“„ Toplam {total_rows} satÄ±r iÅŸlenecek\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "            }\n",
    "\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == total_rows:\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{total_rows} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"ğŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        print(\"ğŸ”® Embedding'ler oluÅŸturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            ) for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "        batch_size = self.config.DB_BATCH_SIZE\n",
    "        total_points = len(points)\n",
    "        print(f\"ğŸš€ {total_points} chunk Qdrant'a yÃ¼kleniyor ({batch_size} batch size)\")\n",
    "\n",
    "        for i in range(0, total_points, batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            try:\n",
    "                start_upload = time.time()\n",
    "\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "\n",
    "                end_upload = time.time()\n",
    "                print(f\"batch Qdrant yÃ¼kleme sÃ¼resi: {end_upload - start_upload:.2f} saniye\")\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i + batch_size, total_points)}/{total_points}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "\n",
    "        print(f\"ğŸ‰ {total_points} chunk Qdrant'a yÃ¼klendi!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"DÃ¼zeltilmiÅŸ search metodu - daha dÃ¼ÅŸÃ¼k threshold ile\"\"\"\n",
    "        \n",
    "        self.reducer.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Query embedding Ã§Ä±kar (normalize edilmiÅŸ)\n",
    "            query_embedding = self.model.encode(\n",
    "                [query], \n",
    "                convert_to_tensor=True, \n",
    "                normalize_embeddings=True  # Bu Ã¶nemli!\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Boyutu 512'ye dÃ¼ÅŸÃ¼r ve normalize et\n",
    "            reduced_query_embedding = self.reducer(query_embedding)\n",
    "            \n",
    "            print(f\"ğŸ” Query vector boyutu: {reduced_query_embedding.shape}\")\n",
    "            print(f\"ğŸ” Query vector norm: {torch.norm(reduced_query_embedding[0]).item():.4f}\")\n",
    "            \n",
    "            # CPU'ya taÅŸÄ± ve numpy array'e Ã§evir\n",
    "            query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "        \n",
    "        print(f\"ğŸ” Final query vector boyutu: {len(query_vector)} (hedef: {self.config.DIMENSION})\")\n",
    "        \n",
    "        try:\n",
    "            # Qdrant aramasÄ± - daha dÃ¼ÅŸÃ¼k threshold ile\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"ğŸ” Ham arama sonucu sayÄ±sÄ±: {len(search_results)}\")\n",
    "            \n",
    "            # EÄŸer hiÃ§ sonuÃ§ yoksa, threshold'suz deneyelim\n",
    "            if not search_results:\n",
    "                print(\"âš ï¸ Threshold ile sonuÃ§ yok, threshold'suz deneniyor...\")\n",
    "                search_results = self.qdrant_client.search(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    query_vector=query_vector,\n",
    "                    limit=limit\n",
    "                )\n",
    "                print(f\"ğŸ” Threshold'suz sonuÃ§ sayÄ±sÄ±: {len(search_results)}\")\n",
    "                \n",
    "                if search_results:\n",
    "                    print(f\"ğŸ” En yÃ¼ksek score: {search_results[0].score:.4f}\")\n",
    "                    print(f\"ğŸ” En dÃ¼ÅŸÃ¼k score: {search_results[-1].score:.4f}\")\n",
    "            \n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def debug_search_issue(self, query: str):\n",
    "        \"\"\"Arama problemini debug et\"\"\"\n",
    "        print(f\"ğŸ”§ Debug baÅŸlÄ±yor - Query: '{query}'\")\n",
    "        \n",
    "        # 1. Koleksiyon durumunu kontrol et\n",
    "        info = self.get_collection_info()\n",
    "        print(f\"ğŸ“Š Koleksiyon durumu: {info}\")\n",
    "        \n",
    "        if info.get('points_count', 0) == 0:\n",
    "            print(\"âŒ Koleksiyonda hiÃ§ point yok!\")\n",
    "            return\n",
    "        \n",
    "        # 2. Query embedding oluÅŸtur\n",
    "        self.reducer.eval()\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model.encode([query], convert_to_tensor=True, normalize_embeddings=True).to(self.device)\n",
    "            reduced_query = self.reducer(query_embedding)\n",
    "            query_vector = reduced_query[0].cpu().numpy().tolist()\n",
    "            \n",
    "        print(f\"ğŸ” Query vector shape: {len(query_vector)}\")\n",
    "        print(f\"ğŸ” Query vector norm: {np.linalg.norm(query_vector):.4f}\")\n",
    "        print(f\"ğŸ” Query vector sample: {query_vector[:5]}\")\n",
    "        \n",
    "        # 3. Ã‡ok dÃ¼ÅŸÃ¼k threshold ile ara\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=5,\n",
    "                score_threshold=0.0  # Ã‡ok dÃ¼ÅŸÃ¼k threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"ğŸ” Score threshold 0.0 ile bulunan sonuÃ§: {len(results)}\")\n",
    "            if results:\n",
    "                for i, r in enumerate(results):\n",
    "                    print(f\"  {i+1}. Score: {r.score:.6f}\")\n",
    "                    text = r.payload.get('text', '')[:100]\n",
    "                    print(f\"     Text preview: {text}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Debug search hatasÄ±: {e}\")\n",
    "    \n",
    "    def fix_dimension_mismatch(self):\n",
    "        \"\"\"Dimension uyumsuzluÄŸunu dÃ¼zelt\"\"\"\n",
    "        print(\"ğŸ”§ Dimension uyumsuzluÄŸu dÃ¼zeltiliyor...\")\n",
    "        response = input(\"âš ï¸ Bu iÅŸlem koleksiyonu silecek. Devam? (y/N): \")\n",
    "        if response.lower() != 'y':\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.config.COLLECTION_NAME)\n",
    "            print(\"âœ… Koleksiyon silindi\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Silme hatasÄ±: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Pipeline sÄ±nÄ±fÄ±\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"ğŸš€ YargÄ±tay Semantic Pipeline BaÅŸlÄ±yor\")\n",
    "\n",
    "        total_start = time.time()\n",
    "\n",
    "        # Koleksiyon oluÅŸturma\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "\n",
    "        # CSV iÅŸlemleri ve chunk oluÅŸturma\n",
    "        chunk_start = time.time()\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        chunk_end = time.time()\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"âŒ Ä°ÅŸlenecek chunk bulunamadÄ±\")\n",
    "            return False\n",
    "\n",
    "        # Embedding oluÅŸturma ve Qdrant yÃ¼kleme\n",
    "        upload_start = time.time()\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        upload_end = time.time()\n",
    "\n",
    "        total_end = time.time()\n",
    "\n",
    "        # Toplam istatistikler\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nğŸ“Š Pipeline SÃ¼releri ve Ä°statistikler:\")\n",
    "        print(json.dumps({\n",
    "            \"collection_name\": self.config.COLLECTION_NAME,\n",
    "            \"points_uploaded\": info.get(\"points_count\", 0),\n",
    "            \"chunk_creation_time_s\": round(chunk_end - chunk_start, 2),\n",
    "            \"embedding_and_upload_time_s\": round(upload_end - upload_start, 2),\n",
    "            \"total_pipeline_time_s\": round(total_end - total_start, 2)\n",
    "        }, indent=2, ensure_ascii=False))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"ğŸ” Ä°nteraktif Arama BaÅŸlatÄ±ldÄ±\")\n",
    "        \n",
    "        # Ã–nce koleksiyon durumunu kontrol et\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(f\"ğŸ“Š Koleksiyon Durumu: {json.dumps(info, indent=2, ensure_ascii=False)}\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nğŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \").strip()\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                print(\"âŒ BoÅŸ sorgu, tekrar deneyin\")\n",
    "                continue\n",
    "            \n",
    "            limit_input = input(\"KaÃ§ sonuÃ§? (default 5): \").strip()\n",
    "            try:\n",
    "                limit = int(limit_input) if limit_input else 5\n",
    "                limit = max(1, min(limit, 50))\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz sayÄ±, varsayÄ±lan 5 kullanÄ±lÄ±yor\")\n",
    "                limit = 5\n",
    "\n",
    "            score_input = input(\"Minimum score? (default 0.3): \").strip()\n",
    "            try:\n",
    "                score_threshold = float(score_input) if score_input else 0.3\n",
    "                score_threshold = max(0.0, min(score_threshold, 1.0))\n",
    "            except ValueError:\n",
    "                print(\"âŒ GeÃ§ersiz score, varsayÄ±lan 0.3 kullanÄ±lÄ±yor\")\n",
    "                score_threshold = 0.3\n",
    "\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            results = self.processor.search_semantic(query, limit=limit, score_threshold=score_threshold)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"âŒ HiÃ§ sonuÃ§ bulunamadÄ±.\")\n",
    "                print(\"ğŸ’¡ Ã–neriler:\")\n",
    "                print(\"   - Score threshold'u dÃ¼ÅŸÃ¼rÃ¼n (0.03 veya 0.05)\")\n",
    "                print(\"   - Debug modunu Ã§alÄ±ÅŸtÄ±rÄ±n (ana menÃ¼den 4)\")\n",
    "                print(\"   - Daha spesifik kelimeler kullanÄ±n\")\n",
    "            else:\n",
    "                print(f\"\\nğŸ“‹ {len(results)} SonuÃ§ Bulundu:\")\n",
    "                for i, r in enumerate(results, 1):\n",
    "                    payload = r['payload']\n",
    "                    text_preview = payload.get('text', '')[:200] + \"...\" if len(payload.get('text', '')) > 200 else payload.get('text', '')\n",
    "                    print(f\"\\n{i}. ğŸ“Š Score: {r['score']:.4f}\")\n",
    "                    print(f\"   ğŸ“‹ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“‹ Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“‹ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“‹ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                    print(f\"   ğŸ“„ Metin: {text_preview}\")\n",
    "                    print(f\"   {'â”€'*50}\")\n",
    "            \n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# Main fonksiyon\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/yargitay_cleaned_2025-08-21.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"google_gemma_chunks\",\n",
    "        DIMENSION=512\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"1. Full pipeline Ã§alÄ±ÅŸtÄ±r\")\n",
    "        print(\"2. Arama yap\") \n",
    "        print(\"3. Koleksiyon bilgisi\")\n",
    "        print(\"4. ğŸ”§ Debug arama sorunu\")\n",
    "        print(\"5. ğŸ—‘ï¸ Dimension uyumsuzluÄŸunu dÃ¼zelt\")\n",
    "        print(\"6. Ã‡Ä±kÄ±ÅŸ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        choice = input(\"SeÃ§im: \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            pipeline.full_pipeline(csv_path)\n",
    "            \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "            \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nğŸ“Š Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "            \n",
    "        elif choice == \"4\":\n",
    "            query = input(\"Debug iÃ§in test query (Enter: 'test'): \").strip() or \"test\"\n",
    "            pipeline.processor.debug_search_issue(query)\n",
    "            \n",
    "        elif choice == \"5\":\n",
    "            if pipeline.processor.fix_dimension_mismatch():\n",
    "                print(\"âœ… Koleksiyon sÄ±fÄ±rlandÄ±. Åimdi '1' seÃ§eneÄŸi ile veriyi yeniden yÃ¼kleyin.\")\n",
    "            else:\n",
    "                print(\"âŒ Ä°ÅŸlem iptal edildi\")\n",
    "                \n",
    "        elif choice == \"6\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
