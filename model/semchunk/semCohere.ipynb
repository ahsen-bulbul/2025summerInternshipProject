{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cohere-multilingual-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "‚úÖ SemChunk chunker hazƒ±r (Token boyutu: 384)\n",
      "‚úÖ Cohere client hazƒ±r (embed-multilingual-v3.0)\n",
      "‚úÖ Qdrant client hazƒ±r (http://localhost:6333)\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí Qdrant)\n",
      "2. ƒ∞nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini g√∂ster\n",
      "4. √áƒ±kƒ±≈ü\n",
      "üöÄ Yargƒ±tay Semantic Pipeline Ba≈ülƒ±yor\n",
      "==================================================\n",
      "‚ùå Cohere baƒülantƒ± hatasƒ±: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '5bf0d1a2552119ff991f5b36ffda06d6', 'x-trial-endpoint-call-limit': '100', 'x-trial-endpoint-call-remaining': '99', 'date': 'Tue, 09 Sep 2025 11:00:17 GMT', 'content-length': '373', 'x-envoy-upstream-service-time': '46', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'}, status_code: 429, body: {'id': 'edbc2123-62f2-4c61-8b0e-df3430d63569', 'message': \"You are using a Trial key, which is limited to 1000 API calls / month. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"}\n",
      "‚ùå Pipeline hatasƒ±!\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí Qdrant)\n",
      "2. ƒ∞nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini g√∂ster\n",
      "4. √áƒ±kƒ±≈ü\n",
      "üöÄ Yargƒ±tay Semantic Pipeline Ba≈ülƒ±yor\n",
      "==================================================\n",
      "‚ùå Cohere baƒülantƒ± hatasƒ±: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '7e9f47aa2f542467d696dc37252e8700', 'x-trial-endpoint-call-limit': '100', 'x-trial-endpoint-call-remaining': '98', 'date': 'Tue, 09 Sep 2025 11:00:30 GMT', 'content-length': '373', 'x-envoy-upstream-service-time': '11', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'}, status_code: 429, body: {'id': '72e4002e-5253-42ef-a5d1-0b239945c502', 'message': \"You are using a Trial key, which is limited to 1000 API calls / month. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"}\n",
      "‚ùå Pipeline hatasƒ±!\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí Qdrant)\n",
      "2. ƒ∞nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini g√∂ster\n",
      "4. √áƒ±kƒ±≈ü\n",
      "üëã G√∂r√º≈ü√ºr√ºz!\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + Cohere Multilingual + Qdrant Entegrasyon\n",
    "# Yargƒ±tay Kararlarƒ± i√ßin Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import cohere\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "cohere_api_key=os.getenv(\"COHERE_API_KEY\")\n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Cohere ayarlarƒ±\n",
    "    COHERE_API_KEY: str = cohere_api_key  # Cohere API anahtarƒ±nƒ±z\n",
    "    COHERE_MODEL: str = \"embed-multilingual-v3.0\"  # Cohere multilingual model\n",
    "    \n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 384  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_semantic_chunks\"\n",
    "    DIMENSION: int = 1024  # Cohere multilingual embedding boyutu\n",
    "    \n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/10data.csv\"\n",
    "    BATCH_SIZE: int = 10\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargƒ±tay kararlarƒ± i√ßin semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # SemChunk chunker olu≈ütur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Cohere client olu≈ütur\n",
    "        self.cohere_client = cohere.Client(config.COHERE_API_KEY)\n",
    "        \n",
    "        # Qdrant client olu≈ütur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"‚úÖ SemChunk chunker hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ Cohere client hazƒ±r ({config.COHERE_MODEL})\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_cohere_connection(self):\n",
    "        \"\"\"Cohere baƒülantƒ±sƒ±nƒ± test et\"\"\"\n",
    "        try:\n",
    "            test_response = self.cohere_client.embed(\n",
    "                texts=[\"Bu bir test metnidir\"],\n",
    "                model=self.config.COHERE_MODEL,\n",
    "                input_type=\"search_document\"\n",
    "            )\n",
    "            embedding_dim = len(test_response.embeddings[0])\n",
    "            print(f\"‚úÖ Cohere test ba≈üarƒ±lƒ± - Embedding boyutu: {embedding_dim}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Cohere baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu olu≈ütur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa olu≈ütur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara b√∂l\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni b√∂l\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Bo≈ü chunk'larƒ± atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 10) -> List[List[float]]:\n",
    "        \"\"\"Metinleri Cohere ile embedding'e √ßevir\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Cohere API limitleri i√ßin batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                response = self.cohere_client.embed(\n",
    "                    texts=batch_texts,\n",
    "                    model=self.config.COHERE_MODEL,\n",
    "                    input_type=\"search_document\"  # Dokuman indexleme i√ßin\n",
    "                )\n",
    "                \n",
    "                batch_embeddings = response.embeddings\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                print(f\"  üìä Embedding olu≈üturuldu: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda bo≈ü embedding ekle\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM] * len(batch_texts))\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasƒ±nƒ± i≈üle ve chunk'larƒ± olu≈ütur\"\"\"\n",
    "        print(f\"üìÑ CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r veri y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Gerekli s√ºtunlarƒ± kontrol et\n",
    "        required_columns = ['rawText']  # Ana metin s√ºtunu\n",
    "        optional_columns = ['esasNo', 'kararNo', 'location', 'extractedDates']\n",
    "        \n",
    "        if 'rawText' not in df.columns:\n",
    "            print(f\"‚ùå 'rawText' s√ºtunu bulunamadƒ±. Mevcut s√ºtunlar: {df.columns.tolist()}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"üîÑ Semantic chunking ba≈ülƒ±yor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazƒ±rla\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "                #'karar_turu': row.get('karar_turu', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress g√∂ster\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'larƒ± Qdrant'a y√ºkle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # Embedding'leri olu≈ütur\n",
    "        print(\"üîÆ Embedding'ler olu≈üturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazƒ±rla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde y√ºkle\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "        print(f\"üì¶ {batch_size} batch size ile y√ºkleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "        \n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "    \n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"Semantic arama yap\"\"\"\n",
    "        print(f\"üîç Arama: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Query i√ßin embedding olu≈ütur\n",
    "            query_response = self.cohere_client.embed(\n",
    "                texts=[query],\n",
    "                model=self.config.COHERE_MODEL,\n",
    "                input_type=\"search_query\"  # Arama query'si i√ßin\n",
    "            )\n",
    "            query_embedding = query_response.embeddings[0]\n",
    "            \n",
    "            # Qdrant'ta ara\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_embedding,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonu√ßlarƒ± formatla\n",
    "            results = []\n",
    "            for point in search_results:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sƒ±nƒ±fƒ±\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sƒ±nƒ±fƒ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ƒ± √ßalƒ±≈ütƒ±r\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"üöÄ Yargƒ±tay Semantic Pipeline Ba≈ülƒ±yor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Baƒülantƒ±larƒ± test et\n",
    "        embedding_dim = self.processor.test_cohere_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon olu≈ütur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi i≈üle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå ƒ∞≈ülenecek chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a y√ºkle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri g√∂ster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"ƒ∞nteraktif arama aray√ºz√º\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY SEMANTƒ∞K ARAMA Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\nüîç Arama metni (√ßƒ±kmak i√ßin 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                print(\"üëã G√∂r√º≈ü√ºr√ºz!\")\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"üìä Ka√ß sonu√ß? (varsayƒ±lan 5): \") or \"5\")\n",
    "            except:\n",
    "                limit = 5\n",
    "            \n",
    "            # Arama yap\n",
    "            results = self.processor.search_semantic(query, limit=limit)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìã {len(results)} sonu√ß bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. üìÑ Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ‚öñÔ∏è Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   üìã Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   üèõÔ∏è Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   üìÖ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   üî§ Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   üìù Metin √ñnizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanƒ±m √∂rneƒüi ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfig√ºrasyon (buraya kendi bilgilerinizi yazƒ±n)\n",
    "    config = Config(\n",
    "        COHERE_API_KEY=str(cohere_api_key),  # Cohere API anahtarƒ±nƒ±z\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",  # CSV dosya yolunuz\n",
    "        TOKEN_SIZE=384,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",  # Lokal Qdrant URL\n",
    "        COLLECTION_NAME=\"cohere_semantic_chunks\",\n",
    "        DIMENSION=1024\n",
    "    )\n",
    "    \n",
    "    # Pipeline olu≈ütur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Men√º g√∂ster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí Qdrant)\")\n",
    "        print(\"2. ƒ∞nteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4. √áƒ±kƒ±≈ü\")\n",
    "        \n",
    "        choice = input(\"\\nSe√ßiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"‚úÖ Pipeline ba≈üarƒ±yla tamamlandƒ±!\")\n",
    "            else:\n",
    "                print(\"‚ùå Pipeline hatasƒ±!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon\n",
    "# Yargƒ±tay Kararlarƒ± i√ßin Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # BGE-M3 ayarlarƒ±\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"  # BGE-M3 model\n",
    "    USE_FP16: bool = True  # Hafƒ±za optimizasyonu\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 512  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 1024  # BGE-M3 dense embedding boyutu\n",
    "    \n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 32  # BGE-M3 i√ßin optimize edilmi≈ü batch size\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargƒ±tay kararlarƒ± i√ßin semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # GPU/CPU kontrol√º\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üöÄ GPU kullanƒ±lƒ±yor: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            print(\"üíª CPU kullanƒ±lƒ±yor\")\n",
    "        \n",
    "        # SemChunk chunker olu≈ütur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # BGE-M3 modelini y√ºkle\n",
    "        print(f\"üîÆ BGE-M3 modeli y√ºkleniyor... ({config.BGE_MODEL_NAME})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        \n",
    "        # Qdrant client olu≈ütur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"‚úÖ SemChunk chunker hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ BGE-M3 model hazƒ±r ({config.BGE_MODEL_NAME})\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_bge_connection(self):\n",
    "        \"\"\"BGE-M3 modelini test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            \n",
    "            # BGE-M3'den dense embedding al\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            embedding_dim = len(dense_embedding)\n",
    "            \n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ± - Dense embedding boyutu: {embedding_dim}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {'colbert_vecs' in embeddings}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu olu≈ütur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa olu≈ütur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Boyut: {self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara b√∂l\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni b√∂l\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Bo≈ü chunk'larƒ± atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        \"\"\"Metinleri BGE-M3 ile embedding'e √ßevir\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.BATCH_SIZE\n",
    "            \n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"üîÆ BGE-M3 ile {len(texts)} metin i≈üleniyor...\")\n",
    "        \n",
    "        # BGE-M3 i√ßin batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # BGE-M3 ile embedding olu≈ütur\n",
    "                embeddings_result = self.bge_model.encode(batch_texts)\n",
    "                \n",
    "                # Dense embedding'leri al (1024 boyut)\n",
    "                dense_embeddings = embeddings_result['dense_vecs']\n",
    "                \n",
    "                # List formatƒ±na √ßevir\n",
    "                for embedding in dense_embeddings:\n",
    "                    all_embeddings.append(embedding.tolist())\n",
    "                \n",
    "                print(f\"  üìä BGE-M3 Embedding: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "                # GPU memory temizliƒüi (gerekirse)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå BGE-M3 Embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda sƒ±fƒ±r embedding ekle\n",
    "                for _ in batch_texts:\n",
    "                    all_embeddings.append([0.0] * self.config.EMBEDDING_DIM)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasƒ±nƒ± i≈üle ve chunk'larƒ± olu≈ütur\"\"\"\n",
    "        print(f\"üìÑ CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r veri y√ºklendi\")\n",
    "            print(f\"üìã Mevcut s√ºtunlar: {df.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Ana metin s√ºtununu belirle (√∂ncelik sƒ±rasƒ±na g√∂re)\n",
    "        text_columns = ['rawText', 'chunk_text', 'text', 'content', 'metin']\n",
    "        text_column = None\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"‚úÖ Ana metin s√ºtunu bulundu: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not text_column:\n",
    "            print(f\"‚ùå Ana metin s√ºtunu bulunamadƒ±. Kontrol edilen s√ºtunlar: {text_columns}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"üîÑ Semantic chunking ba≈ülƒ±yor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get(text_column, '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                print(f\"‚ö†Ô∏è Satƒ±r {idx}: Bo≈ü metin atlandƒ±\")\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazƒ±rla (CSV yapƒ±nƒ±za g√∂re g√ºncellenmi≈ü)\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress g√∂ster\n",
    "            if (idx + 1) % 5 == 0:  # Daha sƒ±k progress g√∂ster (az veri olduƒüu i√ßin)\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'larƒ± Qdrant'a y√ºkle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # BGE-M3 ile embedding'leri olu≈ütur\n",
    "        print(\"üîÆ BGE-M3 embedding'ler olu≈üturuluyor...\")\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazƒ±rla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde y√ºkle\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "        print(f\"üì¶ {batch_size} batch size ile y√ºkleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "        \n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "    \n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"BGE-M3 ile semantic arama yap\"\"\"\n",
    "        print(f\"üîç Arama: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vekt√∂rize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Qdrant'ta ara (g√ºncel query_points metodu)\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonu√ßlarƒ± formatla\n",
    "            results = []\n",
    "            for point in search_results.points:#burda muhtemel hata verir search_results olcak verirse\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"Filtreli arama yap\"\"\"\n",
    "        print(f\"üîç Filtreli arama: '{query}' - Filtreler: {filters}\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vekt√∂rize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Filter olu≈ütur\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = []\n",
    "                for key, value in filters.items():\n",
    "                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "                query_filter = Filter(must=conditions)\n",
    "            \n",
    "            # Qdrant'ta filtreli arama yap\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonu√ßlarƒ± formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sƒ±nƒ±fƒ±\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sƒ±nƒ±fƒ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ƒ± √ßalƒ±≈ütƒ±r\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"üöÄ Yargƒ±tay BGE-M3 Semantic Pipeline Ba≈ülƒ±yor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon olu≈ütur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi i≈üle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå ƒ∞≈ülenecek chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a y√ºkle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri g√∂ster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"ƒ∞nteraktif arama aray√ºz√º\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K ARAMA Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nüîç Arama Se√ßenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana men√ºye d√∂n\")\n",
    "            \n",
    "            search_choice = input(\"Se√ßiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nüîç Arama metni (√ßƒ±kmak i√ßin 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"üìä Ka√ß sonu√ß? (varsayƒ±lan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"üéØ Minimum benzerlik skoru? (varsayƒ±lan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\nüîß Filtre Se√ßenekleri (bo≈ü bƒ±rakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (√∂rn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìã {len(results)} sonu√ß bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. üìÑ BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ‚öñÔ∏è Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   üìã Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   üèõÔ∏è Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   üìÖ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   üî§ Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   üìù Metin √ñnizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanƒ±m √∂rneƒüi ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfig√ºrasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=1024,\n",
    "        BATCH_SIZE=16,  # GPU memory'ye g√∂re ayarlayƒ±n\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline olu≈ütur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Men√º g√∂ster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\")\n",
    "        print(\"2. ƒ∞nteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4. √áƒ±kƒ±≈ü\")\n",
    "        \n",
    "        choice = input(\"\\nSe√ßiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"‚úÖ BGE-M3 Pipeline ba≈üarƒ±yla tamamlandƒ±!\")\n",
    "            else:\n",
    "                print(\"‚ùå Pipeline hatasƒ±!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrol√º\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding k√ºt√ºphanesi y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding k√ºt√ºphanesi bulunamadƒ±!\")\n",
    "        print(\"Kurulum i√ßin: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon\n",
    "# Yargƒ±tay Kararlarƒ± i√ßin Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # BGE-M3 ayarlarƒ±\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"  # BGE-M3 model\n",
    "    USE_FP16: bool = True  # Hafƒ±za optimizasyonu\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 512  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 1024  # BGE-M3 dense embedding boyutu\n",
    "    \n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 32  # BGE-M3 i√ßin optimize edilmi≈ü batch size\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargƒ±tay kararlarƒ± i√ßin semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # GPU/CPU kontrol√º\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üöÄ GPU kullanƒ±lƒ±yor: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            print(\"üíª CPU kullanƒ±lƒ±yor\")\n",
    "        \n",
    "        # SemChunk chunker olu≈ütur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # BGE-M3 modelini y√ºkle\n",
    "        print(f\"üîÆ BGE-M3 modeli y√ºkleniyor... ({config.BGE_MODEL_NAME})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        \n",
    "        # Qdrant client olu≈ütur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"‚úÖ SemChunk chunker hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ BGE-M3 model hazƒ±r ({config.BGE_MODEL_NAME})\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_bge_connection(self):\n",
    "        \"\"\"BGE-M3 modelini test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            \n",
    "            # BGE-M3'den dense embedding al\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            embedding_dim = len(dense_embedding)\n",
    "            \n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ± - Dense embedding boyutu: {embedding_dim}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {'colbert_vecs' in embeddings}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu olu≈ütur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa olu≈ütur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Boyut: {self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara b√∂l\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni b√∂l\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Bo≈ü chunk'larƒ± atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        \"\"\"Metinleri BGE-M3 ile embedding'e √ßevir\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.BATCH_SIZE\n",
    "            \n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"üîÆ BGE-M3 ile {len(texts)} metin i≈üleniyor...\")\n",
    "        \n",
    "        # BGE-M3 i√ßin batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # BGE-M3 ile embedding olu≈ütur\n",
    "                embeddings_result = self.bge_model.encode(batch_texts)\n",
    "                \n",
    "                # Dense embedding'leri al (1024 boyut)\n",
    "                dense_embeddings = embeddings_result['dense_vecs']\n",
    "                \n",
    "                # List formatƒ±na √ßevir\n",
    "                for embedding in dense_embeddings:\n",
    "                    all_embeddings.append(embedding.tolist())\n",
    "                \n",
    "                print(f\"  üìä BGE-M3 Embedding: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "                # GPU memory temizliƒüi (gerekirse)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå BGE-M3 Embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda sƒ±fƒ±r embedding ekle\n",
    "                for _ in batch_texts:\n",
    "                    all_embeddings.append([0.0] * self.config.EMBEDDING_DIM)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasƒ±nƒ± i≈üle ve chunk'larƒ± olu≈ütur\"\"\"\n",
    "        print(f\"üìÑ CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r veri y√ºklendi\")\n",
    "            print(f\"üìã Mevcut s√ºtunlar: {df.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Ana metin s√ºtununu belirle (√∂ncelik sƒ±rasƒ±na g√∂re)\n",
    "        text_columns = ['rawText', 'chunk_text', 'text', 'content', 'metin']\n",
    "        text_column = None\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"‚úÖ Ana metin s√ºtunu bulundu: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not text_column:\n",
    "            print(f\"‚ùå Ana metin s√ºtunu bulunamadƒ±. Kontrol edilen s√ºtunlar: {text_columns}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"üîÑ Semantic chunking ba≈ülƒ±yor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get(text_column, '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                print(f\"‚ö†Ô∏è Satƒ±r {idx}: Bo≈ü metin atlandƒ±\")\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazƒ±rla (CSV yapƒ±nƒ±za g√∂re g√ºncellenmi≈ü)\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', '') or row.get('esas_no', ''),\n",
    "                'karar_no': row.get('kararNo', '') or row.get('karar_no', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '') or row.get('dates', ''),\n",
    "                'esas_no_num': row.get('esasNo_num', ''),\n",
    "                'esas_no_tip': row.get('esasNo_tip', ''),\n",
    "                'karar_no_num': row.get('kararNo_num', ''),\n",
    "                'karar_no_tip': row.get('kararNo_tip', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress g√∂ster\n",
    "            if (idx + 1) % 5 == 0:  # Daha sƒ±k progress g√∂ster (az veri olduƒüu i√ßin)\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'larƒ± Qdrant'a y√ºkle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # BGE-M3 ile embedding'leri olu≈ütur\n",
    "        print(\"üîÆ BGE-M3 embedding'ler olu≈üturuluyor...\")\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazƒ±rla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde y√ºkle\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "        print(f\"üì¶ {batch_size} batch size ile y√ºkleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "        \n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "    \n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"BGE-M3 ile semantic arama yap\"\"\"\n",
    "        print(f\"üîç Arama: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vekt√∂rize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Qdrant'ta ara (g√ºncel query_points metodu)\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonu√ßlarƒ± formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"Filtreli arama yap\"\"\"\n",
    "        print(f\"üîç Filtreli arama: '{query}' - Filtreler: {filters}\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vekt√∂rize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Filter olu≈ütur\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = []\n",
    "                for key, value in filters.items():\n",
    "                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "                query_filter = Filter(must=conditions)\n",
    "            \n",
    "            # Qdrant'ta filtreli arama yap\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonu√ßlarƒ± formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sƒ±nƒ±fƒ±\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sƒ±nƒ±fƒ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ƒ± √ßalƒ±≈ütƒ±r\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"üöÄ Yargƒ±tay BGE-M3 Semantic Pipeline Ba≈ülƒ±yor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon olu≈ütur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi i≈üle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå ƒ∞≈ülenecek chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a y√ºkle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri g√∂ster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"ƒ∞nteraktif arama aray√ºz√º\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K ARAMA Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nüîç Arama Se√ßenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana men√ºye d√∂n\")\n",
    "            \n",
    "            search_choice = input(\"Se√ßiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nüîç Arama metni (√ßƒ±kmak i√ßin 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"üìä Ka√ß sonu√ß? (varsayƒ±lan 5): \") or \"5\")\n",
    "                threshold = float(input(\"üéØ Minimum benzerlik skoru? (varsayƒ±lan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit, score_threshold=threshold)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\nüîß Filtre Se√ßenekleri (bo≈ü bƒ±rakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (√∂rn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit, score_threshold=threshold\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìã {len(results)} sonu√ß bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. üìÑ BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ‚öñÔ∏è Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   üìã Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   üèõÔ∏è Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   üìÖ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   üî§ Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   üìù Metin √ñnizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanƒ±m √∂rneƒüi ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfig√ºrasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=1024,\n",
    "        BATCH_SIZE=16,  # GPU memory'ye g√∂re ayarlayƒ±n\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline olu≈ütur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Men√º g√∂ster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\")\n",
    "        print(\"2. ƒ∞nteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4. √áƒ±kƒ±≈ü\")\n",
    "        \n",
    "        choice = input(\"\\nSe√ßiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"‚úÖ BGE-M3 Pipeline ba≈üarƒ±yla tamamlandƒ±!\")\n",
    "            else:\n",
    "                print(\"‚ùå Pipeline hatasƒ±!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrol√º\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding k√ºt√ºphanesi y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding k√ºt√ºphanesi bulunamadƒ±!\")\n",
    "        print(\"Kurulum i√ßin: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
