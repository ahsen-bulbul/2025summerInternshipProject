{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cohere-multilingual-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "✅ SemChunk chunker hazır (Token boyutu: 384)\n",
      "✅ Cohere client hazır (embed-multilingual-v3.0)\n",
      "✅ Qdrant client hazır (http://localhost:6333)\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "🚀 Yargıtay Semantic Pipeline Başlıyor\n",
      "==================================================\n",
      "❌ Cohere bağlantı hatası: headers: {'access-control-expose-headers': 'X-Debug-Trace-ID', 'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin', 'x-accel-expires': '0', 'x-debug-trace-id': '9e4699f7af22b34b5a38e05ab96b6167', 'x-trial-endpoint-call-limit': '100', 'x-trial-endpoint-call-remaining': '99', 'date': 'Tue, 09 Sep 2025 11:03:45 GMT', 'content-length': '373', 'x-envoy-upstream-service-time': '12', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'}, status_code: 429, body: {'id': '7ff541e5-1f43-4ef0-935a-c11034a2b057', 'message': \"You are using a Trial key, which is limited to 1000 API calls / month. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"}\n",
      "❌ Pipeline hatası!\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "👋 Görüşürüz!\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + Cohere Multilingual + Qdrant Entegrasyon\n",
    "# Yargıtay Kararları için Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import cohere\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "cohere_api_key=os.getenv(\"COHERE_API_KEY\")\n",
    "# Konfigürasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Cohere ayarları\n",
    "    COHERE_API_KEY: str = cohere_api_key  # Cohere API anahtarınız\n",
    "    COHERE_MODEL: str = \"embed-multilingual-v3.0\"  # Cohere multilingual model\n",
    "    \n",
    "    # SemChunk ayarları\n",
    "    TOKEN_SIZE: int = 384  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarları\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_semantic_chunks\"\n",
    "    DIMENSION: int = 1024  # Cohere multilingual embedding boyutu\n",
    "    \n",
    "    # Dosya ayarları\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/10data.csv\"\n",
    "    BATCH_SIZE: int = 10\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargıtay kararları için semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # SemChunk chunker oluştur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # Cohere client oluştur\n",
    "        self.cohere_client = cohere.Client(config.COHERE_API_KEY)\n",
    "        \n",
    "        # Qdrant client oluştur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"✅ SemChunk chunker hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ Cohere client hazır ({config.COHERE_MODEL})\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_cohere_connection(self):\n",
    "        \"\"\"Cohere bağlantısını test et\"\"\"\n",
    "        try:\n",
    "            test_response = self.cohere_client.embed(\n",
    "                texts=[\"Bu bir test metnidir\"],\n",
    "                model=self.config.COHERE_MODEL,\n",
    "                input_type=\"search_document\"\n",
    "            )\n",
    "            embedding_dim = len(test_response.embeddings[0])\n",
    "            print(f\"✅ Cohere test başarılı - Embedding boyutu: {embedding_dim}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Cohere bağlantı hatası: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu oluştur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa oluştur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.DIMENSION,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara böl\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni böl\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Boş chunk'ları atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings(self, texts: List[str], batch_size: int = 10) -> List[List[float]]:\n",
    "        \"\"\"Metinleri Cohere ile embedding'e çevir\"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Cohere API limitleri için batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                response = self.cohere_client.embed(\n",
    "                    texts=batch_texts,\n",
    "                    model=self.config.COHERE_MODEL,\n",
    "                    input_type=\"search_document\"  # Dokuman indexleme için\n",
    "                )\n",
    "                \n",
    "                batch_embeddings = response.embeddings\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                print(f\"  📊 Embedding oluşturuldu: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Embedding hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda boş embedding ekle\n",
    "                all_embeddings.extend([[0.0] * self.config.EMBEDDING_DIM] * len(batch_texts))\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasını işle ve chunk'ları oluştur\"\"\"\n",
    "        print(f\"📄 CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır veri yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Gerekli sütunları kontrol et\n",
    "        required_columns = ['rawText']  # Ana metin sütunu\n",
    "        optional_columns = ['esasNo', 'kararNo', 'location', 'extractedDates']\n",
    "        \n",
    "        if 'rawText' not in df.columns:\n",
    "            print(f\"❌ 'rawText' sütunu bulunamadı. Mevcut sütunlar: {df.columns.tolist()}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"🔄 Semantic chunking başlıyor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get('rawText', '') or row.get('text', '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazırla\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '')\n",
    "                #'karar_turu': row.get('karar_turu', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress göster\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'ları Qdrant'a yükle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # Embedding'leri oluştur\n",
    "        print(\"🔮 Embedding'ler oluşturuluyor...\")\n",
    "        embeddings = self.create_embeddings(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"❌ Embedding sayısı uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazırla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "        print(f\"📦 {batch_size} batch size ile yükleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "        \n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "    \n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"Semantic arama yap\"\"\"\n",
    "        print(f\"🔍 Arama: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Query için embedding oluştur\n",
    "            query_response = self.cohere_client.embed(\n",
    "                texts=[query],\n",
    "                model=self.config.COHERE_MODEL,\n",
    "                input_type=\"search_query\"  # Arama query'si için\n",
    "            )\n",
    "            query_embedding = query_response.embeddings[0]\n",
    "            \n",
    "            # Qdrant'ta ara\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_embedding,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonuçları formatla\n",
    "            results = []\n",
    "            for point in search_results:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Arama hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sınıfı\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sınıfı\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ı çalıştır\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"🚀 Yargıtay Semantic Pipeline Başlıyor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Bağlantıları test et\n",
    "        embedding_dim = self.processor.test_cohere_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon oluştur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi işle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk bulunamadı\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a yükle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri göster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"İnteraktif arama arayüzü\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY SEMANTİK ARAMA SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\n🔍 Arama metni (çıkmak için 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                print(\"👋 Görüşürüz!\")\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"📊 Kaç sonuç? (varsayılan 5): \") or \"5\")\n",
    "            except:\n",
    "                limit = 5\n",
    "            \n",
    "            # Arama yap\n",
    "            results = self.processor.search_semantic(query, limit=limit)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n📋 {len(results)} sonuç bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. 📄 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ⚖️ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   📋 Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   🏛️ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   📅 Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   🔤 Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   📝 Metin Önizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanım örneği ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfigürasyon (buraya kendi bilgilerinizi yazın)\n",
    "    config = Config(\n",
    "        COHERE_API_KEY=str(cohere_api_key),  # Cohere API anahtarınız\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",  # CSV dosya yolunuz\n",
    "        TOKEN_SIZE=384,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",  # Lokal Qdrant URL\n",
    "        COLLECTION_NAME=\"cohere_semantic_chunks\",\n",
    "        DIMENSION=1024\n",
    "    )\n",
    "    \n",
    "    # Pipeline oluştur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Menü göster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY SEMANTİK CHUNK SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline çalıştır (CSV → Semantic Chunks → Qdrant)\")\n",
    "        print(\"2. İnteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini göster\")\n",
    "        print(\"4. Çıkış\")\n",
    "        \n",
    "        choice = input(\"\\nSeçiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"✅ Pipeline başarıyla tamamlandı!\")\n",
    "            else:\n",
    "                print(\"❌ Pipeline hatası!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 Görüşürüz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon\n",
    "# Yargıtay Kararları için Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# Konfigürasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # BGE-M3 ayarları\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"  # BGE-M3 model\n",
    "    USE_FP16: bool = True  # Hafıza optimizasyonu\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # SemChunk ayarları\n",
    "    TOKEN_SIZE: int = 512  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarları\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 1024  # BGE-M3 dense embedding boyutu\n",
    "    \n",
    "    # Dosya ayarları\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 32  # BGE-M3 için optimize edilmiş batch size\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargıtay kararları için semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # GPU/CPU kontrolü\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🚀 GPU kullanılıyor: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            print(\"💻 CPU kullanılıyor\")\n",
    "        \n",
    "        # SemChunk chunker oluştur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # BGE-M3 modelini yükle\n",
    "        print(f\"🔮 BGE-M3 modeli yükleniyor... ({config.BGE_MODEL_NAME})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        \n",
    "        # Qdrant client oluştur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"✅ SemChunk chunker hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ BGE-M3 model hazır ({config.BGE_MODEL_NAME})\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_bge_connection(self):\n",
    "        \"\"\"BGE-M3 modelini test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            \n",
    "            # BGE-M3'den dense embedding al\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            embedding_dim = len(dense_embedding)\n",
    "            \n",
    "            print(f\"✅ BGE-M3 test başarılı - Dense embedding boyutu: {embedding_dim}\")\n",
    "            print(f\"🔍 Sparse embedding mevcut: {'colbert_vecs' in embeddings}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu oluştur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa oluştur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} (Boyut: {self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara böl\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni böl\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Boş chunk'ları atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        \"\"\"Metinleri BGE-M3 ile embedding'e çevir\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.BATCH_SIZE\n",
    "            \n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"🔮 BGE-M3 ile {len(texts)} metin işleniyor...\")\n",
    "        \n",
    "        # BGE-M3 için batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # BGE-M3 ile embedding oluştur\n",
    "                embeddings_result = self.bge_model.encode(batch_texts)\n",
    "                \n",
    "                # Dense embedding'leri al (1024 boyut)\n",
    "                dense_embeddings = embeddings_result['dense_vecs']\n",
    "                \n",
    "                # List formatına çevir\n",
    "                for embedding in dense_embeddings:\n",
    "                    all_embeddings.append(embedding.tolist())\n",
    "                \n",
    "                print(f\"  📊 BGE-M3 Embedding: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "                # GPU memory temizliği (gerekirse)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ BGE-M3 Embedding hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda sıfır embedding ekle\n",
    "                for _ in batch_texts:\n",
    "                    all_embeddings.append([0.0] * self.config.EMBEDDING_DIM)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasını işle ve chunk'ları oluştur\"\"\"\n",
    "        print(f\"📄 CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır veri yüklendi\")\n",
    "            print(f\"📋 Mevcut sütunlar: {df.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Ana metin sütununu belirle (öncelik sırasına göre)\n",
    "        text_columns = ['rawText', 'chunk_text', 'text', 'content', 'metin']\n",
    "        text_column = None\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"✅ Ana metin sütunu bulundu: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not text_column:\n",
    "            print(f\"❌ Ana metin sütunu bulunamadı. Kontrol edilen sütunlar: {text_columns}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"🔄 Semantic chunking başlıyor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get(text_column, '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                print(f\"⚠️ Satır {idx}: Boş metin atlandı\")\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazırla (CSV yapınıza göre güncellenmiş)\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress göster\n",
    "            if (idx + 1) % 5 == 0:  # Daha sık progress göster (az veri olduğu için)\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'ları Qdrant'a yükle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # BGE-M3 ile embedding'leri oluştur\n",
    "        print(\"🔮 BGE-M3 embedding'ler oluşturuluyor...\")\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"❌ Embedding sayısı uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazırla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "        print(f\"📦 {batch_size} batch size ile yükleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "        \n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "    \n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"BGE-M3 ile semantic arama yap\"\"\"\n",
    "        print(f\"🔍 Arama: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vektörize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Qdrant'ta ara (güncel query_points metodu)\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonuçları formatla\n",
    "            results = []\n",
    "            for point in search_results.points:#burda muhtemel hata verir search_results olcak verirse\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Arama hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"Filtreli arama yap\"\"\"\n",
    "        print(f\"🔍 Filtreli arama: '{query}' - Filtreler: {filters}\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vektörize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Filter oluştur\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = []\n",
    "                for key, value in filters.items():\n",
    "                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "                query_filter = Filter(must=conditions)\n",
    "            \n",
    "            # Qdrant'ta filtreli arama yap\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonuçları formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"📊 {len(results)} filtreli sonuç bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Filtreli arama hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sınıfı\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sınıfı\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ı çalıştır\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"🚀 Yargıtay BGE-M3 Semantic Pipeline Başlıyor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon oluştur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi işle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk bulunamadı\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a yükle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri göster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"İnteraktif arama arayüzü\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK ARAMA SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n🔍 Arama Seçenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana menüye dön\")\n",
    "            \n",
    "            search_choice = input(\"Seçiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"❌ Geçersiz seçim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\n🔍 Arama metni (çıkmak için 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"📊 Kaç sonuç? (varsayılan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"🎯 Minimum benzerlik skoru? (varsayılan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\n🔧 Filtre Seçenekleri (boş bırakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (örn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n📋 {len(results)} sonuç bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. 📄 BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ⚖️ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   📋 Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   🏛️ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   📅 Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   🔤 Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   📝 Metin Önizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanım örneği ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfigürasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=1024,\n",
    "        BATCH_SIZE=16,  # GPU memory'ye göre ayarlayın\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline oluştur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Menü göster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\")\n",
    "        print(\"2. İnteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini göster\")\n",
    "        print(\"4. Çıkış\")\n",
    "        \n",
    "        choice = input(\"\\nSeçiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"✅ BGE-M3 Pipeline başarıyla tamamlandı!\")\n",
    "            else:\n",
    "                print(\"❌ Pipeline hatası!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 Görüşürüz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrolü\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"✅ FlagEmbedding kütüphanesi yüklü\")\n",
    "    except ImportError:\n",
    "        print(\"❌ FlagEmbedding kütüphanesi bulunamadı!\")\n",
    "        print(\"Kurulum için: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "✅ FlagEmbedding kütüphanesi yüklü\n",
      "🚀 GPU kullanılıyor: NVIDIA RTX A6000\n",
      "🔮 BGE-M3 modeli yükleniyor... (BAAI/bge-m3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 50091.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SemChunk chunker hazır (Token boyutu: 512)\n",
      "✅ BGE-M3 model hazır (BAAI/bge-m3)\n",
      "✅ Qdrant client hazır (http://localhost:6333)\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK ARAMA SİSTEMİ\n",
      "==================================================\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n",
      "🔍 Arama: 'ihtiyati tedbir tazminat'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 5 sonuç bulundu\n",
      "\n",
      "📋 5 sonuç bulundu:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. 📄 BGE-M3 Benzerlik Skoru: 0.668\n",
      "   ⚖️ Esas No: 2022/3281 E.\n",
      "   📋 Karar No: 2024/117 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   🔤 Token: 413\n",
      "   📝 Metin Önizleme:\n",
      "      maddesi, ihtiyati tedbir kararının haksız olduğunun belirlenmesi halinde tedbir kararı yüzünden uğranılan zararın tazminini düzenlediğini, ihtiyati tedbir kararını icra ettiren tarafın yasal sürede dava açmaması halinde ihtiyati tedbirin haksız konulduğunun kabulü gerektiği, kaldı ki süresinde dava ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. 📄 BGE-M3 Benzerlik Skoru: 0.649\n",
      "   ⚖️ Esas No: 2022/3281 E.\n",
      "   📋 Karar No: 2024/117 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   🔤 Token: 431\n",
      "   📝 Metin Önizleme:\n",
      "      3. Değerlendirme Mahkemece, tazminat davalısının ödemekle yükümlü olduğu miktarın uyulmasına karar verilen Yargıtay ilamında da belirtildiği üzer ihtiyati tedbir kararının icra edildiği tarih ile ihtiyati tedbirin kalktığı ya da kalkmış sayıldığı tarih arasındaki zarar olduğu, ihtiyati tedbir kararı...\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. 📄 BGE-M3 Benzerlik Skoru: 0.620\n",
      "   ⚖️ Esas No: 2022/3281 E.\n",
      "   📋 Karar No: 2024/117 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   🔤 Token: 290\n",
      "   📝 Metin Önizleme:\n",
      "      maddesi hükmüne aykırı olarak ihtiyati tedbire ilişkin karar tarihinden itibaren 10 gün içinde dava açılmamış olduğu, haksız ihtiyati tedbirden dolayı olan sorumluluğun kusursuz sorumluluk olduğu, yani, haksız ihtiyati tedbir koydurtmuş olan tarafın, bundan doğan maddi zararla sorumlu tutulabilmesi ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. 📄 BGE-M3 Benzerlik Skoru: 0.609\n",
      "   ⚖️ Esas No: 2022/3281 E.\n",
      "   📋 Karar No: 2024/117 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   🔤 Token: 409\n",
      "   📝 Metin Önizleme:\n",
      "      maddesindeki on günlük süre içerisinde esas hakkında dava açmazsa, ihtiyati tedbirin haksız konulmuş sayılacağı, haksız ihtiyati tedbirden dolayı tazminat davası açan davacının ödenmesini istediği zararı ile haksız ihtiyati tedbir arasında uygun illiyet (nedensellik) bağı (sebep sonuç ilişkisi) bulu...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. 📄 BGE-M3 Benzerlik Skoru: 0.607\n",
      "   ⚖️ Esas No: 2022/3281 E.\n",
      "   📋 Karar No: 2024/117 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   🔤 Token: 422\n",
      "   📝 Metin Önizleme:\n",
      "      6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasındaki tazminat davasından dolayı yapılan yargılama sonunda İlk Derece Mahkemesince davanın reddine karar verilmiştir. İlk Derece Mahkemesi kararı davacılar vekilince temyiz edilmekle; kesin...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n",
      "🔍 Arama: 'inşaat davası'\n",
      "📊 5 sonuç bulundu\n",
      "\n",
      "📋 5 sonuç bulundu:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. 📄 BGE-M3 Benzerlik Skoru: 0.575\n",
      "   ⚖️ Esas No: 2022/3993 E.\n",
      "   📋 Karar No: 2024/775 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 04.02.2011,07.02.2011,15.07.2012,22.10.2014,16.11.2017,22.09.2020,02.12.2020,17.02.2022,15.07.2012,16.11.2017,12.07.2018,12.07.2018,08.06.2022,28.03.2024\n",
      "   🔤 Token: 492\n",
      "   📝 Metin Önizleme:\n",
      "      Bölge Adliye Mahkemesinin yukarıda belirtilen kararına karşı süresi içinde davalı ... vekili temyiz isteminde bulunmuştur. 2. Dairemizin 17.02.2022 tarihli ve 2021/2532 Esas, 2022/901 Karar sayılı ilamıyla; \"Mahkemece davalı ... hakkındaki tapu iptali ve tescil kararı davalı yüklenici şirketin arsa ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. 📄 BGE-M3 Benzerlik Skoru: 0.566\n",
      "   ⚖️ Esas No: 2022/3993 E.\n",
      "   📋 Karar No: 2024/775 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 04.02.2011,07.02.2011,15.07.2012,22.10.2014,16.11.2017,22.09.2020,02.12.2020,17.02.2022,15.07.2012,16.11.2017,12.07.2018,12.07.2018,08.06.2022,28.03.2024\n",
      "   🔤 Token: 153\n",
      "   📝 Metin Önizleme:\n",
      "      kişilere devretse bile 3. kişi ve daha sonraki devralanların iyiniyet savunmasında bulunmasının mümkün olmadığını, davalı ...’ın yükleniciye kat karşılığı inşaat sözleşmesi gereği avans olarak verilmiş arsa üzerine yapılmış binadan bağımsız bölüm edinmeyi amaçladığını, bunun için de diğer davalı ......\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. 📄 BGE-M3 Benzerlik Skoru: 0.565\n",
      "   ⚖️ Esas No: 2022/3281 E.\n",
      "   📋 Karar No: 2024/117 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   🔤 Token: 409\n",
      "   📝 Metin Önizleme:\n",
      "      maddesindeki on günlük süre içerisinde esas hakkında dava açmazsa, ihtiyati tedbirin haksız konulmuş sayılacağı, haksız ihtiyati tedbirden dolayı tazminat davası açan davacının ödenmesini istediği zararı ile haksız ihtiyati tedbir arasında uygun illiyet (nedensellik) bağı (sebep sonuç ilişkisi) bulu...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. 📄 BGE-M3 Benzerlik Skoru: 0.560\n",
      "   ⚖️ Esas No: 2022/3281 E.\n",
      "   📋 Karar No: 2024/117 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   🔤 Token: 422\n",
      "   📝 Metin Önizleme:\n",
      "      6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasındaki tazminat davasından dolayı yapılan yargılama sonunda İlk Derece Mahkemesince davanın reddine karar verilmiştir. İlk Derece Mahkemesi kararı davacılar vekilince temyiz edilmekle; kesin...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. 📄 BGE-M3 Benzerlik Skoru: 0.556\n",
      "   ⚖️ Esas No: 2023/576 E.\n",
      "   📋 Karar No: 2024/399 K.\n",
      "   🏛️ Daire: 6.HukukDairesisi\n",
      "   📅 Tarih: 30.01.2024,15.07.2017,24.04.2013,24.01.2023,30.01.2024\n",
      "   🔤 Token: 448\n",
      "   📝 Metin Önizleme:\n",
      "      vekili cevap dilekçesinde özetle; ....A.Ş.'nin diğer davalı şirket ile yapmış olduğu sözleşme çerçevesinde davacıya dava konusu villanın satıldığını ve teslim edildiğini, dava konusu villanın tapusunun diğer davalı şirket üzerinde olup davacıya tapu devrinin gerektiğini savunarak, davanın reddini is...\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "👋 Görüşürüz!\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon\n",
    "# Yargıtay Kararları için Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# Konfigürasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # BGE-M3 ayarları\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"  # BGE-M3 model\n",
    "    USE_FP16: bool = True  # Hafıza optimizasyonu\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # SemChunk ayarları\n",
    "    TOKEN_SIZE: int = 512  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarları\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 1024  # BGE-M3 dense embedding boyutu\n",
    "    \n",
    "    # Dosya ayarları\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 32  # BGE-M3 için optimize edilmiş batch size\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargıtay kararları için semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # GPU/CPU kontrolü\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🚀 GPU kullanılıyor: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            print(\"💻 CPU kullanılıyor\")\n",
    "        \n",
    "        # SemChunk chunker oluştur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # BGE-M3 modelini yükle\n",
    "        print(f\"🔮 BGE-M3 modeli yükleniyor... ({config.BGE_MODEL_NAME})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        \n",
    "        # Qdrant client oluştur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"✅ SemChunk chunker hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ BGE-M3 model hazır ({config.BGE_MODEL_NAME})\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_bge_connection(self):\n",
    "        \"\"\"BGE-M3 modelini test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            \n",
    "            # BGE-M3'den dense embedding al\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            embedding_dim = len(dense_embedding)\n",
    "            \n",
    "            print(f\"✅ BGE-M3 test başarılı - Dense embedding boyutu: {embedding_dim}\")\n",
    "            print(f\"🔍 Sparse embedding mevcut: {'colbert_vecs' in embeddings}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu oluştur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa oluştur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} (Boyut: {self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara böl\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni böl\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Boş chunk'ları atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None) -> List[List[float]]:\n",
    "        \"\"\"Metinleri BGE-M3 ile embedding'e çevir\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.BATCH_SIZE\n",
    "            \n",
    "        all_embeddings = []\n",
    "        \n",
    "        print(f\"🔮 BGE-M3 ile {len(texts)} metin işleniyor...\")\n",
    "        \n",
    "        # BGE-M3 için batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # BGE-M3 ile embedding oluştur\n",
    "                embeddings_result = self.bge_model.encode(batch_texts)\n",
    "                \n",
    "                # Dense embedding'leri al (1024 boyut)\n",
    "                dense_embeddings = embeddings_result['dense_vecs']\n",
    "                \n",
    "                # List formatına çevir\n",
    "                for embedding in dense_embeddings:\n",
    "                    all_embeddings.append(embedding.tolist())\n",
    "                \n",
    "                print(f\"  📊 BGE-M3 Embedding: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "                # GPU memory temizliği (gerekirse)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ BGE-M3 Embedding hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda sıfır embedding ekle\n",
    "                for _ in batch_texts:\n",
    "                    all_embeddings.append([0.0] * self.config.EMBEDDING_DIM)\n",
    "        \n",
    "        return all_embeddings\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasını işle ve chunk'ları oluştur\"\"\"\n",
    "        print(f\"📄 CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır veri yüklendi\")\n",
    "            print(f\"📋 Mevcut sütunlar: {df.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Ana metin sütununu belirle (öncelik sırasına göre)\n",
    "        text_columns = ['rawText', 'chunk_text', 'text', 'content', 'metin']\n",
    "        text_column = None\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"✅ Ana metin sütunu bulundu: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not text_column:\n",
    "            print(f\"❌ Ana metin sütunu bulunamadı. Kontrol edilen sütunlar: {text_columns}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"🔄 Semantic chunking başlıyor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get(text_column, '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                print(f\"⚠️ Satır {idx}: Boş metin atlandı\")\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazırla (CSV yapınıza göre güncellenmiş)\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', '') or row.get('esas_no', ''),\n",
    "                'karar_no': row.get('kararNo', '') or row.get('karar_no', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', '') or row.get('dates', ''),\n",
    "                'esas_no_num': row.get('esasNo_num', ''),\n",
    "                'esas_no_tip': row.get('esasNo_tip', ''),\n",
    "                'karar_no_num': row.get('kararNo_num', ''),\n",
    "                'karar_no_tip': row.get('kararNo_tip', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress göster\n",
    "            if (idx + 1) % 5 == 0:  # Daha sık progress göster (az veri olduğu için)\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'ları Qdrant'a yükle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # BGE-M3 ile embedding'leri oluştur\n",
    "        print(\"🔮 BGE-M3 embedding'ler oluşturuluyor...\")\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"❌ Embedding sayısı uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazırla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "        print(f\"📦 {batch_size} batch size ile yükleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "        \n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "    \n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"BGE-M3 ile semantic arama yap\"\"\"\n",
    "        print(f\"🔍 Arama: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vektörize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Qdrant'ta ara (güncel query_points metodu)\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonuçları formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Arama hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"Filtreli arama yap\"\"\"\n",
    "        print(f\"🔍 Filtreli arama: '{query}' - Filtreler: {filters}\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vektörize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Filter oluştur\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = []\n",
    "                for key, value in filters.items():\n",
    "                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "                query_filter = Filter(must=conditions)\n",
    "            \n",
    "            # Qdrant'ta filtreli arama yap\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonuçları formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"📊 {len(results)} filtreli sonuç bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Filtreli arama hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sınıfı\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sınıfı\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ı çalıştır\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"🚀 Yargıtay BGE-M3 Semantic Pipeline Başlıyor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon oluştur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi işle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk bulunamadı\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a yükle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri göster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"İnteraktif arama arayüzü\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK ARAMA SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n🔍 Arama Seçenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana menüye dön\")\n",
    "            \n",
    "            search_choice = input(\"Seçiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"❌ Geçersiz seçim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\n🔍 Arama metni (çıkmak için 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"📊 Kaç sonuç? (varsayılan 5): \") or \"5\")\n",
    "                threshold = float(input(\"🎯 Minimum benzerlik skoru? (varsayılan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit, score_threshold=threshold)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\n🔧 Filtre Seçenekleri (boş bırakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (örn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit, score_threshold=threshold\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n📋 {len(results)} sonuç bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. 📄 BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ⚖️ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   📋 Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   🏛️ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   📅 Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   🔤 Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   📝 Metin Önizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanım örneği ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfigürasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=1024,\n",
    "        BATCH_SIZE=16,  # GPU memory'ye göre ayarlayın\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline oluştur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Menü göster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\")\n",
    "        print(\"2. İnteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini göster\")\n",
    "        print(\"4. Çıkış\")\n",
    "        \n",
    "        choice = input(\"\\nSeçiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"✅ BGE-M3 Pipeline başarıyla tamamlandı!\")\n",
    "            else:\n",
    "                print(\"❌ Pipeline hatası!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 Görüşürüz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrolü\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"✅ FlagEmbedding kütüphanesi yüklü\")\n",
    "    except ImportError:\n",
    "        print(\"❌ FlagEmbedding kütüphanesi bulunamadı!\")\n",
    "        print(\"Kurulum için: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
