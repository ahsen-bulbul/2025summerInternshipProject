{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "‚úÖ FlagEmbedding y√ºkl√º\n",
      "üîÆ BGE-M3 y√ºkleniyor: BAAI/bge-m3 (device=cuda)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 8080.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hazƒ±r - Cihaz: NVIDIA RTX A6000\n",
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) √áƒ±kƒ±≈ü\n",
      "üöÄ Full pipeline ba≈ülƒ±yor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dense embedding boyutu: 1024\n",
      "üîç Sparse embedding mevcut: True\n",
      "üóëÔ∏è Eski koleksiyon silindi: bge_hybrid_chunks\n",
      "‚úÖ Koleksiyon olu≈üturuldu: bge_hybrid_chunks (Dense+Sparse)\n",
      "üìÑ CSV okunuyor: /home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\n",
      "üìä 10 satƒ±r y√ºklendi\n",
      "  ‚úÖ ƒ∞≈ülenen satƒ±r: 5/10 (Toplam chunk: 44)\n",
      "  ‚úÖ ƒ∞≈ülenen satƒ±r: 10/10 (Toplam chunk: 59)\n",
      "üß© Toplam 59 chunk olu≈üturuldu\n",
      "üöÄ 59 chunk Qdrant'a y√ºkleniyor...\n",
      "üîÆ 59 metin i≈üleniyor (batch_size=100)...\n",
      "‚ùå Embedding hatasƒ± (batch 1): must be real number, not NoneType\n",
      "  ‚úÖ Batch y√ºklendi: 59/59\n",
      "üéâ Y√ºkleme tamamlandƒ±!\n",
      "\n",
      "üìä Koleksiyon Bilgileri:\n",
      "{\n",
      "  \"collection_name\": \"bge_hybrid_chunks\",\n",
      "  \"points_count\": 59,\n",
      "  \"vectors_count\": null,\n",
      "  \"status\": \"green\",\n",
      "  \"embedding_model\": \"BGE-M3\",\n",
      "  \"embedding_dim\": 512\n",
      "}\n",
      "‚úÖ Tamamlandƒ±\n",
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) √áƒ±kƒ±≈ü\n",
      "\n",
      "üîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana men√º\n",
      "‚ùå Arama hatasƒ±: must be real number, not NoneType\n",
      "‚ùå Sonu√ß bulunamadƒ±\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana men√º\n",
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) √áƒ±kƒ±≈ü\n",
      "üëã G√∂r√º≈ü√ºr√ºz\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Dense + Sparse, 512 dim slice, L2 normalize, hibrit search)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense+sparse\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            sparse_available = 'colbert_vecs' in emb_res\n",
    "            print(f\"‚úÖ Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {sparse_available}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse i√ßin yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE),\n",
    "                    \"sparse_vec\": VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "\n",
    "                dense = emb_res.get('dense_vecs', emb_res)\n",
    "                sparse = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)]*len(batch_texts))\n",
    "\n",
    "                dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                sparse_t = torch.tensor(sparse, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dense_slice = dense_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    dense_norm = l2_normalize_tensor(dense_slice)\n",
    "                    sparse_slice = sparse_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    sparse_norm = l2_normalize_tensor(sparse_slice)\n",
    "\n",
    "                all_embeddings_dense.extend([v.cpu().tolist() for v in dense_norm])\n",
    "                all_embeddings_sparse.extend([v.cpu().tolist() for v in sparse_norm])\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings_bge(texts)\n",
    "\n",
    "        points = []\n",
    "        \n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vectors={'dense_vec': d, 'sparse_vec': s} \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            if dense_q is None or any(v is None for v in dense_q[0]):\n",
    "                dense_q = [np.zeros(self.config.EMBEDDING_DIM)]\n",
    "\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "            if sparse_q is None or any(v is None for v in sparse_q[0]):\n",
    "                sparse_q = [np.zeros(self.config.EMBEDDING_DIM)]\n",
    "\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=None,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                # Sparse hibrit param Qdrant 1.2+\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k,v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana men√º\")\n",
    "            ch = input(\"Se√ßiminiz (1-3): \").strip()\n",
    "            if ch==\"3\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (√∂rn: '6.HukukDairesi', bo≈ü = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit)\n",
    "\n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß:\")\n",
    "            for i,r in enumerate(results,1):\n",
    "                p=r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text',''))>300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_hybrid_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding bulunamadƒ± ‚Äî pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
