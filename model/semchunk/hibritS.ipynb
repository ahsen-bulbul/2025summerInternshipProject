{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full kod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "üîÆ BGE-M3 y√ºkleniyor: BAAI/bge-m3 (device=cuda)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 277156.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hazƒ±r - Cihaz: NVIDIA RTX A6000\n",
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) √áƒ±kƒ±≈ü\n",
      "üöÄ Full pipeline ba≈ülƒ±yor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dense embedding boyutu: 1024\n",
      "üîç Sparse embedding mevcut: True\n",
      "üóëÔ∏è Eski koleksiyon silindi: bge_hybrid_chunks\n",
      "‚úÖ Koleksiyon olu≈üturuldu: bge_hybrid_chunks (Dense+Sparse)\n",
      "üìÑ CSV okunuyor: /home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\n",
      "üìä 10 satƒ±r y√ºklendi\n",
      "  ‚úÖ ƒ∞≈ülenen satƒ±r: 5/10 (Toplam chunk: 44)\n",
      "  ‚úÖ ƒ∞≈ülenen satƒ±r: 10/10 (Toplam chunk: 59)\n",
      "üß© Toplam 59 chunk olu≈üturuldu\n",
      "üöÄ 59 chunk Qdrant'a y√ºkleniyor...\n",
      "üîÆ 59 metin i≈üleniyor (batch_size=100)...\n",
      "  üìä Batch i≈ülendi: 59/59\n",
      "  ‚úÖ Batch y√ºklendi: 59/59\n",
      "üéâ Y√ºkleme tamamlandƒ±!\n",
      "\n",
      "üìä Koleksiyon Bilgileri:\n",
      "{\n",
      "  \"collection_name\": \"bge_hybrid_chunks\",\n",
      "  \"points_count\": 59,\n",
      "  \"vectors_count\": null,\n",
      "  \"status\": \"green\",\n",
      "  \"embedding_model\": \"BGE-M3\",\n",
      "  \"embedding_dim\": 512\n",
      "}\n",
      "‚úÖ Tamamlandƒ±\n",
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) √áƒ±kƒ±≈ü\n",
      "\n",
      "üîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana men√º\n",
      "üìä 5 sonu√ß bulundu (Dense only)\n",
      "\n",
      "üìã 5 sonu√ß:\n",
      "\n",
      "1. Skor: 0.6925\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 3. Deƒüerlendirme Mahkemece, tazminat davalƒ±sƒ±nƒ±n √∂demekle y√ºk√ºml√º olduƒüu miktarƒ±n uyulmasƒ±na karar verilen Yargƒ±tay ilamƒ±nda da belirtildiƒüi √ºzer ihtiyati tedbir kararƒ±nƒ±n icra edildiƒüi tarih ile ihtiyati tedbirin kalktƒ±ƒüƒ± ya da kalkmƒ±≈ü sayƒ±ldƒ±ƒüƒ± tarih arasƒ±ndaki zarar olduƒüu, ihtiyati tedbir kararƒ±...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Skor: 0.6892\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi, ihtiyati tedbir kararƒ±nƒ±n haksƒ±z olduƒüunun belirlenmesi halinde tedbir kararƒ± y√ºz√ºnden uƒüranƒ±lan zararƒ±n tazminini d√ºzenlediƒüini, ihtiyati tedbir kararƒ±nƒ± icra ettiren tarafƒ±n yasal s√ºrede dava a√ßmamasƒ± halinde ihtiyati tedbirin haksƒ±z konulduƒüunun kabul√º gerektiƒüi, kaldƒ± ki s√ºresinde dava ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Skor: 0.6577\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±ndaki tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda ƒ∞lk Derece Mahkemesince davanƒ±n reddine karar verilmi≈ütir. ƒ∞lk Derece Mahkemesi kararƒ± davacƒ±lar vekilince temyiz edilmekle; kesin...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. Skor: 0.6522\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi h√ºkm√ºne aykƒ±rƒ± olarak ihtiyati tedbire ili≈ükin karar tarihinden itibaren 10 g√ºn i√ßinde dava a√ßƒ±lmamƒ±≈ü olduƒüu, haksƒ±z ihtiyati tedbirden dolayƒ± olan sorumluluƒüun kusursuz sorumluluk olduƒüu, yani, haksƒ±z ihtiyati tedbir koydurtmu≈ü olan tarafƒ±n, bundan doƒüan maddi zararla sorumlu tutulabilmesi ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. Skor: 0.6519\n",
      "   Daire: 6.HukukDairesisi | Tarih: 18.01.2024\n",
      "   Metin: 6. Hukuk Dairesi 2023/596 E. , 2024/257 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ : ... B√∂lge Adliye Mahkemesi 13. Hukuk Dairesi Taraflar arasƒ±ndaki r√ºcuen tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda, ƒ∞lk Derece Mahkemesince davanƒ±n kabul√ºne karar verilmi≈ütir. Kararƒ±n davacƒ± vekili tarafƒ±ndan istinaf...\n",
      "------------------------------------------------------------\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana men√º\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657479/765771685.py:272: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  qr = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä 5 sonu√ß bulundu (Dense + TF-IDF Sparse)\n",
      "SONU√áLAR:[{'score': 0.673231, 'payload': {'chunk_id': 1, 'text': 'maddesi, ihtiyati tedbir kararƒ±nƒ±n haksƒ±z olduƒüunun belirlenmesi halinde tedbir kararƒ± y√ºz√ºnden uƒüranƒ±lan zararƒ±n tazminini d√ºzenlediƒüini, ihtiyati tedbir kararƒ±nƒ± icra ettiren tarafƒ±n yasal s√ºrede dava a√ßmamasƒ± halinde ihtiyati tedbirin haksƒ±z konulduƒüunun kabul√º gerektiƒüi, kaldƒ± ki s√ºresinde dava a√ßsa da durumun deƒüi≈ümeyeceƒüini belirterek m√ºvekkillerinin in≈üaatƒ±nƒ±n ge√ß bitirilmesinden kaynaklƒ± 10.000,00 TL maddi tazminatƒ±n tahsiline karar verilmesini talep etmi≈ütir. 2.Davacƒ± vekili duru≈ümadaki beyanƒ±nda; tedbir sebebiyle baƒüƒ±msƒ±z b√∂l√ºmlerinin ge√ß teslim edileceƒüini, bundan kaynaklƒ± doƒüacak zararlarƒ± talep ettiklerini beyan etmi≈ütir. II. CEVAP Davalƒ± vekili cevap dilek√ßesinde √∂zetle; ekonomik nedenlerle 10 g√ºn i√ßerisinde dava a√ßamadƒ±klarƒ±nƒ±, 19 g√ºn sonra a√ßtƒ±klarƒ± davanƒ±n ... 4. Asliye Hukuk Mahkemesinin 2010/27 Esas sayƒ±lƒ± sƒ±rasƒ±nda kayƒ±tlƒ± olduƒüunu belirterek davanƒ±n reddini talep etmi≈ütir. III. MAHKEME KARARI Mahkemenin 30.12.2011 tarihli ve 2010/695 Esas, 2011/916 Karar sayƒ±lƒ± kararƒ± ile ... 1.Asliye Hukuk Mahkemesinin 2009/139 D....', 'token_count': 413, 'char_count': 1054, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}, {'score': 0.66766644, 'payload': {'chunk_id': 10, 'text': '3. Deƒüerlendirme Mahkemece, tazminat davalƒ±sƒ±nƒ±n √∂demekle y√ºk√ºml√º olduƒüu miktarƒ±n uyulmasƒ±na karar verilen Yargƒ±tay ilamƒ±nda da belirtildiƒüi √ºzer ihtiyati tedbir kararƒ±nƒ±n icra edildiƒüi tarih ile ihtiyati tedbirin kalktƒ±ƒüƒ± ya da kalkmƒ±≈ü sayƒ±ldƒ±ƒüƒ± tarih arasƒ±ndaki zarar olduƒüu, ihtiyati tedbir kararƒ±nƒ±n kalkmƒ±≈ü sayƒ±ldƒ±ƒüƒ± tarih ile icra edildiƒüi tarihin aynƒ± olduƒüu, arada s√ºre zarfƒ± bulunmamasƒ± nedeniyle yapƒ±lmasƒ± talep edilen ke≈üifte yoksun kalƒ±nan kira kaybƒ±na ili≈ükin hesaplama yapƒ±lamayacaƒüƒ±ndan yargƒ±lamaya bir katkƒ± saƒülamayacaƒüƒ± kanaatine varƒ±larak davanƒ±n reddine karar verildiƒüi, somut olayda; 05.01.2010 tarihinde tedbirin icra edildiƒüi, 14.01.2010 tarihinde ise kaldƒ±rƒ±ldƒ±ƒüƒ± anla≈üƒ±lmaktadƒ±r. Bu durumda mahkemece ihtiyati tedbir kararƒ±nƒ±n kalkmƒ±≈ü sayƒ±ldƒ±ƒüƒ± tarih ile icra edildiƒüi tarihin aynƒ± olduƒüuna dair karar hatalƒ± olup, 05.01.2010 tarihi ile 14.01.2010 tarihleri arasƒ±nda davacƒ±nƒ±n maddi zararlarƒ±nƒ± ve illiyet baƒüƒ±nƒ± ispat imkanƒ± tanƒ±nmasƒ± ve sonucuna g√∂re karar verilmesi gerekirken, davanƒ±n reddine karar verilmesi hatalƒ± olmu≈ü, kararƒ±n bozulmasƒ±na karar verilmi≈ütir. VI.', 'token_count': 431, 'char_count': 1094, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}, {'score': 0.6383259, 'payload': {'chunk_id': 0, 'text': '6. Hukuk Dairesi 2022/4600 E. , 2024/467 K. \\\\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±nda g√∂r√ºlen r√ºcuen tazminat davasƒ±nda verilen karar hakkƒ±nda yapƒ±lan temyiz incelemesi sonucunda, Dairece Mahkeme kararƒ±nƒ±n bozulmasƒ±na karar verilmi≈ütir. Mahkemece bozmaya uyularak yeniden yapƒ±lan yargƒ±lama sonucunda; davanƒ±n kƒ±smen kabul√ºne karar verilmi≈ütir. Mahkeme kararƒ±, davacƒ± ve davalƒ± ....Ltd.≈ûti. vekili tarafƒ±ndan temyiz edilmekle; kesinlik, s√ºre, temyiz ≈üartƒ± ve diƒüer usul eksiklikleri y√∂n√ºnden yapƒ±lan √∂n inceleme sonucunda, temyiz dilek√ßesinin kabul√ºne karar verildikten ve Tetkik H√¢kimi tarafƒ±ndan hazƒ±rlanan rapor dinlendikten sonra dosyadaki belgeler incelenip gereƒüi d√º≈ü√ºn√ºld√º: I. DAVA Davacƒ± vekili dava dilek√ßesinde; m√ºvekkili ile davalƒ± y√ºklenici ≈üirketler arasƒ±ndaki hizmet alƒ±m s√∂zle≈ümeleri kapsamƒ±nda √ßalƒ±≈üan i≈ü√ßilere, ... mahkemesi ilamlarƒ±na dayalƒ± olarak ba≈ülatƒ±lan icra takipleri sonucunda, asƒ±l i≈üveren konumundaki m√ºvekkili tarafƒ±ndan √∂demeler yapƒ±ldƒ±ƒüƒ±nƒ±, bu √∂demelerden davalƒ±larƒ±n sorumlu olduklarƒ±nƒ± ileri s√ºrerek, toplam 79.248,74 TL‚Äônin √∂deme tarihlerinden itibaren i≈üleyecek reeskont faizi ile birlikte davalƒ±lardan m√º≈ütereken ve m√ºteselsilen tahsilini talep ve dava etmi≈ütir. II. CEVAP Davalƒ± ... Saƒülƒ±k Hiz. Oto. Gƒ±d. Tem. Nak. San. ve Tic. Ltd. ≈ûti.', 'token_count': 508, 'char_count': 1302, 'original_index': 7, 'esas_no': '2022/4600 E.', 'karar_no': '2024/467 K.', 'daire': '6.HukukDairesisi', 'tarih': '15.12.2015,03.03.2020,01.02.2024', 'document_id': '6750524485b6640290c37b0d'}}, {'score': 0.6316994, 'payload': {'chunk_id': 0, 'text': '6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\\\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±ndaki tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda ƒ∞lk Derece Mahkemesince davanƒ±n reddine karar verilmi≈ütir. ƒ∞lk Derece Mahkemesi kararƒ± davacƒ±lar vekilince temyiz edilmekle; kesinlik, s√ºre, temyiz ≈üartƒ± ve diƒüer usul eksiklikleri y√∂n√ºnden yapƒ±lan √∂n inceleme sonucunda, temyiz dilek√ßesinin kabul√ºne karar verildikten ve Tetkik H√¢kimi tarafƒ±ndan hazƒ±rlanan rapor dinlendikten sonra dosyadaki belgeler incelenip gereƒüi d√º≈ü√ºn√ºld√º: I. DAVA 1.Davacƒ± vekili dava dilek√ßesinde √∂zetle; m√ºvekkillerinin arsa sahibi olduƒüunu, ...\\'ƒ±n m√ºteahhit olarak i≈übu arsa √ºzerine yapmƒ±≈ü olduƒüu binanƒ±n ... 1. Asliye Hukuk Mahkemesinin 2009/139 D. ... dosyasƒ± √ºzerinden ihtiyati tedbir konularak in≈üaatƒ±n durdurulduƒüunu, HUMK\\'un 109. maddesi gereƒüince ihtiyati tedbir kararƒ±nƒ±n verildiƒüi tarihten itibaren 10 g√ºn i√ßerisinde esas hakkƒ±nda davanƒ±n a√ßƒ±lmasƒ± gerektiƒüini ve bu durumun dosyaya ibrazƒ± gerekirken davanƒ±n a√ßƒ±lmadƒ±ƒüƒ±nƒ±, Hukuk Usul√º Muhakemeleri Kanununun 110.', 'token_count': 422, 'char_count': 1066, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}, {'score': 0.6258817, 'payload': {'chunk_id': 6, 'text': \"maddesi h√ºkm√ºne aykƒ±rƒ± olarak ihtiyati tedbire ili≈ükin karar tarihinden itibaren 10 g√ºn i√ßinde dava a√ßƒ±lmamƒ±≈ü olduƒüu, haksƒ±z ihtiyati tedbirden dolayƒ± olan sorumluluƒüun kusursuz sorumluluk olduƒüu, yani, haksƒ±z ihtiyati tedbir koydurtmu≈ü olan tarafƒ±n, bundan doƒüan maddi zararla sorumlu tutulabilmesi i√ßin, ihtiyati tedbiri k√∂t√ºniyetle istemi≈ü ve koydurmu≈ü olmasƒ± veya bunda herhangi bir ihmalinin bulunmasƒ±nƒ±n ≈üart olmadƒ±ƒüƒ±, ihtiyati tedbir haksƒ±z ve bundan da bir zarar doƒümu≈ü ise, bu haksƒ±z ihtiyati tedbiri koydurtmu≈ü olan tarafƒ±n kusurlu olmasa bile bundan zarar g√∂ren kar≈üƒ± tarafa veya √º√ß√ºnc√º ki≈üiye tazminat √∂demekle y√ºk√ºml√º olduƒüu, ihtiyati tedbir isteyen ve bu kararƒ± icra ettiren ki≈üinin, ihtiyati tedbiri icra ettirdiƒüi halde, HUMK'un 109.\", 'token_count': 290, 'char_count': 749, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}]\n",
      "\n",
      "üìã 5 sonu√ß:\n",
      "\n",
      "1. Skor: 0.6732\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi, ihtiyati tedbir kararƒ±nƒ±n haksƒ±z olduƒüunun belirlenmesi halinde tedbir kararƒ± y√ºz√ºnden uƒüranƒ±lan zararƒ±n tazminini d√ºzenlediƒüini, ihtiyati tedbir kararƒ±nƒ± icra ettiren tarafƒ±n yasal s√ºrede dava a√ßmamasƒ± halinde ihtiyati tedbirin haksƒ±z konulduƒüunun kabul√º gerektiƒüi, kaldƒ± ki s√ºresinde dava ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Skor: 0.6677\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 3. Deƒüerlendirme Mahkemece, tazminat davalƒ±sƒ±nƒ±n √∂demekle y√ºk√ºml√º olduƒüu miktarƒ±n uyulmasƒ±na karar verilen Yargƒ±tay ilamƒ±nda da belirtildiƒüi √ºzer ihtiyati tedbir kararƒ±nƒ±n icra edildiƒüi tarih ile ihtiyati tedbirin kalktƒ±ƒüƒ± ya da kalkmƒ±≈ü sayƒ±ldƒ±ƒüƒ± tarih arasƒ±ndaki zarar olduƒüu, ihtiyati tedbir kararƒ±...\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Skor: 0.6383\n",
      "   Daire: 6.HukukDairesisi | Tarih: 15.12.2015,03.03.2020,01.02.2024\n",
      "   Metin: 6. Hukuk Dairesi 2022/4600 E. , 2024/467 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±nda g√∂r√ºlen r√ºcuen tazminat davasƒ±nda verilen karar hakkƒ±nda yapƒ±lan temyiz incelemesi sonucunda, Dairece Mahkeme kararƒ±nƒ±n bozulmasƒ±na karar verilmi≈ütir. Mahkemece bozmaya uyularak yeniden...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. Skor: 0.6317\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"ƒ∞√ßtihat Metni\" MAHKEMESƒ∞ :Asliye Hukuk Mahkemesi Taraflar arasƒ±ndaki tazminat davasƒ±ndan dolayƒ± yapƒ±lan yargƒ±lama sonunda ƒ∞lk Derece Mahkemesince davanƒ±n reddine karar verilmi≈ütir. ƒ∞lk Derece Mahkemesi kararƒ± davacƒ±lar vekilince temyiz edilmekle; kesin...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. Skor: 0.6259\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi h√ºkm√ºne aykƒ±rƒ± olarak ihtiyati tedbire ili≈ükin karar tarihinden itibaren 10 g√ºn i√ßinde dava a√ßƒ±lmamƒ±≈ü olduƒüu, haksƒ±z ihtiyati tedbirden dolayƒ± olan sorumluluƒüun kusursuz sorumluluk olduƒüu, yani, haksƒ±z ihtiyati tedbir koydurtmu≈ü olan tarafƒ±n, bundan doƒüan maddi zararla sorumlu tutulabilmesi ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana men√º\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657479/765771685.py:328: DeprecationWarning: `search_batch` method is deprecated and will be removed in the future. Use `query_batch_points` instead.\n",
      "  qr = self.qdrant_client.search_batch(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\n",
      "2) ƒ∞nteraktif arama\n",
      "3) Koleksiyon bilgilerini g√∂ster\n",
      "4) √áƒ±kƒ±≈ü\n",
      "üëã G√∂r√º≈ü√ºr√ºz\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Dense + Sparse, 512 dim slice, L2 normalize, hibrit search)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense+sparse\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            sparse_available = 'colbert_vecs' in emb_res\n",
    "            print(f\"‚úÖ Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {sparse_available}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse i√ßin yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(size=self.config.EMBEDDING_DIM, distance=models.Distance.COSINE),\n",
    "                }\n",
    "                sparse_config = {\n",
    "                    \"sparse_vec\": models.SparseVectorParams(\n",
    "                        index=models.SparseIndexParams(on_disk=False))\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config = sparse_config\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # Model dense embedding √ºret\n",
    "                emb_res = self.bge_model.encode(\n",
    "                    batch_texts,\n",
    "                    return_dense=True,\n",
    "                    return_sparse=True\n",
    "                )\n",
    "                dense = emb_res.get(\"dense_vecs\", [[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "                # Dense i√ßinde None veya kƒ±sa vekt√∂r varsa d√ºzelt\n",
    "                dense_clean = []\n",
    "                for vec in dense:\n",
    "                    if vec is None:\n",
    "                        dense_clean.append([0.0]*self.config.EMBEDDING_DIM)\n",
    "                    elif len(vec) < self.config.EMBEDDING_DIM:\n",
    "                        dense_clean.append(vec + [0.0]*(self.config.EMBEDDING_DIM - len(vec)))\n",
    "                    else:\n",
    "                        dense_clean.append(vec[:self.config.EMBEDDING_DIM])\n",
    "\n",
    "                # TF-IDF ile sparse embedding √ºret\n",
    "                from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                vectorizer = TfidfVectorizer(max_features=5000)\n",
    "                X_sparse = vectorizer.fit_transform(batch_texts)\n",
    "                sparse_vectors = []\n",
    "                for row in X_sparse:\n",
    "                    row_coo = row.tocoo()\n",
    "                    sparse_vectors.append({\"indices\": row_coo.col.tolist(), \"values\": row_coo.data.tolist()})\n",
    "\n",
    "                # Listeye ekle\n",
    "                all_embeddings_dense.extend(dense_clean)\n",
    "                all_embeddings_sparse.extend(sparse_vectors)\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings_bge(texts)\n",
    "\n",
    "        points = []\n",
    "        \n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector={\n",
    "                    \"dense_vec\": d,\n",
    "                    \"sparse_vec\": SparseVector(\n",
    "                        indices=s[\"indices\"],\n",
    "                        values=s[\"values\"]\n",
    "                    )\n",
    "                },\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Dense-only semantic search\n",
    "        \"\"\"\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            # Tensor -> first 512 dims -> list\n",
    "            q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            q_sliced = q_t[0, :self.config.EMBEDDING_DIM]\n",
    "            query_v = NamedVector(\n",
    "                name=\"dense_vec\",\n",
    "                vector=q_sliced.cpu().tolist()\n",
    "            )\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_v,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                #vector_name=\"dense_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Semantic search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "   # from qdrant_client.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Dense + Sparse (TF-IDF) hybrid search\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # --- Dense tarafƒ± (BGE embeddings) ---\n",
    "            emb_res = self.bge_model.encode(\n",
    "                [query],\n",
    "                return_dense=True,\n",
    "                return_sparse=True\n",
    "            )\n",
    "\n",
    "            # Dense vekt√∂r\n",
    "            q_dense = emb_res.get(\"dense_vecs\", [[0.0]*self.config.EMBEDDING_DIM])[0]\n",
    "            q_dense = q_dense[:self.config.EMBEDDING_DIM]  # boyut kƒ±rpma\n",
    "            query_dense = NamedVector(\n",
    "                name=\"dense_vec\",\n",
    "                vector=q_dense\n",
    "            )\n",
    "\n",
    "            # Sparse vekt√∂r (senin TF-IDF √ßƒ±ktƒ±n)\n",
    "            query_sparse = None\n",
    "            sparse_raw = emb_res.get(\"sparse_vecs\", [None])[0]\n",
    "            if sparse_raw and \"indices\" in sparse_raw and \"values\" in sparse_raw:\n",
    "                query_sparse = NamedSparseVector(\n",
    "                    name=\"sparse_vec\",\n",
    "                    vector=SparseVector(\n",
    "                        indices=sparse_raw[\"indices\"],\n",
    "                        values=sparse_raw[\"values\"]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # --- Search requests ---\n",
    "            requests = [SearchRequest(vector=query_dense, limit=limit, with_payload=True, score_threshold=score_threshold)]\n",
    "            if query_sparse:\n",
    "                requests.append(SearchRequest(vector=query_sparse, limit=limit, score_threshold=score_threshold))\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            # --- Sonu√ßlarƒ± topla ---\n",
    "            results = []\n",
    "            for request_result in qr:  # her request_result: List[ScoredPoint]\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu (Dense + TF-IDF Sparse)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Hybrid search hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            \n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k,v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.search(  #hibrit armada search_batch olucak bura\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana men√º\")\n",
    "            ch = input(\"Se√ßiminiz (1-3): \").strip()\n",
    "            if ch==\"3\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (√∂rn: '6.HukukDairesi', bo≈ü = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.search_hybrid(q,  limit=limit)\n",
    "                print(f\"SONU√áLAR:{results}\")\n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r.get(\"payload\") or {}   # None ise bo≈ü dict d√∂ner\n",
    "                score = r.get(\"score\", 0.0)\n",
    "                print(f\"\\n{i}. Skor: {score:.4f}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_hybrid_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    main()\n",
    "\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ayrƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Dense + Sparse, 512 dim slice, L2 normalize, hibrit search)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense+sparse\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            sparse_available = 'colbert_vecs' in emb_res\n",
    "            print(f\"‚úÖ Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {sparse_available}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "        if collection_name not in existing:\n",
    "            try:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config={\n",
    "                        \"dense_vec\": VectorParams(\n",
    "                            size=self.config.EMBEDDING_DIM,\n",
    "                            distance=Distance.COSINE\n",
    "                        )\n",
    "                    },\n",
    "                    sparse_vectors_config={\n",
    "                        \"sparse_vec\": SparseVectorParams(\n",
    "                            index={\"on_disk\": False}  # Hibrid search i√ßin gerekli\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                emb_res = self.bge_model.encode(\n",
    "                    batch_texts,\n",
    "                    return_dense=True,\n",
    "                    return_sparse=True\n",
    "                )\n",
    "\n",
    "                dense = emb_res[\"dense_vecs\"]\n",
    "                sparse = emb_res[\"sparse_vecs\"]   # burada dict listesi geliyor: [{\"indices\": [...], \"values\": [...]}, ...]\n",
    "\n",
    "                # Dense i√ßin normalize\n",
    "                dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    dense_slice = dense_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    dense_norm = l2_normalize_tensor(dense_slice)\n",
    "\n",
    "                all_embeddings_dense.extend([v.cpu().tolist() for v in dense_norm])\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings_bge(texts)\n",
    "\n",
    "        points = []\n",
    "        \n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector={\n",
    "                    \"dense_vec\": d,\n",
    "                    \"sparse_vec\": SparseVector(\n",
    "                        indices=s[\"indices\"],\n",
    "                        values=s[\"values\"]\n",
    "                    )\n",
    "                },\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "\n",
    "            # Dense\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "            query_vector_dense = NamedVector(\n",
    "                name=\"dense_vec\",\n",
    "                vector=dense_norm[0].cpu().tolist()\n",
    "            )\n",
    "\n",
    "            # Sparse\n",
    "            colbert_vec = emb_res.get(\"colbert_vecs\", [None])[0]\n",
    "            if colbert_vec is None:\n",
    "                query_vector_sparse = None\n",
    "            else:\n",
    "                indices = list(colbert_vec.keys())\n",
    "                values = list(colbert_vec.values())\n",
    "                query_vector_sparse = NamedSparseVector(\n",
    "                    name=\"sparse_vec\",\n",
    "                    vector=SparseVector(indices=indices, values=values)\n",
    "                )\n",
    "\n",
    "            # Tek sorguda hem dense hem sparse\n",
    "            query_vectors = [query_vector_dense]\n",
    "            if query_vector_sparse:\n",
    "                query_vectors.append(query_vector_sparse)\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vectors,\n",
    "                query_filter=None,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                with_vectors=False,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k,v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana men√º\")\n",
    "            ch = input(\"Se√ßiminiz (1-3): \").strip()\n",
    "            if ch==\"3\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (√∂rn: '6.HukukDairesi', bo≈ü = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit)\n",
    "\n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß:\")\n",
    "            for i,r in enumerate(results,1):\n",
    "                p=r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text',''))>300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_hybrid_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOZUK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# BGE-M3 + Qdrant Hybrid Search (Dense + Sparse Vectors)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    VectorParams, Distance, PointStruct, \n",
    "    SparseVectorParams, SparseIndexParams,\n",
    "    NamedVector,\n",
    "    Filter, FieldCondition, MatchValue\n",
    ")\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"L2 normalize tensor for cosine similarity\"\"\"\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "def convert_sparse_to_qdrant_format(sparse_vecs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert BGE-M3 sparse format to Qdrant sparse format\"\"\"\n",
    "    result = []\n",
    "    for sparse_vec in sparse_vecs:\n",
    "        if isinstance(sparse_vec, dict):\n",
    "            indices = list(sparse_vec.keys())\n",
    "            values = list(sparse_vec.values())\n",
    "        else:\n",
    "            # If it's already in indices/values format\n",
    "            indices = sparse_vec.get('indices', [])\n",
    "            values = sparse_vec.get('values', [])\n",
    "        \n",
    "        result.append({\n",
    "            'indices': [int(idx) for idx in indices],\n",
    "            'values': [float(val) for val in values]\n",
    "        })\n",
    "    return result\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_hybrid_search\"\n",
    "    DENSE_DIM: int = 512  # BGE-M3 dense vector dimension\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 32\n",
    "    DB_BATCH: int = 64\n",
    "    # Hybrid search weights\n",
    "    DENSE_WEIGHT: float = 0.7\n",
    "    SPARSE_WEIGHT: float = 0.3\n",
    "\n",
    "class YargitayHybridProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # BGE-M3 Model\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16, \n",
    "            device=config.DEVICE\n",
    "        )\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    def test_bge_connection(self) -> bool:\n",
    "        \"\"\"Test BGE-M3 connection and show embedding dimensions\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            embeddings = self.bge_model.encode(\n",
    "                test_text,\n",
    "                return_dense=True,\n",
    "                return_sparse=True,\n",
    "                return_colbert_vecs=False\n",
    "            )\n",
    "            \n",
    "            dense = embeddings['dense_vecs']\n",
    "            sparse = embeddings['lexical_weights']\n",
    "            \n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ±\")\n",
    "            print(f\"üìä Dense embedding boyutu: {len(dense[0])}\")\n",
    "            print(f\"üìä Sparse embedding token sayƒ±sƒ±: {len(sparse[0])}\")\n",
    "            print(f\"üîç Dense sample: {dense[0][:5]}...\")\n",
    "            print(f\"üîç Sparse sample keys: {list(sparse[0].keys())[:5]}...\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return False\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Create Qdrant collection with hybrid vector support\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Create collection with named vectors (dense + sparse)\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config={\n",
    "                        \"dense\": VectorParams(\n",
    "                            size=self.config.DENSE_DIM,\n",
    "                            distance=Distance.COSINE\n",
    "                        )\n",
    "                    },\n",
    "                    sparse_vectors_config={\n",
    "                        \"sparse\": SparseVectorParams(\n",
    "                            index=SparseIndexParams()\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "                print(f\"‚úÖ Hybrid koleksiyon olu≈üturuldu: {collection_name}\")\n",
    "                print(f\"   Dense boyut: {self.config.DENSE_DIM}\")\n",
    "                print(f\"   Sparse: Aktif\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Chunk text using semantic chunking\"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_hybrid_embeddings(self, texts: List[str], batch_size: int = None) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Create both dense and sparse embeddings using BGE-M3\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_dense: List[List[float]] = []\n",
    "        all_sparse: List[Dict] = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        print(f\"üîÆ BGE-M3 ile hybrid embedding olu≈üturuluyor: {total} metin (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # Get both dense and sparse embeddings\n",
    "                embeddings = self.bge_model.encode(\n",
    "                    batch_texts,\n",
    "                    return_dense=True,\n",
    "                    return_sparse=True,\n",
    "                    return_colbert_vecs=False\n",
    "                )\n",
    "                \n",
    "                dense_vecs = embeddings['dense_vecs']\n",
    "                sparse_vecs = embeddings['lexical_weights']\n",
    "                \n",
    "                # Process dense vectors (normalize)\n",
    "                if not isinstance(dense_vecs, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense_vecs, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense_vecs.to(self.config.DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    normed_dense = l2_normalize_tensor(dense_t)\n",
    "                \n",
    "                # Convert to lists\n",
    "                batch_dense = [v.cpu().tolist() for v in normed_dense]\n",
    "                all_dense.extend(batch_dense)\n",
    "                \n",
    "                # Process sparse vectors\n",
    "                batch_sparse = convert_sparse_to_qdrant_format(sparse_vecs)\n",
    "                all_sparse.extend(batch_sparse)\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Hybrid embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # Fallback zero vectors\n",
    "                fallback_dense = [[0.0] * self.config.DENSE_DIM for _ in batch_texts]\n",
    "                fallback_sparse = [{'indices': [], 'values': []} for _ in batch_texts]\n",
    "                all_dense.extend(fallback_dense)\n",
    "                all_sparse.extend(fallback_sparse)\n",
    "\n",
    "        print(f\"‚úÖ Hybrid embeddings olu≈üturuldu: {len(all_dense)} dense, {len(all_sparse)} sparse\")\n",
    "        \n",
    "        # Debug: verify dense vector dimensions\n",
    "        if all_dense:\n",
    "            sample_dense_dim = len(all_dense[0])\n",
    "            print(f\"üîç Dense vector boyutu kontrol√º: {sample_dense_dim} (hedef: {self.config.DENSE_DIM})\")\n",
    "            if sample_dense_dim != self.config.DENSE_DIM:\n",
    "                print(f\"‚ùå Boyut uyumsuzluƒüu tespit edildi!\")\n",
    "        \n",
    "        return all_dense, all_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"Process CSV file and create chunks\"\"\"\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            \n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Upload chunks with hybrid embeddings to Qdrant\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk hybrid embedding ile Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        dense_embeddings, sparse_embeddings = self.create_hybrid_embeddings(texts)\n",
    "\n",
    "        if len(dense_embeddings) != len(chunks) or len(sparse_embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for i, (chunk, dense, sparse) in enumerate(zip(chunks, dense_embeddings, sparse_embeddings)):\n",
    "            # Create vectors dictionary with named vectors\n",
    "            vectors = {\"dense\": dense}\n",
    "            \n",
    "            # Only add sparse if it has data\n",
    "            if sparse['indices'] and sparse['values']:\n",
    "                vectors[\"sparse\"] = {\n",
    "                    \"indices\": sparse['indices'],\n",
    "                    \"values\": sparse['values']\n",
    "                }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        # Upload in batches\n",
    "        batch_size = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch_size):\n",
    "            try:\n",
    "                batch_points = points[i:i+batch_size]\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch_points\n",
    "                )\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch_size, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Hybrid y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    from qdrant_client.models import SearchRequest, NamedVector, NamedSparseVector, SparseVector\n",
    "\n",
    "def hybrid_search(self, query: str, limit: int = 10, score_threshold: float = None, \n",
    "                  dense_weight: float = None, sparse_weight: float = None) -> List[Dict]:\n",
    "    \n",
    "    try:\n",
    "        # Kullanƒ±lacak aƒüƒ±rlƒ±klarƒ± al\n",
    "        dense_weight = dense_weight or self.config.DENSE_WEIGHT\n",
    "        sparse_weight = sparse_weight or self.config.SPARSE_WEIGHT\n",
    "\n",
    "        print(f\"üîç Hybrid arama: dense_weight={dense_weight}, sparse_weight={sparse_weight}\")\n",
    "\n",
    "        # Query embeddings\n",
    "        embeddings = self.bge_model.encode(\n",
    "            [query],\n",
    "            return_dense=True,\n",
    "            return_sparse=True,\n",
    "            return_colbert_vecs=False\n",
    "        )\n",
    "        query_dense = embeddings['dense_vecs'][0]\n",
    "        query_sparse = embeddings['lexical_weights'][0]\n",
    "\n",
    "        # Dense vector normalize\n",
    "        query_dense_t = torch.tensor(query_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            query_dense_norm = l2_normalize_tensor(query_dense_t).cpu().tolist()\n",
    "\n",
    "        # Sparse vector Qdrant format\n",
    "        query_sparse_qdrant = SparseVector(\n",
    "            indices=[int(idx) for idx in query_sparse.keys()],\n",
    "            values=[float(val) for val in query_sparse.values()]\n",
    "        )\n",
    "\n",
    "        # Search batch\n",
    "        search_requests = [\n",
    "            SearchRequest(\n",
    "                vector=NamedVector(\n",
    "                    name=\"dense\",\n",
    "                    vector=query_dense_norm\n",
    "                ),\n",
    "                limit=limit * 2,\n",
    "                score_threshold=score_threshold,\n",
    "                with_payload=True\n",
    "            ),\n",
    "            SearchRequest(\n",
    "                vector=NamedSparseVector(\n",
    "                    name=\"sparse\",\n",
    "                    vector=query_sparse_qdrant\n",
    "                ),\n",
    "                limit=limit * 2,\n",
    "                score_threshold=score_threshold,\n",
    "                with_payload=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        results = self.qdrant_client.search_batch(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            requests=search_requests\n",
    "        )\n",
    "\n",
    "        dense_results = results[0]\n",
    "        sparse_results = results[1]\n",
    "\n",
    "        # Combine results with weights\n",
    "        combined_scores = {}\n",
    "        for r in dense_results:\n",
    "            combined_scores[r.id] = {\n",
    "                'dense_score': r.score * dense_weight,\n",
    "                'sparse_score': 0,\n",
    "                'payload': r.payload\n",
    "            }\n",
    "        for r in sparse_results:\n",
    "            if r.id in combined_scores:\n",
    "                combined_scores[r.id]['sparse_score'] = r.score * sparse_weight\n",
    "            else:\n",
    "                combined_scores[r.id] = {\n",
    "                    'dense_score': 0,\n",
    "                    'sparse_score': r.score * sparse_weight,\n",
    "                    'payload': r.payload\n",
    "                }\n",
    "\n",
    "        final_results = []\n",
    "        for point_id, scores in combined_scores.items():\n",
    "            final_score = scores['dense_score'] + scores['sparse_score']\n",
    "            final_results.append({\n",
    "                'score': final_score,\n",
    "                'dense_score': scores['dense_score'],\n",
    "                'sparse_score': scores['sparse_score'],\n",
    "                'payload': scores['payload']\n",
    "            })\n",
    "\n",
    "        # Sort by final score and limit\n",
    "        final_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return final_results[:limit]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hybrid arama hatasƒ±: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "    def search_dense_only(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Search using only dense vectors\"\"\"\n",
    "        try:\n",
    "            query_embeddings = self.bge_model.encode([query], return_dense=True, return_sparse=False)\n",
    "            query_dense = query_embeddings['dense_vecs'][0]\n",
    "            \n",
    "            # Normalize\n",
    "            query_dense_t = torch.tensor(query_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                query_dense_norm = l2_normalize_tensor(query_dense_t).cpu().tolist()\n",
    "            \n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=(\"dense\", query_dense_norm),\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            return [{'score': r.score, 'payload': r.payload} for r in results]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dense arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_sparse_only(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Search using only sparse vectors\"\"\"\n",
    "        try:\n",
    "            query_embeddings = self.bge_model.encode([query], return_dense=False, return_sparse=True)\n",
    "            query_sparse = query_embeddings['lexical_weights'][0]\n",
    "            \n",
    "            query_sparse_qdrant = {\n",
    "                \"name\": \"sparse\",\n",
    "                \"indices\": [int(idx) for idx in query_sparse.keys()],\n",
    "                \"values\": [float(val) for val in query_sparse.values()]\n",
    "            }\n",
    "            \n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_sparse_qdrant,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            return [{'score': r.score, 'payload': r.payload} for r in results]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Sparse arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Get collection information\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3 Hybrid\",\n",
    "                \"dense_dim\": self.config.DENSE_DIM,\n",
    "                \"sparse_enabled\": True,\n",
    "                \"dense_weight\": self.config.DENSE_WEIGHT,\n",
    "                \"sparse_weight\": self.config.SPARSE_WEIGHT\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline\n",
    "# -------------------------\n",
    "class YargitayHybridPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitayHybridProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Run full pipeline: CSV -> chunks -> hybrid embeddings -> Qdrant\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Hybrid pipeline ba≈ülƒ±yor\")\n",
    "        \n",
    "        if not self.processor.test_bge_connection():\n",
    "            return False\n",
    "        \n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        \n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        \n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive search interface\"\"\"\n",
    "        print(\"\\nüîé ƒ∞nteraktif hybrid arama ba≈ülatƒ±ldƒ±\")\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"1) Hybrid arama (Dense + Sparse)\")\n",
    "            print(\"2) Sadece Dense arama\")\n",
    "            print(\"3) Sadece Sparse arama\")\n",
    "            print(\"4) Hybrid aƒüƒ±rlƒ±k ayarlarƒ±\")\n",
    "            print(\"5) Ana men√º\")\n",
    "            \n",
    "            choice = input(\"Se√ßiminiz (1-5): \").strip()\n",
    "            \n",
    "            if choice == \"5\":\n",
    "                break\n",
    "            \n",
    "            if choice == \"4\":\n",
    "                try:\n",
    "                    dense_w = float(input(f\"Dense aƒüƒ±rlƒ±k (mevcut: {self.config.DENSE_WEIGHT}): \") or self.config.DENSE_WEIGHT)\n",
    "                    sparse_w = float(input(f\"Sparse aƒüƒ±rlƒ±k (mevcut: {self.config.SPARSE_WEIGHT}): \") or self.config.SPARSE_WEIGHT)\n",
    "                    self.config.DENSE_WEIGHT = dense_w\n",
    "                    self.config.SPARSE_WEIGHT = sparse_w\n",
    "                    print(f\"‚úÖ Aƒüƒ±rlƒ±klar g√ºncellendi: Dense={dense_w}, Sparse={sparse_w}\")\n",
    "                except ValueError:\n",
    "                    print(\"‚ùå Ge√ßersiz deƒüer\")\n",
    "                continue\n",
    "            \n",
    "            if choice not in {\"1\", \"2\", \"3\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if query.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "            \n",
    "            # Perform search based on choice\n",
    "            if choice == \"1\":\n",
    "                results = self.processor.hybrid_search(query, limit=limit)\n",
    "            elif choice == \"2\":\n",
    "                results = self.processor.search_dense_only(query, limit=limit)\n",
    "            elif choice == \"3\":\n",
    "                results = self.processor.search_sparse_only(query, limit=limit)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß ({['Hybrid', 'Dense Only', 'Sparse Only'][int(choice)-1]} arama):\")\n",
    "            \n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                \n",
    "                # Show component scores for hybrid search\n",
    "                if choice == \"1\" and 'dense_score' in r and 'sparse_score' in r:\n",
    "                    print(f\"   (Dense: {r['dense_score']:.4f}, Sparse: {r['sparse_score']:.4f})\")\n",
    "                \n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                \n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"yargitay_hybrid_search\",\n",
    "        DENSE_DIM=512,  # 512-dimensional dense vectors\n",
    "        BATCH_SIZE=32,\n",
    "        DB_BATCH=64,\n",
    "        DENSE_WEIGHT=0.7,\n",
    "        SPARSE_WEIGHT=0.3\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayHybridPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 HYBRID SEARCH Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> hybrid embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        \n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if success else \"‚ùå Hata olu≈ütu\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding bulunamadƒ± ‚Äî pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hƒ∞BRƒ∞T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Dense + Sparse, 512 dim slice, L2 normalize, hibrit search)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model\n",
    "        print(f\"üîÆ BGE-M3 y√ºkleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Hazƒ±r - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense+sparse\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            sparse_available = 'colbert_vecs' in emb_res\n",
    "            print(f\"‚úÖ Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {sparse_available}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse i√ßin yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE),\n",
    "                    \"sparse_vec\": VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ {total} metin i≈üleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "\n",
    "                dense = emb_res.get('dense_vecs', emb_res)\n",
    "                sparse = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)]*len(batch_texts))\n",
    "\n",
    "                dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                sparse_t = torch.tensor(sparse, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dense_slice = dense_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    dense_norm = l2_normalize_tensor(dense_slice)\n",
    "                    sparse_slice = sparse_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    sparse_norm = l2_normalize_tensor(sparse_slice)\n",
    "\n",
    "                all_embeddings_dense.extend([v.cpu().tolist() for v in dense_norm])\n",
    "                all_embeddings_sparse.extend([v.cpu().tolist() for v in sparse_norm])\n",
    "\n",
    "                print(f\"  üìä Batch i≈ülendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Embedding hatasƒ± (batch {i//batch_size+1}): {e}\")\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"üìÑ CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings_bge(texts)\n",
    "\n",
    "        points = []\n",
    "        \n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vectors={'dense_vec': d, 'sparse_vec': s} \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            if dense_q is None or any(v is None for v in dense_q[0]):\n",
    "                dense_q = [np.zeros(self.config.EMBEDDING_DIM)]\n",
    "\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "            if sparse_q is None or any(v is None for v in sparse_q[0]):\n",
    "                sparse_q = [np.zeros(self.config.EMBEDDING_DIM)]\n",
    "\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=None,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                # Sparse hibrit param Qdrant 1.2+\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k,v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"üöÄ Full pipeline ba≈ülƒ±yor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\nüîé ƒ∞nteraktif arama ba≈ülatƒ±ldƒ±\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana men√º\")\n",
    "            ch = input(\"Se√ßiminiz (1-3): \").strip()\n",
    "            if ch==\"3\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\"}:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "                continue\n",
    "            q = input(\"üîç Arama metni (√ßƒ±kmak i√ßin 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Ka√ß sonu√ß? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (√∂rn: '6.HukukDairesi', bo≈ü = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit)\n",
    "\n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nüìã {len(results)} sonu√ß:\")\n",
    "            for i,r in enumerate(results,1):\n",
    "                p=r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text',''))>300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_hybrid_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K Sƒ∞STEM (Dense+Sparse)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline √ßalƒ±≈ütƒ±r (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) ƒ∞nteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4) √áƒ±kƒ±≈ü\")\n",
    "        choice = input(\"Se√ßiminiz (1-4): \").strip()\n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"‚úÖ Tamamlandƒ±\" if ok else \"‚ùå Hata √ßƒ±ktƒ±\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding bulunamadƒ± ‚Äî pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
