{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full kod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "🔮 BGE-M3 yükleniyor: BAAI/bge-m3 (device=cuda)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 277156.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hazır - Cihaz: NVIDIA RTX A6000\n",
      "\n",
      "============================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK SİSTEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\n",
      "2) İnteraktif arama\n",
      "3) Koleksiyon bilgilerini göster\n",
      "4) Çıkış\n",
      "🚀 Full pipeline başlıyor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dense embedding boyutu: 1024\n",
      "🔍 Sparse embedding mevcut: True\n",
      "🗑️ Eski koleksiyon silindi: bge_hybrid_chunks\n",
      "✅ Koleksiyon oluşturuldu: bge_hybrid_chunks (Dense+Sparse)\n",
      "📄 CSV okunuyor: /home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\n",
      "📊 10 satır yüklendi\n",
      "  ✅ İşlenen satır: 5/10 (Toplam chunk: 44)\n",
      "  ✅ İşlenen satır: 10/10 (Toplam chunk: 59)\n",
      "🧩 Toplam 59 chunk oluşturuldu\n",
      "🚀 59 chunk Qdrant'a yükleniyor...\n",
      "🔮 59 metin işleniyor (batch_size=100)...\n",
      "  📊 Batch işlendi: 59/59\n",
      "  ✅ Batch yüklendi: 59/59\n",
      "🎉 Yükleme tamamlandı!\n",
      "\n",
      "📊 Koleksiyon Bilgileri:\n",
      "{\n",
      "  \"collection_name\": \"bge_hybrid_chunks\",\n",
      "  \"points_count\": 59,\n",
      "  \"vectors_count\": null,\n",
      "  \"status\": \"green\",\n",
      "  \"embedding_model\": \"BGE-M3\",\n",
      "  \"embedding_dim\": 512\n",
      "}\n",
      "✅ Tamamlandı\n",
      "\n",
      "============================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK SİSTEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\n",
      "2) İnteraktif arama\n",
      "3) Koleksiyon bilgilerini göster\n",
      "4) Çıkış\n",
      "\n",
      "🔎 İnteraktif arama başlatıldı\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana menü\n",
      "📊 5 sonuç bulundu (Dense only)\n",
      "\n",
      "📋 5 sonuç:\n",
      "\n",
      "1. Skor: 0.6925\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 3. Değerlendirme Mahkemece, tazminat davalısının ödemekle yükümlü olduğu miktarın uyulmasına karar verilen Yargıtay ilamında da belirtildiği üzer ihtiyati tedbir kararının icra edildiği tarih ile ihtiyati tedbirin kalktığı ya da kalkmış sayıldığı tarih arasındaki zarar olduğu, ihtiyati tedbir kararı...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Skor: 0.6892\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi, ihtiyati tedbir kararının haksız olduğunun belirlenmesi halinde tedbir kararı yüzünden uğranılan zararın tazminini düzenlediğini, ihtiyati tedbir kararını icra ettiren tarafın yasal sürede dava açmaması halinde ihtiyati tedbirin haksız konulduğunun kabulü gerektiği, kaldı ki süresinde dava ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Skor: 0.6577\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasındaki tazminat davasından dolayı yapılan yargılama sonunda İlk Derece Mahkemesince davanın reddine karar verilmiştir. İlk Derece Mahkemesi kararı davacılar vekilince temyiz edilmekle; kesin...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. Skor: 0.6522\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi hükmüne aykırı olarak ihtiyati tedbire ilişkin karar tarihinden itibaren 10 gün içinde dava açılmamış olduğu, haksız ihtiyati tedbirden dolayı olan sorumluluğun kusursuz sorumluluk olduğu, yani, haksız ihtiyati tedbir koydurtmuş olan tarafın, bundan doğan maddi zararla sorumlu tutulabilmesi ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. Skor: 0.6519\n",
      "   Daire: 6.HukukDairesisi | Tarih: 18.01.2024\n",
      "   Metin: 6. Hukuk Dairesi 2023/596 E. , 2024/257 K. \\n \"İçtihat Metni\" MAHKEMESİ : ... Bölge Adliye Mahkemesi 13. Hukuk Dairesi Taraflar arasındaki rücuen tazminat davasından dolayı yapılan yargılama sonunda, İlk Derece Mahkemesince davanın kabulüne karar verilmiştir. Kararın davacı vekili tarafından istinaf...\n",
      "------------------------------------------------------------\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana menü\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657479/765771685.py:272: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  qr = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 5 sonuç bulundu (Dense + TF-IDF Sparse)\n",
      "SONUÇLAR:[{'score': 0.673231, 'payload': {'chunk_id': 1, 'text': 'maddesi, ihtiyati tedbir kararının haksız olduğunun belirlenmesi halinde tedbir kararı yüzünden uğranılan zararın tazminini düzenlediğini, ihtiyati tedbir kararını icra ettiren tarafın yasal sürede dava açmaması halinde ihtiyati tedbirin haksız konulduğunun kabulü gerektiği, kaldı ki süresinde dava açsa da durumun değişmeyeceğini belirterek müvekkillerinin inşaatının geç bitirilmesinden kaynaklı 10.000,00 TL maddi tazminatın tahsiline karar verilmesini talep etmiştir. 2.Davacı vekili duruşmadaki beyanında; tedbir sebebiyle bağımsız bölümlerinin geç teslim edileceğini, bundan kaynaklı doğacak zararları talep ettiklerini beyan etmiştir. II. CEVAP Davalı vekili cevap dilekçesinde özetle; ekonomik nedenlerle 10 gün içerisinde dava açamadıklarını, 19 gün sonra açtıkları davanın ... 4. Asliye Hukuk Mahkemesinin 2010/27 Esas sayılı sırasında kayıtlı olduğunu belirterek davanın reddini talep etmiştir. III. MAHKEME KARARI Mahkemenin 30.12.2011 tarihli ve 2010/695 Esas, 2011/916 Karar sayılı kararı ile ... 1.Asliye Hukuk Mahkemesinin 2009/139 D....', 'token_count': 413, 'char_count': 1054, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}, {'score': 0.66766644, 'payload': {'chunk_id': 10, 'text': '3. Değerlendirme Mahkemece, tazminat davalısının ödemekle yükümlü olduğu miktarın uyulmasına karar verilen Yargıtay ilamında da belirtildiği üzer ihtiyati tedbir kararının icra edildiği tarih ile ihtiyati tedbirin kalktığı ya da kalkmış sayıldığı tarih arasındaki zarar olduğu, ihtiyati tedbir kararının kalkmış sayıldığı tarih ile icra edildiği tarihin aynı olduğu, arada süre zarfı bulunmaması nedeniyle yapılması talep edilen keşifte yoksun kalınan kira kaybına ilişkin hesaplama yapılamayacağından yargılamaya bir katkı sağlamayacağı kanaatine varılarak davanın reddine karar verildiği, somut olayda; 05.01.2010 tarihinde tedbirin icra edildiği, 14.01.2010 tarihinde ise kaldırıldığı anlaşılmaktadır. Bu durumda mahkemece ihtiyati tedbir kararının kalkmış sayıldığı tarih ile icra edildiği tarihin aynı olduğuna dair karar hatalı olup, 05.01.2010 tarihi ile 14.01.2010 tarihleri arasında davacının maddi zararlarını ve illiyet bağını ispat imkanı tanınması ve sonucuna göre karar verilmesi gerekirken, davanın reddine karar verilmesi hatalı olmuş, kararın bozulmasına karar verilmiştir. VI.', 'token_count': 431, 'char_count': 1094, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}, {'score': 0.6383259, 'payload': {'chunk_id': 0, 'text': '6. Hukuk Dairesi 2022/4600 E. , 2024/467 K. \\\\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasında görülen rücuen tazminat davasında verilen karar hakkında yapılan temyiz incelemesi sonucunda, Dairece Mahkeme kararının bozulmasına karar verilmiştir. Mahkemece bozmaya uyularak yeniden yapılan yargılama sonucunda; davanın kısmen kabulüne karar verilmiştir. Mahkeme kararı, davacı ve davalı ....Ltd.Şti. vekili tarafından temyiz edilmekle; kesinlik, süre, temyiz şartı ve diğer usul eksiklikleri yönünden yapılan ön inceleme sonucunda, temyiz dilekçesinin kabulüne karar verildikten ve Tetkik Hâkimi tarafından hazırlanan rapor dinlendikten sonra dosyadaki belgeler incelenip gereği düşünüldü: I. DAVA Davacı vekili dava dilekçesinde; müvekkili ile davalı yüklenici şirketler arasındaki hizmet alım sözleşmeleri kapsamında çalışan işçilere, ... mahkemesi ilamlarına dayalı olarak başlatılan icra takipleri sonucunda, asıl işveren konumundaki müvekkili tarafından ödemeler yapıldığını, bu ödemelerden davalıların sorumlu olduklarını ileri sürerek, toplam 79.248,74 TL’nin ödeme tarihlerinden itibaren işleyecek reeskont faizi ile birlikte davalılardan müştereken ve müteselsilen tahsilini talep ve dava etmiştir. II. CEVAP Davalı ... Sağlık Hiz. Oto. Gıd. Tem. Nak. San. ve Tic. Ltd. Şti.', 'token_count': 508, 'char_count': 1302, 'original_index': 7, 'esas_no': '2022/4600 E.', 'karar_no': '2024/467 K.', 'daire': '6.HukukDairesisi', 'tarih': '15.12.2015,03.03.2020,01.02.2024', 'document_id': '6750524485b6640290c37b0d'}}, {'score': 0.6316994, 'payload': {'chunk_id': 0, 'text': '6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\\\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasındaki tazminat davasından dolayı yapılan yargılama sonunda İlk Derece Mahkemesince davanın reddine karar verilmiştir. İlk Derece Mahkemesi kararı davacılar vekilince temyiz edilmekle; kesinlik, süre, temyiz şartı ve diğer usul eksiklikleri yönünden yapılan ön inceleme sonucunda, temyiz dilekçesinin kabulüne karar verildikten ve Tetkik Hâkimi tarafından hazırlanan rapor dinlendikten sonra dosyadaki belgeler incelenip gereği düşünüldü: I. DAVA 1.Davacı vekili dava dilekçesinde özetle; müvekkillerinin arsa sahibi olduğunu, ...\\'ın müteahhit olarak işbu arsa üzerine yapmış olduğu binanın ... 1. Asliye Hukuk Mahkemesinin 2009/139 D. ... dosyası üzerinden ihtiyati tedbir konularak inşaatın durdurulduğunu, HUMK\\'un 109. maddesi gereğince ihtiyati tedbir kararının verildiği tarihten itibaren 10 gün içerisinde esas hakkında davanın açılması gerektiğini ve bu durumun dosyaya ibrazı gerekirken davanın açılmadığını, Hukuk Usulü Muhakemeleri Kanununun 110.', 'token_count': 422, 'char_count': 1066, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}, {'score': 0.6258817, 'payload': {'chunk_id': 6, 'text': \"maddesi hükmüne aykırı olarak ihtiyati tedbire ilişkin karar tarihinden itibaren 10 gün içinde dava açılmamış olduğu, haksız ihtiyati tedbirden dolayı olan sorumluluğun kusursuz sorumluluk olduğu, yani, haksız ihtiyati tedbir koydurtmuş olan tarafın, bundan doğan maddi zararla sorumlu tutulabilmesi için, ihtiyati tedbiri kötüniyetle istemiş ve koydurmuş olması veya bunda herhangi bir ihmalinin bulunmasının şart olmadığı, ihtiyati tedbir haksız ve bundan da bir zarar doğmuş ise, bu haksız ihtiyati tedbiri koydurtmuş olan tarafın kusurlu olmasa bile bundan zarar gören karşı tarafa veya üçüncü kişiye tazminat ödemekle yükümlü olduğu, ihtiyati tedbir isteyen ve bu kararı icra ettiren kişinin, ihtiyati tedbiri icra ettirdiği halde, HUMK'un 109.\", 'token_count': 290, 'char_count': 749, 'original_index': 1, 'esas_no': '2022/3281 E.', 'karar_no': '2024/117 K.', 'daire': '6.HukukDairesisi', 'tarih': '30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024', 'document_id': '6750524485b6640290c37b07'}}]\n",
      "\n",
      "📋 5 sonuç:\n",
      "\n",
      "1. Skor: 0.6732\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi, ihtiyati tedbir kararının haksız olduğunun belirlenmesi halinde tedbir kararı yüzünden uğranılan zararın tazminini düzenlediğini, ihtiyati tedbir kararını icra ettiren tarafın yasal sürede dava açmaması halinde ihtiyati tedbirin haksız konulduğunun kabulü gerektiği, kaldı ki süresinde dava ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Skor: 0.6677\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 3. Değerlendirme Mahkemece, tazminat davalısının ödemekle yükümlü olduğu miktarın uyulmasına karar verilen Yargıtay ilamında da belirtildiği üzer ihtiyati tedbir kararının icra edildiği tarih ile ihtiyati tedbirin kalktığı ya da kalkmış sayıldığı tarih arasındaki zarar olduğu, ihtiyati tedbir kararı...\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Skor: 0.6383\n",
      "   Daire: 6.HukukDairesisi | Tarih: 15.12.2015,03.03.2020,01.02.2024\n",
      "   Metin: 6. Hukuk Dairesi 2022/4600 E. , 2024/467 K. \\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasında görülen rücuen tazminat davasında verilen karar hakkında yapılan temyiz incelemesi sonucunda, Dairece Mahkeme kararının bozulmasına karar verilmiştir. Mahkemece bozmaya uyularak yeniden...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. Skor: 0.6317\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: 6. Hukuk Dairesi 2022/3281 E. , 2024/117 K. \\n \"İçtihat Metni\" MAHKEMESİ :Asliye Hukuk Mahkemesi Taraflar arasındaki tazminat davasından dolayı yapılan yargılama sonunda İlk Derece Mahkemesince davanın reddine karar verilmiştir. İlk Derece Mahkemesi kararı davacılar vekilince temyiz edilmekle; kesin...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. Skor: 0.6259\n",
      "   Daire: 6.HukukDairesisi | Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   Metin: maddesi hükmüne aykırı olarak ihtiyati tedbire ilişkin karar tarihinden itibaren 10 gün içinde dava açılmamış olduğu, haksız ihtiyati tedbirden dolayı olan sorumluluğun kusursuz sorumluluk olduğu, yani, haksız ihtiyati tedbir koydurtmuş olan tarafın, bundan doğan maddi zararla sorumlu tutulabilmesi ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "1) Basit arama\n",
      "2) Filtreli arama\n",
      "3) Ana menü\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657479/765771685.py:328: DeprecationWarning: `search_batch` method is deprecated and will be removed in the future. Use `query_batch_points` instead.\n",
      "  qr = self.qdrant_client.search_batch(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK SİSTEM (Dense+Sparse)\n",
      "============================================================\n",
      "1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\n",
      "2) İnteraktif arama\n",
      "3) Koleksiyon bilgilerini göster\n",
      "4) Çıkış\n",
      "👋 Görüşürüz\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Dense + Sparse, 512 dim slice, L2 normalize, hibrit search)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from qdrant_client import models\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from qdrant_client.http.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model\n",
    "        print(f\"🔮 BGE-M3 yükleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"✅ Hazır - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense+sparse\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            sparse_available = 'colbert_vecs' in emb_res\n",
    "            print(f\"✅ Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"🔍 Sparse embedding mevcut: {sparse_available}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse için yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": models.VectorParams(size=self.config.EMBEDDING_DIM, distance=models.Distance.COSINE),\n",
    "                }\n",
    "                sparse_config = {\n",
    "                    \"sparse_vec\": models.SparseVectorParams(\n",
    "                        index=models.SparseIndexParams(on_disk=False))\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config,\n",
    "                    sparse_vectors_config = sparse_config\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"🔮 {total} metin işleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # Model dense embedding üret\n",
    "                emb_res = self.bge_model.encode(\n",
    "                    batch_texts,\n",
    "                    return_dense=True,\n",
    "                    return_sparse=True\n",
    "                )\n",
    "                dense = emb_res.get(\"dense_vecs\", [[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "                # Dense içinde None veya kısa vektör varsa düzelt\n",
    "                dense_clean = []\n",
    "                for vec in dense:\n",
    "                    if vec is None:\n",
    "                        dense_clean.append([0.0]*self.config.EMBEDDING_DIM)\n",
    "                    elif len(vec) < self.config.EMBEDDING_DIM:\n",
    "                        dense_clean.append(vec + [0.0]*(self.config.EMBEDDING_DIM - len(vec)))\n",
    "                    else:\n",
    "                        dense_clean.append(vec[:self.config.EMBEDDING_DIM])\n",
    "\n",
    "                # TF-IDF ile sparse embedding üret\n",
    "                from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                vectorizer = TfidfVectorizer(max_features=5000)\n",
    "                X_sparse = vectorizer.fit_transform(batch_texts)\n",
    "                sparse_vectors = []\n",
    "                for row in X_sparse:\n",
    "                    row_coo = row.tocoo()\n",
    "                    sparse_vectors.append({\"indices\": row_coo.col.tolist(), \"values\": row_coo.data.tolist()})\n",
    "\n",
    "                # Listeye ekle\n",
    "                all_embeddings_dense.extend(dense_clean)\n",
    "                all_embeddings_sparse.extend(sparse_vectors)\n",
    "\n",
    "                print(f\"  📊 Batch işlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Embedding hatası (batch {i//batch_size+1}): {e}\")\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"📄 CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"❌ Ana metin sütunu bulunamadı\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ✅ İşlenen satır: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings_bge(texts)\n",
    "\n",
    "        points = []\n",
    "        \n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector={\n",
    "                    \"dense_vec\": d,\n",
    "                    \"sparse_vec\": SparseVector(\n",
    "                        indices=s[\"indices\"],\n",
    "                        values=s[\"values\"]\n",
    "                    )\n",
    "                },\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Dense-only semantic search\n",
    "        \"\"\"\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            q_dense = emb_res['dense_vecs'] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res\n",
    "\n",
    "            # Tensor -> first 512 dims -> list\n",
    "            q_t = torch.tensor(q_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            q_sliced = q_t[0, :self.config.EMBEDDING_DIM]\n",
    "            query_v = NamedVector(\n",
    "                name=\"dense_vec\",\n",
    "                vector=q_sliced.cpu().tolist()\n",
    "            )\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_v,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                #vector_name=\"dense_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{\"score\": p.score, \"payload\": p.payload} for p in qr]\n",
    "            print(f\"📊 {len(results)} sonuç bulundu (Dense only)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Semantic search hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "   # from qdrant_client.models import NamedVector, NamedSparseVector, SparseVector, SearchRequest\n",
    "\n",
    "    def search_hybrid(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Dense + Sparse (TF-IDF) hybrid search\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # --- Dense tarafı (BGE embeddings) ---\n",
    "            emb_res = self.bge_model.encode(\n",
    "                [query],\n",
    "                return_dense=True,\n",
    "                return_sparse=True\n",
    "            )\n",
    "\n",
    "            # Dense vektör\n",
    "            q_dense = emb_res.get(\"dense_vecs\", [[0.0]*self.config.EMBEDDING_DIM])[0]\n",
    "            q_dense = q_dense[:self.config.EMBEDDING_DIM]  # boyut kırpma\n",
    "            query_dense = NamedVector(\n",
    "                name=\"dense_vec\",\n",
    "                vector=q_dense\n",
    "            )\n",
    "\n",
    "            # Sparse vektör (senin TF-IDF çıktın)\n",
    "            query_sparse = None\n",
    "            sparse_raw = emb_res.get(\"sparse_vecs\", [None])[0]\n",
    "            if sparse_raw and \"indices\" in sparse_raw and \"values\" in sparse_raw:\n",
    "                query_sparse = NamedSparseVector(\n",
    "                    name=\"sparse_vec\",\n",
    "                    vector=SparseVector(\n",
    "                        indices=sparse_raw[\"indices\"],\n",
    "                        values=sparse_raw[\"values\"]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # --- Search requests ---\n",
    "            requests = [SearchRequest(vector=query_dense, limit=limit, with_payload=True, score_threshold=score_threshold)]\n",
    "            if query_sparse:\n",
    "                requests.append(SearchRequest(vector=query_sparse, limit=limit, score_threshold=score_threshold))\n",
    "\n",
    "            qr = self.qdrant_client.search_batch(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                requests=requests,\n",
    "            )\n",
    "\n",
    "            # --- Sonuçları topla ---\n",
    "            results = []\n",
    "            for request_result in qr:  # her request_result: List[ScoredPoint]\n",
    "                for scored_point in request_result:\n",
    "                    results.append({\n",
    "                        \"score\": scored_point.score,\n",
    "                        \"payload\": scored_point.payload\n",
    "                    })\n",
    "\n",
    "            print(f\"📊 {len(results)} sonuç bulundu (Dense + TF-IDF Sparse)\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Hybrid search hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            \n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k,v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.search(  #hibrit armada search_batch olucak bura\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"📊 {len(results)} filtreli sonuç bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Filtreli arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Full pipeline başlıyor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ Chunk bulunamadı\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\n🔎 İnteraktif arama başlatıldı\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana menü\")\n",
    "            ch = input(\"Seçiminiz (1-3): \").strip()\n",
    "            if ch==\"3\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\"}:\n",
    "                print(\"❌ Geçersiz seçim\")\n",
    "                continue\n",
    "            q = input(\"🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Kaç sonuç? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (örn: '6.HukukDairesi', boş = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.search_hybrid(q,  limit=limit)\n",
    "                print(f\"SONUÇLAR:{results}\")\n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n📋 {len(results)} sonuç:\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r.get(\"payload\") or {}   # None ise boş dict döner\n",
    "                score = r.get(\"score\", 0.0)\n",
    "                print(f\"\\n{i}. Skor: {score:.4f}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_hybrid_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK SİSTEM (Dense+Sparse)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) İnteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini göster\")\n",
    "        print(\"4) Çıkış\")\n",
    "        choice = input(\"Seçiminiz (1-4): \").strip()\n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"✅ Tamamlandı\" if ok else \"❌ Hata çıktı\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            print(\"👋 Görüşürüz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    main()\n",
    "\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ayrı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Dense + Sparse, 512 dim slice, L2 normalize, hibrit search)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from qdrant_client.models import NamedVector, NamedSparseVector, SparseVectorParams, SparseVector\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model\n",
    "        print(f\"🔮 BGE-M3 yükleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"✅ Hazır - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense+sparse\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            sparse_available = 'colbert_vecs' in emb_res\n",
    "            print(f\"✅ Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"🔍 Sparse embedding mevcut: {sparse_available}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "        if collection_name not in existing:\n",
    "            try:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config={\n",
    "                        \"dense_vec\": VectorParams(\n",
    "                            size=self.config.EMBEDDING_DIM,\n",
    "                            distance=Distance.COSINE\n",
    "                        )\n",
    "                    },\n",
    "                    sparse_vectors_config={\n",
    "                        \"sparse_vec\": SparseVectorParams(\n",
    "                            index={\"on_disk\": False}  # Hibrid search için gerekli\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"🔮 {total} metin işleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                emb_res = self.bge_model.encode(\n",
    "                    batch_texts,\n",
    "                    return_dense=True,\n",
    "                    return_sparse=True\n",
    "                )\n",
    "\n",
    "                dense = emb_res[\"dense_vecs\"]\n",
    "                sparse = emb_res[\"sparse_vecs\"]   # burada dict listesi geliyor: [{\"indices\": [...], \"values\": [...]}, ...]\n",
    "\n",
    "                # Dense için normalize\n",
    "                dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    dense_slice = dense_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    dense_norm = l2_normalize_tensor(dense_slice)\n",
    "\n",
    "                all_embeddings_dense.extend([v.cpu().tolist() for v in dense_norm])\n",
    "                all_embeddings_sparse.extend(sparse)\n",
    "\n",
    "                print(f\"  📊 Batch işlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Embedding hatası (batch {i//batch_size+1}): {e}\")\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([{\"indices\": [], \"values\": []} for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"📄 CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"❌ Ana metin sütunu bulunamadı\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ✅ İşlenen satır: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings_bge(texts)\n",
    "\n",
    "        points = []\n",
    "        \n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector={\n",
    "                    \"dense_vec\": d,\n",
    "                    \"sparse_vec\": SparseVector(\n",
    "                        indices=s[\"indices\"],\n",
    "                        values=s[\"values\"]\n",
    "                    )\n",
    "                },\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "\n",
    "            # Dense\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "            query_vector_dense = NamedVector(\n",
    "                name=\"dense_vec\",\n",
    "                vector=dense_norm[0].cpu().tolist()\n",
    "            )\n",
    "\n",
    "            # Sparse\n",
    "            colbert_vec = emb_res.get(\"colbert_vecs\", [None])[0]\n",
    "            if colbert_vec is None:\n",
    "                query_vector_sparse = None\n",
    "            else:\n",
    "                indices = list(colbert_vec.keys())\n",
    "                values = list(colbert_vec.values())\n",
    "                query_vector_sparse = NamedSparseVector(\n",
    "                    name=\"sparse_vec\",\n",
    "                    vector=SparseVector(indices=indices, values=values)\n",
    "                )\n",
    "\n",
    "            # Tek sorguda hem dense hem sparse\n",
    "            query_vectors = [query_vector_dense]\n",
    "            if query_vector_sparse:\n",
    "                query_vectors.append(query_vector_sparse)\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vectors,\n",
    "                query_filter=None,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                with_vectors=False,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k,v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"📊 {len(results)} filtreli sonuç bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Filtreli arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Full pipeline başlıyor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ Chunk bulunamadı\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\n🔎 İnteraktif arama başlatıldı\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana menü\")\n",
    "            ch = input(\"Seçiminiz (1-3): \").strip()\n",
    "            if ch==\"3\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\"}:\n",
    "                print(\"❌ Geçersiz seçim\")\n",
    "                continue\n",
    "            q = input(\"🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Kaç sonuç? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (örn: '6.HukukDairesi', boş = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit)\n",
    "\n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n📋 {len(results)} sonuç:\")\n",
    "            for i,r in enumerate(results,1):\n",
    "                p=r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text',''))>300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_hybrid_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK SİSTEM (Dense+Sparse)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) İnteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini göster\")\n",
    "        print(\"4) Çıkış\")\n",
    "        choice = input(\"Seçiminiz (1-4): \").strip()\n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"✅ Tamamlandı\" if ok else \"❌ Hata çıktı\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            print(\"👋 Görüşürüz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOZUK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# BGE-M3 + Qdrant Hybrid Search (Dense + Sparse Vectors)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    VectorParams, Distance, PointStruct, \n",
    "    SparseVectorParams, SparseIndexParams,\n",
    "    NamedVector,\n",
    "    Filter, FieldCondition, MatchValue\n",
    ")\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"L2 normalize tensor for cosine similarity\"\"\"\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "def convert_sparse_to_qdrant_format(sparse_vecs: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert BGE-M3 sparse format to Qdrant sparse format\"\"\"\n",
    "    result = []\n",
    "    for sparse_vec in sparse_vecs:\n",
    "        if isinstance(sparse_vec, dict):\n",
    "            indices = list(sparse_vec.keys())\n",
    "            values = list(sparse_vec.values())\n",
    "        else:\n",
    "            # If it's already in indices/values format\n",
    "            indices = sparse_vec.get('indices', [])\n",
    "            values = sparse_vec.get('values', [])\n",
    "        \n",
    "        result.append({\n",
    "            'indices': [int(idx) for idx in indices],\n",
    "            'values': [float(val) for val in values]\n",
    "        })\n",
    "    return result\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_hybrid_search\"\n",
    "    DENSE_DIM: int = 512  # BGE-M3 dense vector dimension\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 32\n",
    "    DB_BATCH: int = 64\n",
    "    # Hybrid search weights\n",
    "    DENSE_WEIGHT: float = 0.7\n",
    "    SPARSE_WEIGHT: float = 0.3\n",
    "\n",
    "class YargitayHybridProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # BGE-M3 Model\n",
    "        print(f\"🔮 BGE-M3 yükleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16, \n",
    "            device=config.DEVICE\n",
    "        )\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"✅ Hazır - Cihaz: {device_name}\")\n",
    "\n",
    "    def test_bge_connection(self) -> bool:\n",
    "        \"\"\"Test BGE-M3 connection and show embedding dimensions\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            embeddings = self.bge_model.encode(\n",
    "                test_text,\n",
    "                return_dense=True,\n",
    "                return_sparse=True,\n",
    "                return_colbert_vecs=False\n",
    "            )\n",
    "            \n",
    "            dense = embeddings['dense_vecs']\n",
    "            sparse = embeddings['lexical_weights']\n",
    "            \n",
    "            print(f\"✅ BGE-M3 test başarılı\")\n",
    "            print(f\"📊 Dense embedding boyutu: {len(dense[0])}\")\n",
    "            print(f\"📊 Sparse embedding token sayısı: {len(sparse[0])}\")\n",
    "            print(f\"🔍 Dense sample: {dense[0][:5]}...\")\n",
    "            print(f\"🔍 Sparse sample keys: {list(sparse[0].keys())[:5]}...\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return False\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Create Qdrant collection with hybrid vector support\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Create collection with named vectors (dense + sparse)\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config={\n",
    "                        \"dense\": VectorParams(\n",
    "                            size=self.config.DENSE_DIM,\n",
    "                            distance=Distance.COSINE\n",
    "                        )\n",
    "                    },\n",
    "                    sparse_vectors_config={\n",
    "                        \"sparse\": SparseVectorParams(\n",
    "                            index=SparseIndexParams()\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "                print(f\"✅ Hybrid koleksiyon oluşturuldu: {collection_name}\")\n",
    "                print(f\"   Dense boyut: {self.config.DENSE_DIM}\")\n",
    "                print(f\"   Sparse: Aktif\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Chunk text using semantic chunking\"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_hybrid_embeddings(self, texts: List[str], batch_size: int = None) -> Tuple[List[List[float]], List[Dict]]:\n",
    "        \"\"\"Create both dense and sparse embeddings using BGE-M3\"\"\"\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_dense: List[List[float]] = []\n",
    "        all_sparse: List[Dict] = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        print(f\"🔮 BGE-M3 ile hybrid embedding oluşturuluyor: {total} metin (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                # Get both dense and sparse embeddings\n",
    "                embeddings = self.bge_model.encode(\n",
    "                    batch_texts,\n",
    "                    return_dense=True,\n",
    "                    return_sparse=True,\n",
    "                    return_colbert_vecs=False\n",
    "                )\n",
    "                \n",
    "                dense_vecs = embeddings['dense_vecs']\n",
    "                sparse_vecs = embeddings['lexical_weights']\n",
    "                \n",
    "                # Process dense vectors (normalize)\n",
    "                if not isinstance(dense_vecs, torch.Tensor):\n",
    "                    dense_t = torch.tensor(dense_vecs, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                else:\n",
    "                    dense_t = dense_vecs.to(self.config.DEVICE)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    normed_dense = l2_normalize_tensor(dense_t)\n",
    "                \n",
    "                # Convert to lists\n",
    "                batch_dense = [v.cpu().tolist() for v in normed_dense]\n",
    "                all_dense.extend(batch_dense)\n",
    "                \n",
    "                # Process sparse vectors\n",
    "                batch_sparse = convert_sparse_to_qdrant_format(sparse_vecs)\n",
    "                all_sparse.extend(batch_sparse)\n",
    "\n",
    "                print(f\"  📊 Batch işlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Hybrid embedding hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                # Fallback zero vectors\n",
    "                fallback_dense = [[0.0] * self.config.DENSE_DIM for _ in batch_texts]\n",
    "                fallback_sparse = [{'indices': [], 'values': []} for _ in batch_texts]\n",
    "                all_dense.extend(fallback_dense)\n",
    "                all_sparse.extend(fallback_sparse)\n",
    "\n",
    "        print(f\"✅ Hybrid embeddings oluşturuldu: {len(all_dense)} dense, {len(all_sparse)} sparse\")\n",
    "        \n",
    "        # Debug: verify dense vector dimensions\n",
    "        if all_dense:\n",
    "            sample_dense_dim = len(all_dense[0])\n",
    "            print(f\"🔍 Dense vector boyutu kontrolü: {sample_dense_dim} (hedef: {self.config.DENSE_DIM})\")\n",
    "            if sample_dense_dim != self.config.DENSE_DIM:\n",
    "                print(f\"❌ Boyut uyumsuzluğu tespit edildi!\")\n",
    "        \n",
    "        return all_dense, all_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"Process CSV file and create chunks\"\"\"\n",
    "        print(f\"📄 CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"❌ Ana metin sütunu bulunamadı\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            \n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Upload chunks with hybrid embeddings to Qdrant\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"🚀 {len(chunks)} chunk hybrid embedding ile Qdrant'a yükleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        dense_embeddings, sparse_embeddings = self.create_hybrid_embeddings(texts)\n",
    "\n",
    "        if len(dense_embeddings) != len(chunks) or len(sparse_embeddings) != len(chunks):\n",
    "            print(f\"❌ Embedding sayısı uyumsuz\")\n",
    "            return\n",
    "\n",
    "        points = []\n",
    "        for i, (chunk, dense, sparse) in enumerate(zip(chunks, dense_embeddings, sparse_embeddings)):\n",
    "            # Create vectors dictionary with named vectors\n",
    "            vectors = {\"dense\": dense}\n",
    "            \n",
    "            # Only add sparse if it has data\n",
    "            if sparse['indices'] and sparse['values']:\n",
    "                vectors[\"sparse\"] = {\n",
    "                    \"indices\": sparse['indices'],\n",
    "                    \"values\": sparse['values']\n",
    "                }\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        # Upload in batches\n",
    "        batch_size = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch_size):\n",
    "            try:\n",
    "                batch_points = points[i:i+batch_size]\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch_points\n",
    "                )\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i+batch_size, len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(\"🎉 Hybrid yükleme tamamlandı!\")\n",
    "\n",
    "    from qdrant_client.models import SearchRequest, NamedVector, NamedSparseVector, SparseVector\n",
    "\n",
    "def hybrid_search(self, query: str, limit: int = 10, score_threshold: float = None, \n",
    "                  dense_weight: float = None, sparse_weight: float = None) -> List[Dict]:\n",
    "    \n",
    "    try:\n",
    "        # Kullanılacak ağırlıkları al\n",
    "        dense_weight = dense_weight or self.config.DENSE_WEIGHT\n",
    "        sparse_weight = sparse_weight or self.config.SPARSE_WEIGHT\n",
    "\n",
    "        print(f\"🔍 Hybrid arama: dense_weight={dense_weight}, sparse_weight={sparse_weight}\")\n",
    "\n",
    "        # Query embeddings\n",
    "        embeddings = self.bge_model.encode(\n",
    "            [query],\n",
    "            return_dense=True,\n",
    "            return_sparse=True,\n",
    "            return_colbert_vecs=False\n",
    "        )\n",
    "        query_dense = embeddings['dense_vecs'][0]\n",
    "        query_sparse = embeddings['lexical_weights'][0]\n",
    "\n",
    "        # Dense vector normalize\n",
    "        query_dense_t = torch.tensor(query_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            query_dense_norm = l2_normalize_tensor(query_dense_t).cpu().tolist()\n",
    "\n",
    "        # Sparse vector Qdrant format\n",
    "        query_sparse_qdrant = SparseVector(\n",
    "            indices=[int(idx) for idx in query_sparse.keys()],\n",
    "            values=[float(val) for val in query_sparse.values()]\n",
    "        )\n",
    "\n",
    "        # Search batch\n",
    "        search_requests = [\n",
    "            SearchRequest(\n",
    "                vector=NamedVector(\n",
    "                    name=\"dense\",\n",
    "                    vector=query_dense_norm\n",
    "                ),\n",
    "                limit=limit * 2,\n",
    "                score_threshold=score_threshold,\n",
    "                with_payload=True\n",
    "            ),\n",
    "            SearchRequest(\n",
    "                vector=NamedSparseVector(\n",
    "                    name=\"sparse\",\n",
    "                    vector=query_sparse_qdrant\n",
    "                ),\n",
    "                limit=limit * 2,\n",
    "                score_threshold=score_threshold,\n",
    "                with_payload=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        results = self.qdrant_client.search_batch(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            requests=search_requests\n",
    "        )\n",
    "\n",
    "        dense_results = results[0]\n",
    "        sparse_results = results[1]\n",
    "\n",
    "        # Combine results with weights\n",
    "        combined_scores = {}\n",
    "        for r in dense_results:\n",
    "            combined_scores[r.id] = {\n",
    "                'dense_score': r.score * dense_weight,\n",
    "                'sparse_score': 0,\n",
    "                'payload': r.payload\n",
    "            }\n",
    "        for r in sparse_results:\n",
    "            if r.id in combined_scores:\n",
    "                combined_scores[r.id]['sparse_score'] = r.score * sparse_weight\n",
    "            else:\n",
    "                combined_scores[r.id] = {\n",
    "                    'dense_score': 0,\n",
    "                    'sparse_score': r.score * sparse_weight,\n",
    "                    'payload': r.payload\n",
    "                }\n",
    "\n",
    "        final_results = []\n",
    "        for point_id, scores in combined_scores.items():\n",
    "            final_score = scores['dense_score'] + scores['sparse_score']\n",
    "            final_results.append({\n",
    "                'score': final_score,\n",
    "                'dense_score': scores['dense_score'],\n",
    "                'sparse_score': scores['sparse_score'],\n",
    "                'payload': scores['payload']\n",
    "            })\n",
    "\n",
    "        # Sort by final score and limit\n",
    "        final_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return final_results[:limit]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Hybrid arama hatası: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "    def search_dense_only(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Search using only dense vectors\"\"\"\n",
    "        try:\n",
    "            query_embeddings = self.bge_model.encode([query], return_dense=True, return_sparse=False)\n",
    "            query_dense = query_embeddings['dense_vecs'][0]\n",
    "            \n",
    "            # Normalize\n",
    "            query_dense_t = torch.tensor(query_dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                query_dense_norm = l2_normalize_tensor(query_dense_t).cpu().tolist()\n",
    "            \n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=(\"dense\", query_dense_norm),\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            return [{'score': r.score, 'payload': r.payload} for r in results]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Dense arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def search_sparse_only(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Search using only sparse vectors\"\"\"\n",
    "        try:\n",
    "            query_embeddings = self.bge_model.encode([query], return_dense=False, return_sparse=True)\n",
    "            query_sparse = query_embeddings['lexical_weights'][0]\n",
    "            \n",
    "            query_sparse_qdrant = {\n",
    "                \"name\": \"sparse\",\n",
    "                \"indices\": [int(idx) for idx in query_sparse.keys()],\n",
    "                \"values\": [float(val) for val in query_sparse.values()]\n",
    "            }\n",
    "            \n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_sparse_qdrant,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            return [{'score': r.score, 'payload': r.payload} for r in results]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Sparse arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Get collection information\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3 Hybrid\",\n",
    "                \"dense_dim\": self.config.DENSE_DIM,\n",
    "                \"sparse_enabled\": True,\n",
    "                \"dense_weight\": self.config.DENSE_WEIGHT,\n",
    "                \"sparse_weight\": self.config.SPARSE_WEIGHT\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline\n",
    "# -------------------------\n",
    "class YargitayHybridPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitayHybridProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Run full pipeline: CSV -> chunks -> hybrid embeddings -> Qdrant\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Hybrid pipeline başlıyor\")\n",
    "        \n",
    "        if not self.processor.test_bge_connection():\n",
    "            return False\n",
    "        \n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"❌ Chunk bulunamadı\")\n",
    "            return False\n",
    "        \n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        \n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive search interface\"\"\"\n",
    "        print(\"\\n🔎 İnteraktif hybrid arama başlatıldı\")\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"1) Hybrid arama (Dense + Sparse)\")\n",
    "            print(\"2) Sadece Dense arama\")\n",
    "            print(\"3) Sadece Sparse arama\")\n",
    "            print(\"4) Hybrid ağırlık ayarları\")\n",
    "            print(\"5) Ana menü\")\n",
    "            \n",
    "            choice = input(\"Seçiminiz (1-5): \").strip()\n",
    "            \n",
    "            if choice == \"5\":\n",
    "                break\n",
    "            \n",
    "            if choice == \"4\":\n",
    "                try:\n",
    "                    dense_w = float(input(f\"Dense ağırlık (mevcut: {self.config.DENSE_WEIGHT}): \") or self.config.DENSE_WEIGHT)\n",
    "                    sparse_w = float(input(f\"Sparse ağırlık (mevcut: {self.config.SPARSE_WEIGHT}): \") or self.config.SPARSE_WEIGHT)\n",
    "                    self.config.DENSE_WEIGHT = dense_w\n",
    "                    self.config.SPARSE_WEIGHT = sparse_w\n",
    "                    print(f\"✅ Ağırlıklar güncellendi: Dense={dense_w}, Sparse={sparse_w}\")\n",
    "                except ValueError:\n",
    "                    print(\"❌ Geçersiz değer\")\n",
    "                continue\n",
    "            \n",
    "            if choice not in {\"1\", \"2\", \"3\"}:\n",
    "                print(\"❌ Geçersiz seçim\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if query.lower() in {'q', 'quit', 'exit'}:\n",
    "                break\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"Kaç sonuç? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "            \n",
    "            # Perform search based on choice\n",
    "            if choice == \"1\":\n",
    "                results = self.processor.hybrid_search(query, limit=limit)\n",
    "            elif choice == \"2\":\n",
    "                results = self.processor.search_dense_only(query, limit=limit)\n",
    "            elif choice == \"3\":\n",
    "                results = self.processor.search_sparse_only(query, limit=limit)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n📋 {len(results)} sonuç ({['Hybrid', 'Dense Only', 'Sparse Only'][int(choice)-1]} arama):\")\n",
    "            \n",
    "            for i, r in enumerate(results, 1):\n",
    "                p = r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                \n",
    "                # Show component scores for hybrid search\n",
    "                if choice == \"1\" and 'dense_score' in r and 'sparse_score' in r:\n",
    "                    print(f\"   (Dense: {r['dense_score']:.4f}, Sparse: {r['sparse_score']:.4f})\")\n",
    "                \n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                \n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text','')) > 300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"yargitay_hybrid_search\",\n",
    "        DENSE_DIM=512,  # 512-dimensional dense vectors\n",
    "        BATCH_SIZE=32,\n",
    "        DB_BATCH=64,\n",
    "        DENSE_WEIGHT=0.7,\n",
    "        SPARSE_WEIGHT=0.3\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayHybridPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 HYBRID SEARCH SİSTEMİ\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline çalıştır (CSV -> chunks -> hybrid embed -> qdrant)\")\n",
    "        print(\"2) İnteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini göster\")\n",
    "        print(\"4) Çıkış\")\n",
    "        \n",
    "        choice = input(\"Seçiminiz (1-4): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            print(\"✅ Tamamlandı\" if success else \"❌ Hata oluştu\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 Görüşürüz\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"✅ FlagEmbedding yüklü\")\n",
    "    except ImportError:\n",
    "        print(\"❌ FlagEmbedding bulunamadı — pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HİBRİT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (Dense + Sparse, 512 dim slice, L2 normalize, hibrit search)\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize tensor rows (L2)\n",
    "# -------------------------\n",
    "def l2_normalize_tensor(t: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    if t.dim() == 1:\n",
    "        norm = torch.norm(t).clamp(min=eps)\n",
    "        return t / norm\n",
    "    norm = torch.norm(t, dim=1, keepdim=True).clamp(min=eps)\n",
    "    return t / norm\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "    DB_BATCH: int = 256\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "        # Encoding & chunker\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "\n",
    "        # Model\n",
    "        print(f\"🔮 BGE-M3 yükleniyor: {config.BGE_MODEL_NAME} (device={config.DEVICE})\")\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "\n",
    "        # Qdrant\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"✅ Hazır - Cihaz: {device_name}\")\n",
    "\n",
    "    # Test connection & print dense+sparse\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            emb_res = self.bge_model.encode(test_text)\n",
    "            dense = emb_res['dense_vecs'][0] if isinstance(emb_res, dict) and 'dense_vecs' in emb_res else emb_res[0]\n",
    "            sparse_available = 'colbert_vecs' in emb_res\n",
    "            print(f\"✅ Dense embedding boyutu: {len(dense)}\")\n",
    "            print(f\"🔍 Sparse embedding mevcut: {sparse_available}\")\n",
    "            return len(dense)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "            if collection_name not in existing:\n",
    "                # Dense + Sparse (sparse için yine 512 dim)\n",
    "                vectors_config = {\n",
    "                    \"dense_vec\": VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE),\n",
    "                    \"sparse_vec\": VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "                }\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=vectors_config\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} (Dense+Sparse)\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text or not text.strip():\n",
    "            return []\n",
    "        try:\n",
    "            chunks = self.chunker(text)\n",
    "            result = []\n",
    "            for i, c in enumerate(chunks):\n",
    "                if c.strip():\n",
    "                    cd = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': c.strip(),\n",
    "                        'token_count': len(self.encoding.encode(c)),\n",
    "                        'char_count': len(c)\n",
    "                    }\n",
    "                    if metadata:\n",
    "                        cd.update(metadata)\n",
    "                    result.append(cd)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = None):\n",
    "        batch_size = batch_size or self.config.BATCH_SIZE\n",
    "        all_embeddings_dense, all_embeddings_sparse = [], []\n",
    "        total = len(texts)\n",
    "        print(f\"🔮 {total} metin işleniyor (batch_size={batch_size})...\")\n",
    "\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            try:\n",
    "                emb_res = self.bge_model.encode(batch_texts)\n",
    "\n",
    "                dense = emb_res.get('dense_vecs', emb_res)\n",
    "                sparse = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)]*len(batch_texts))\n",
    "\n",
    "                dense_t = torch.tensor(dense, dtype=torch.float32, device=self.config.DEVICE)\n",
    "                sparse_t = torch.tensor(sparse, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dense_slice = dense_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    dense_norm = l2_normalize_tensor(dense_slice)\n",
    "                    sparse_slice = sparse_t[:, :self.config.EMBEDDING_DIM]\n",
    "                    sparse_norm = l2_normalize_tensor(sparse_slice)\n",
    "\n",
    "                all_embeddings_dense.extend([v.cpu().tolist() for v in dense_norm])\n",
    "                all_embeddings_sparse.extend([v.cpu().tolist() for v in sparse_norm])\n",
    "\n",
    "                print(f\"  📊 Batch işlendi: {i + len(batch_texts)}/{total}\")\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Embedding hatası (batch {i//batch_size+1}): {e}\")\n",
    "                all_embeddings_dense.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "                all_embeddings_sparse.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch_texts])\n",
    "\n",
    "        return all_embeddings_dense, all_embeddings_sparse\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        print(f\"📄 CSV okunuyor: {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "        text_column = next((c for c in ['rawText', 'chunk_text', 'text', 'content', 'metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"❌ Ana metin sütunu bulunamadı\")\n",
    "            return []\n",
    "\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            meta = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            chunks = self.semantic_chunk_text(str(text), meta)\n",
    "            all_chunks.extend(chunks)\n",
    "            if (idx+1)%5==0:\n",
    "                print(f\"  ✅ İşlenen satır: {idx+1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "\n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "\n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        texts = [c['text'] for c in chunks]\n",
    "        embeddings_dense, embeddings_sparse = self.create_embeddings_bge(texts)\n",
    "\n",
    "        points = []\n",
    "        \n",
    "        for c, d, s in zip(chunks, embeddings_dense, embeddings_sparse):\n",
    "            vectors={'dense_vec': d, 'sparse_vec': s} \n",
    "            points.append(PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors,\n",
    "                payload=c,\n",
    "            ))\n",
    "\n",
    "        batch = self.config.DB_BATCH\n",
    "        for i in range(0, len(points), batch):\n",
    "            try:\n",
    "                self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+batch])\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i+batch,len(points))}/{len(points)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "\n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            if dense_q is None or any(v is None for v in dense_q[0]):\n",
    "                dense_q = [np.zeros(self.config.EMBEDDING_DIM)]\n",
    "\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "            if sparse_q is None or any(v is None for v in sparse_q[0]):\n",
    "                sparse_q = [np.zeros(self.config.EMBEDDING_DIM)]\n",
    "\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=None,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                # Sparse hibrit param Qdrant 1.2+\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = None):\n",
    "        try:\n",
    "            emb_res = self.bge_model.encode([query])\n",
    "            dense_q = emb_res.get('dense_vecs', emb_res)\n",
    "            sparse_q = emb_res.get('colbert_vecs', [np.zeros(self.config.EMBEDDING_DIM)])\n",
    "\n",
    "            dense_t = torch.tensor(dense_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "            sparse_t = torch.tensor(sparse_q, dtype=torch.float32, device=self.config.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dense_norm = l2_normalize_tensor(dense_t[:, :self.config.EMBEDDING_DIM])\n",
    "                sparse_norm = l2_normalize_tensor(sparse_t[:, :self.config.EMBEDDING_DIM])\n",
    "\n",
    "            query_vector = dense_norm[0].cpu().tolist()\n",
    "            query_sparse = sparse_norm[0].cpu().tolist()\n",
    "\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k,v in filters.items()]\n",
    "                query_filter = Filter(must=conditions)\n",
    "\n",
    "            qr = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "                params={\"hnsw_ef\": 128},\n",
    "                vector_name=\"dense_vec\",\n",
    "                query_vector_sparse=query_sparse,\n",
    "                vector_name_sparse=\"sparse_vec\",\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in qr]\n",
    "            print(f\"📊 {len(results)} filtreli sonuç bulundu\")\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Filtreli arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline\n",
    "# -------------------------\n",
    "class YargitayPipeline:\n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "\n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        print(\"🚀 Full pipeline başlıyor\")\n",
    "        emb_dim = self.processor.test_bge_connection()\n",
    "        if not emb_dim:\n",
    "            return False\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ Chunk bulunamadı\")\n",
    "            return False\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        return True\n",
    "\n",
    "    def interactive_search(self):\n",
    "        print(\"\\n🔎 İnteraktif arama başlatıldı\")\n",
    "        while True:\n",
    "            print(\"\\n1) Basit arama\\n2) Filtreli arama\\n3) Ana menü\")\n",
    "            ch = input(\"Seçiminiz (1-3): \").strip()\n",
    "            if ch==\"3\":\n",
    "                break\n",
    "            if ch not in {\"1\",\"2\"}:\n",
    "                print(\"❌ Geçersiz seçim\")\n",
    "                continue\n",
    "            q = input(\"🔍 Arama metni (çıkmak için 'q'): \").strip()\n",
    "            if q.lower() in {'q','quit','exit'}:\n",
    "                break\n",
    "            if not q:\n",
    "                continue\n",
    "            try:\n",
    "                limit = int(input(\"Kaç sonuç? (default 5): \") or 5)\n",
    "            except:\n",
    "                limit = 5\n",
    "\n",
    "            if ch==\"1\":\n",
    "                results = self.processor.search_semantic(q, limit=limit)\n",
    "            else:\n",
    "                daire = input(\"Daire filtresi (örn: '6.HukukDairesi', boş = none): \").strip()\n",
    "                filters = {'daire': daire} if daire else None\n",
    "                results = self.processor.advanced_search_with_filters(q, filters=filters, limit=limit)\n",
    "\n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n📋 {len(results)} sonuç:\")\n",
    "            for i,r in enumerate(results,1):\n",
    "                p=r['payload']\n",
    "                print(f\"\\n{i}. Skor: {r['score']:.4f}\")\n",
    "                print(f\"   Esas No: {p.get('esas_no','N/A')} | Karar No: {p.get('karar_no','N/A')}\")\n",
    "                print(f\"   Daire: {p.get('daire','N/A')} | Tarih: {p.get('tarih','N/A')}\")\n",
    "                text_preview = (p.get('text','')[:300] + '...') if len(p.get('text',''))>300 else p.get('text','')\n",
    "                print(f\"   Metin: {text_preview}\")\n",
    "                print(\"-\"*60)\n",
    "\n",
    "def main():\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_hybrid_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100\n",
    "    )\n",
    "\n",
    "    pipeline = YargitayPipeline(config)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK SİSTEM (Dense+Sparse)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1) Tam pipeline çalıştır (CSV -> chunks -> embed -> qdrant)\")\n",
    "        print(\"2) İnteraktif arama\")\n",
    "        print(\"3) Koleksiyon bilgilerini göster\")\n",
    "        print(\"4) Çıkış\")\n",
    "        choice = input(\"Seçiminiz (1-4): \").strip()\n",
    "        if choice==\"1\":\n",
    "            csv_path = input(f\"CSV yolu (enter ile default: {config.CSV_FILE}): \").strip() or config.CSV_FILE\n",
    "            ok = pipeline.full_pipeline(csv_path)\n",
    "            print(\"✅ Tamamlandı\" if ok else \"❌ Hata çıktı\")\n",
    "        elif choice==\"2\":\n",
    "            pipeline.interactive_search()\n",
    "        elif choice==\"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        elif choice==\"4\":\n",
    "            print(\"👋 Görüşürüz\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"✅ FlagEmbedding yüklü\")\n",
    "    except ImportError:\n",
    "        print(\"❌ FlagEmbedding bulunamadı — pip install FlagEmbedding\")\n",
    "        raise SystemExit(1)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
