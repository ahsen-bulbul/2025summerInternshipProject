{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "✅ FlagEmbedding kütüphanesi yüklü\n",
      "🚀 GPU kullanılıyor: NVIDIA RTX A6000\n",
      "🔮 BGE-M3 modeli yükleniyor... (BAAI/bge-m3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 243383.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SemChunk chunker hazır (Token boyutu: 512)\n",
      "✅ BGE-M3 model hazır (BAAI/bge-m3)\n",
      "✅ Qdrant client hazır (http://localhost:6333)\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK ARAMA SİSTEMİ\n",
      "==================================================\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Query vector boyutu: 512 (hedef: 512)\n",
      "📊 0 sonuç bulundu\n",
      "❌ Sonuç bulunamadı\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3679998/561803670.py:349: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "👋 Görüşürüz!\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon\n",
    "# Yargıtay Kararları için Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# Konfigürasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # BGE-M3 ayarları\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"  # BGE-M3 model\n",
    "    USE_FP16: bool = True  # Hafıza optimizasyonu\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # SemChunk ayarları\n",
    "    TOKEN_SIZE: int = 512  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarları\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512  # BGE-M3 dense embedding boyutu\n",
    "    \n",
    "    # Dosya ayarları\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100  # BGE-M3 için optimize edilmiş batch size\n",
    "    db_batch=256\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargıtay kararları için semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # GPU/CPU kontrolü\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"🚀 GPU kullanılıyor: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            print(\"💻 CPU kullanılıyor\")\n",
    "        \n",
    "        # SemChunk chunker oluştur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # BGE-M3 modelini yükle\n",
    "        print(f\"🔮 BGE-M3 modeli yükleniyor... ({config.BGE_MODEL_NAME})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        self.reducer = EmbedReducer(1024, 512).to('cuda:0')\n",
    "        # Qdrant client oluştur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"✅ SemChunk chunker hazır (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"✅ BGE-M3 model hazır ({config.BGE_MODEL_NAME})\")\n",
    "        print(f\"✅ Qdrant client hazır ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_bge_connection(self):\n",
    "        \"\"\"BGE-M3 modelini test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            \n",
    "            # BGE-M3'den dense embedding al\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            embedding_dim = len(dense_embedding)\n",
    "            \n",
    "            print(f\"✅ BGE-M3 test başarılı - Dense embedding boyutu: {embedding_dim}\")\n",
    "            print(f\"🔍 Sparse embedding mevcut: {'colbert_vecs' in embeddings}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu oluştur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa oluştur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Koleksiyon oluşturuldu: {collection_name} (Boyut: {self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"ℹ️ Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Koleksiyon oluşturma hatası: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara böl\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni böl\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Boş chunk'ları atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Chunking hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.BATCH_SIZE\n",
    "            \n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        print(f\"🔮 BGE-M3 ile {len(texts)} metin işleniyor...\")\n",
    "        \n",
    "        # BGE-M3 için batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # BGE-M3 ile embedding oluştur\n",
    "                embeddings_result = self.bge_model.encode(batch_texts)\n",
    "                \n",
    "                # dense_vecs key kontrolü\n",
    "                if isinstance(embeddings_result, dict) and 'dense_vecs' in embeddings_result:\n",
    "                    dense_embeddings = embeddings_result['dense_vecs']\n",
    "                else:\n",
    "                    dense_embeddings = embeddings_result  # direkt tensor veya list ise\n",
    "                \n",
    "                # Tensor'a çevir ve GPU'ya taşı\n",
    "                if not isinstance(dense_embeddings, torch.Tensor):\n",
    "                    dense_embeddings = torch.tensor(dense_embeddings, device=self.config.DEVICE, dtype=torch.float32)\n",
    "                else:\n",
    "                    dense_embeddings = dense_embeddings.to(self.config.DEVICE)\n",
    "                \n",
    "                # Reducer ile boyut küçültme\n",
    "                reduced_vector = self.reducer(dense_embeddings)\n",
    "                \n",
    "                # Listeye çevir\n",
    "                for embedding in reduced_vector:\n",
    "                    all_embeddings.append(embedding.detach().cpu().tolist())\n",
    "                \n",
    "                print(f\"  📊 BGE-M3 Embedding: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "                # GPU memory temizliği (gerekirse)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ BGE-M3 Embedding hatası (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda sıfır embedding ekle\n",
    "                for _ in batch_texts:\n",
    "                    all_embeddings.append([0.0] * self.config.EMBEDDING_DIM)\n",
    "        \n",
    "        return all_embeddings\n",
    "\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasını işle ve chunk'ları oluştur\"\"\"\n",
    "        print(f\"📄 CSV dosyası okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"📊 {len(df)} satır veri yüklendi\")\n",
    "            print(f\"📋 Mevcut sütunlar: {df.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ CSV okuma hatası: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Ana metin sütununu belirle (öncelik sırasına göre)\n",
    "        text_columns = ['rawText', 'chunk_text', 'text', 'content', 'metin']\n",
    "        text_column = None\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"✅ Ana metin sütunu bulundu: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not text_column:\n",
    "            print(f\"❌ Ana metin sütunu bulunamadı. Kontrol edilen sütunlar: {text_columns}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"🔄 Semantic chunking başlıyor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get(text_column, '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                print(f\"⚠️ Satır {idx}: Boş metin atlandı\")\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazırla (CSV yapınıza göre güncellenmiş)\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress göster\n",
    "            if (idx + 1) % 5 == 0:  # Daha sık progress göster (az veri olduğu için)\n",
    "                print(f\"  ✅ İşlenen satır: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"🧩 Toplam {len(all_chunks)} chunk oluşturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'ları Qdrant'a yükle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"🚀 {len(chunks)} chunk Qdrant'a yükleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # BGE-M3 ile embedding'leri oluştur\n",
    "        print(\"🔮 BGE-M3 embedding'ler oluşturuluyor...\")\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"❌ Embedding sayısı uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazırla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yükle\n",
    "        batch_size = 256\n",
    "        print(f\"📦 {batch_size} batch size ile yükleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ✅ Batch yüklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch yükleme hatası: {e}\")\n",
    "        \n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"Düzeltilmiş search metodu - 512 dimension uyumlu\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Query embedding çıkar (BGE-M3 dense embedding, 1024 boyut)\n",
    "            query_embedding_result = self.bge_model.encode([query])\n",
    "            \n",
    "            # dense_vecs key kontrolü\n",
    "            if isinstance(query_embedding_result, dict) and 'dense_vecs' in query_embedding_result:\n",
    "                query_embedding = query_embedding_result['dense_vecs']\n",
    "            else:\n",
    "                query_embedding = query_embedding_result\n",
    "            \n",
    "            # Tensor'a çevir ve GPU'ya taşı\n",
    "            if not isinstance(query_embedding, torch.Tensor):\n",
    "                query_embedding = torch.tensor(query_embedding, device=self.config.DEVICE, dtype=torch.float32)\n",
    "            else:\n",
    "                query_embedding = query_embedding.to(self.config.DEVICE)\n",
    "            \n",
    "            query_embedding = query_embedding.clone().detach()\n",
    "            \n",
    "            # Boyutu 512'ye düşür (reducer)\n",
    "            with torch.no_grad():\n",
    "                reduced_query_embedding = self.reducer(query_embedding)\n",
    "            \n",
    "            # CPU'ya taşı ve listeye çevir\n",
    "            query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "            print(f\"🔍 Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "            \n",
    "            # Qdrant araması\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonuçları formatla\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "            print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Arama hatası: {e}\")\n",
    "            return []\n",
    "\n",
    "    # def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "    #     \"\"\"BGE-M3 ile semantic arama yap\"\"\"\n",
    "    #     print(f\"🔍 Arama: '{query}'\")\n",
    "        \n",
    "    #     try:\n",
    "    #         # Query'yi BGE-M3 ile vektörize et\n",
    "    #         query_embeddings = self.bge_model.encode([query]).to('cuda:0')\n",
    "    #         query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "    #         # Qdrant'ta ara (güncel query_points metodu)\n",
    "    #         search_results = self.qdrant_client.query_points(\n",
    "    #             collection_name=self.config.COLLECTION_NAME,\n",
    "    #             query=query_vector,\n",
    "    #             limit=limit,\n",
    "    #             score_threshold=score_threshold\n",
    "    #         )\n",
    "            \n",
    "    #         # Sonuçları formatla\n",
    "    #         results = []\n",
    "    #         for point in search_results.points:#burda muhtemel hata verir search_results olcak verirse\n",
    "    #             results.append({\n",
    "    #                 'score': point.score,\n",
    "    #                 'payload': point.payload\n",
    "    #             })\n",
    "            \n",
    "    #         print(f\"📊 {len(results)} sonuç bulundu\")\n",
    "    #         return results\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"❌ Arama hatası: {e}\")\n",
    "    #         return []\n",
    "    \n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"Filtreli arama yap\"\"\"\n",
    "        print(f\"🔍 Filtreli arama: '{query}' - Filtreler: {filters}\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vektörize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Filter oluştur\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = []\n",
    "                for key, value in filters.items():\n",
    "                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "                query_filter = Filter(must=conditions)\n",
    "            \n",
    "            # Qdrant'ta filtreli arama yap\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonuçları formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"📊 {len(results)} filtreli sonuç bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Filtreli arama hatası: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sınıfı\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sınıfı\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ı çalıştır\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"🚀 Yargıtay BGE-M3 Semantic Pipeline Başlıyor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon oluştur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi işle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk bulunamadı\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a yükle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri göster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"İnteraktif arama arayüzü\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK ARAMA SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n🔍 Arama Seçenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana menüye dön\")\n",
    "            \n",
    "            search_choice = input(\"Seçiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"❌ Geçersiz seçim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\n🔍 Arama metni (çıkmak için 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"📊 Kaç sonuç? (varsayılan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"🎯 Minimum benzerlik skoru? (varsayılan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\n🔧 Filtre Seçenekleri (boş bırakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (örn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n📋 {len(results)} sonuç bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. 📄 BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ⚖️ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   📋 Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   🏛️ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   📅 Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   🔤 Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   📝 Metin Önizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanım örneği ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfigürasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100,  # GPU memory'ye göre ayarlayın\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline oluştur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Menü göster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\")\n",
    "        print(\"2. İnteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini göster\")\n",
    "        print(\"4. Çıkış\")\n",
    "        \n",
    "        choice = input(\"\\nSeçiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"✅ BGE-M3 Pipeline başarıyla tamamlandı!\")\n",
    "            else:\n",
    "                print(\"❌ Pipeline hatası!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 Görüşürüz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrolü\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"✅ FlagEmbedding kütüphanesi yüklü\")\n",
    "    except ImportError:\n",
    "        print(\"❌ FlagEmbedding kütüphanesi bulunamadı!\")\n",
    "        print(\"Kurulum için: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (512 boyut embedding)\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# Embed Reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(1024, 512).to(config.DEVICE)\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"✅ Model ve sistem hazır - Kullanılan cihaz: {device_name}\")\n",
    "\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargıtay 6. Hukuk Dairesi'nin ihtiyati tedbir kararı\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            print(f\"✅ BGE-M3 test başarılı - Dense embedding boyutu: {len(dense_embedding)}\")\n",
    "            return len(dense_embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ BGE-M3 bağlantı hatası: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(self.config.COLLECTION_NAME)\n",
    "                print(f\"🗑️ Eski koleksiyon silindi\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "        if self.config.COLLECTION_NAME not in existing:\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "            )\n",
    "            print(f\"✅ Koleksiyon oluşturuldu: {self.config.COLLECTION_NAME}\")\n",
    "        else:\n",
    "            print(f\"ℹ️ Koleksiyon zaten var\")\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text)\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str]) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.config.BATCH_SIZE):\n",
    "            batch = texts[i:i + self.config.BATCH_SIZE]\n",
    "            try:\n",
    "                emb_result = self.bge_model.encode(batch)\n",
    "                dense_embeddings = emb_result['dense_vecs']\n",
    "                tensor_emb = torch.tensor(dense_embeddings, device=self.config.DEVICE, dtype=torch.float32)\n",
    "                reduced = self.reducer(tensor_emb)\n",
    "                all_embeddings.extend([v.detach().cpu().tolist() for v in reduced])\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch embedding hatası: {e}\")\n",
    "                all_embeddings.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch])\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        text_column = next((c for c in ['rawText','chunk_text','text','content','metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"❌ Ana metin sütunu bulunamadı\")\n",
    "            return []\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            all_chunks.extend(self.semantic_chunk_text(str(text), metadata))\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"❌ Yüklenecek chunk yok\")\n",
    "            return\n",
    "        embeddings = self.create_embeddings_bge([c['text'] for c in chunks])\n",
    "        points = [PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk)\n",
    "                  for chunk, emb in zip(chunks, embeddings)]\n",
    "        for i in range(0, len(points), 256):\n",
    "            self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+256])\n",
    "        print(\"🎉 Yükleme tamamlandı!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7):\n",
    "        query_emb_result = self.bge_model.encode([query])\n",
    "        query_tensor = torch.tensor(query_emb_result['dense_vecs'], device=self.config.DEVICE, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            reduced_query = self.reducer(query_tensor)\n",
    "        query_vector = reduced_query[0].cpu().tolist()\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query=query_vector,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        return [{'score': p.score, 'payload': p.payload} for p in results.points]\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6):\n",
    "        query_emb_result = self.bge_model.encode([query])\n",
    "        query_vector = query_emb_result['dense_vecs'][0].tolist()\n",
    "        query_filter = None\n",
    "        if filters:\n",
    "            from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "            conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "            query_filter = Filter(must=conditions)\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query=query_vector,\n",
    "            query_filter=query_filter,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        return [{'score': p.score, 'payload': p.payload} for p in results.points]\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\"collection_name\": self.config.COLLECTION_NAME, \"points_count\": info.points_count,\n",
    "                    \"vectors_count\": info.vectors_count, \"status\": info.status, \"embedding_model\": \"BGE-M3\",\n",
    "                    \"embedding_dim\": self.config.EMBEDDING_DIM}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Pipeline ve main fonksiyon aynı mantıkla entegre edildi\n",
    "# interactive_search ve full_pipeline fonksiyonları reducer uyumlu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sınıfı\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ı çalıştır\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"🚀 Yargıtay BGE-M3 Semantic Pipeline Başlıyor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon oluştur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi işle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"❌ İşlenecek chunk bulunamadı\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a yükle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri göster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"İnteraktif arama arayüzü\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK ARAMA SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n🔍 Arama Seçenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana menüye dön\")\n",
    "            \n",
    "            search_choice = input(\"Seçiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"❌ Geçersiz seçim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\n🔍 Arama metni (çıkmak için 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"📊 Kaç sonuç? (varsayılan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"🎯 Minimum benzerlik skoru? (varsayılan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\n🔧 Filtre Seçenekleri (boş bırakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (örn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"❌ Sonuç bulunamadı\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n📋 {len(results)} sonuç bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. 📄 BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ⚖️ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   📋 Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   🏛️ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   📅 Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   🔤 Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   📝 Metin Önizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanım örneği ve main fonksiyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FlagEmbedding kütüphanesi yüklü\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 102717.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ve sistem hazır - Kullanılan cihaz: NVIDIA RTX A6000\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK ARAMA SİSTEMİ\n",
      "==================================================\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Sonuç bulunamadı\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n",
      "❌ Geçersiz seçim!\n",
      "\n",
      "🔍 Arama Seçenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menüye dön\n",
      "\n",
      "==================================================\n",
      "🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\n",
      "==================================================\n",
      "1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\n",
      "2. İnteraktif arama yap\n",
      "3. Koleksiyon bilgilerini göster\n",
      "4. Çıkış\n",
      "👋 Görüşürüz!\n"
     ]
    }
   ],
   "source": [
    "# Kullanım örneği ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfigürasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100,  # GPU memory'ye göre ayarlayın\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline oluştur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Menü göster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"🏛️ YARGITAY BGE-M3 SEMANTİK CHUNK SİSTEMİ\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline çalıştır (CSV → Semantic Chunks → BGE-M3 → Qdrant)\")\n",
    "        print(\"2. İnteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini göster\")\n",
    "        print(\"4. Çıkış\")\n",
    "        \n",
    "        choice = input(\"\\nSeçiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"✅ BGE-M3 Pipeline başarıyla tamamlandı!\")\n",
    "            else:\n",
    "                print(\"❌ Pipeline hatası!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\n📊 Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"👋 Görüşürüz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"❌ Geçersiz seçim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrolü\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"✅ FlagEmbedding kütüphanesi yüklü\")\n",
    "    except ImportError:\n",
    "        print(\"❌ FlagEmbedding kütüphanesi bulunamadı!\")\n",
    "        print(\"Kurulum için: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
