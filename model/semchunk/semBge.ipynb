{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "âœ… FlagEmbedding kÃ¼tÃ¼phanesi yÃ¼klÃ¼\n",
      "ðŸš€ GPU kullanÄ±lÄ±yor: NVIDIA RTX A6000\n",
      "ðŸ”® BGE-M3 modeli yÃ¼kleniyor... (BAAI/bge-m3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 243383.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SemChunk chunker hazÄ±r (Token boyutu: 512)\n",
      "âœ… BGE-M3 model hazÄ±r (BAAI/bge-m3)\n",
      "âœ… Qdrant client hazÄ±r (http://localhost:6333)\n",
      "\n",
      "==================================================\n",
      "ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K CHUNK SÄ°STEMÄ°\n",
      "==================================================\n",
      "1. Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV â†’ Semantic Chunks â†’ BGE-M3 â†’ Qdrant)\n",
      "2. Ä°nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini gÃ¶ster\n",
      "4. Ã‡Ä±kÄ±ÅŸ\n",
      "\n",
      "==================================================\n",
      "ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K ARAMA SÄ°STEMÄ°\n",
      "==================================================\n",
      "\n",
      "ðŸ” Arama SeÃ§enekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menÃ¼ye dÃ¶n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query vector boyutu: 512 (hedef: 512)\n",
      "ðŸ“Š 0 sonuÃ§ bulundu\n",
      "âŒ SonuÃ§ bulunamadÄ±\n",
      "\n",
      "ðŸ” Arama SeÃ§enekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menÃ¼ye dÃ¶n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3679998/561803670.py:349: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K CHUNK SÄ°STEMÄ°\n",
      "==================================================\n",
      "1. Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV â†’ Semantic Chunks â†’ BGE-M3 â†’ Qdrant)\n",
      "2. Ä°nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini gÃ¶ster\n",
      "4. Ã‡Ä±kÄ±ÅŸ\n",
      "ðŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon\n",
    "# YargÄ±tay KararlarÄ± iÃ§in Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# KonfigÃ¼rasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # BGE-M3 ayarlarÄ±\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"  # BGE-M3 model\n",
    "    USE_FP16: bool = True  # HafÄ±za optimizasyonu\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # SemChunk ayarlarÄ±\n",
    "    TOKEN_SIZE: int = 512  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarlarÄ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512  # BGE-M3 dense embedding boyutu\n",
    "    \n",
    "    # Dosya ayarlarÄ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100  # BGE-M3 iÃ§in optimize edilmiÅŸ batch size\n",
    "    db_batch=256\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"YargÄ±tay kararlarÄ± iÃ§in semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # GPU/CPU kontrolÃ¼\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"ðŸš€ GPU kullanÄ±lÄ±yor: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            print(\"ðŸ’» CPU kullanÄ±lÄ±yor\")\n",
    "        \n",
    "        # SemChunk chunker oluÅŸtur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # BGE-M3 modelini yÃ¼kle\n",
    "        print(f\"ðŸ”® BGE-M3 modeli yÃ¼kleniyor... ({config.BGE_MODEL_NAME})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        self.reducer = EmbedReducer(1024, 512).to('cuda:0')\n",
    "        # Qdrant client oluÅŸtur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"âœ… SemChunk chunker hazÄ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"âœ… BGE-M3 model hazÄ±r ({config.BGE_MODEL_NAME})\")\n",
    "        print(f\"âœ… Qdrant client hazÄ±r ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_bge_connection(self):\n",
    "        \"\"\"BGE-M3 modelini test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"YargÄ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararÄ±\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            \n",
    "            # BGE-M3'den dense embedding al\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            embedding_dim = len(dense_embedding)\n",
    "            \n",
    "            print(f\"âœ… BGE-M3 test baÅŸarÄ±lÄ± - Dense embedding boyutu: {embedding_dim}\")\n",
    "            print(f\"ðŸ” Sparse embedding mevcut: {'colbert_vecs' in embeddings}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ BGE-M3 baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu oluÅŸtur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"ðŸ—‘ï¸ Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa oluÅŸtur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"âœ… Koleksiyon oluÅŸturuldu: {collection_name} (Boyut: {self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"â„¹ï¸ Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Koleksiyon oluÅŸturma hatasÄ±: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara bÃ¶l\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni bÃ¶l\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # BoÅŸ chunk'larÄ± atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Chunking hatasÄ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.BATCH_SIZE\n",
    "            \n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        print(f\"ðŸ”® BGE-M3 ile {len(texts)} metin iÅŸleniyor...\")\n",
    "        \n",
    "        # BGE-M3 iÃ§in batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # BGE-M3 ile embedding oluÅŸtur\n",
    "                embeddings_result = self.bge_model.encode(batch_texts)\n",
    "                \n",
    "                # dense_vecs key kontrolÃ¼\n",
    "                if isinstance(embeddings_result, dict) and 'dense_vecs' in embeddings_result:\n",
    "                    dense_embeddings = embeddings_result['dense_vecs']\n",
    "                else:\n",
    "                    dense_embeddings = embeddings_result  # direkt tensor veya list ise\n",
    "                \n",
    "                # Tensor'a Ã§evir ve GPU'ya taÅŸÄ±\n",
    "                if not isinstance(dense_embeddings, torch.Tensor):\n",
    "                    dense_embeddings = torch.tensor(dense_embeddings, device=self.config.DEVICE, dtype=torch.float32)\n",
    "                else:\n",
    "                    dense_embeddings = dense_embeddings.to(self.config.DEVICE)\n",
    "                \n",
    "                # Reducer ile boyut kÃ¼Ã§Ã¼ltme\n",
    "                reduced_vector = self.reducer(dense_embeddings)\n",
    "                \n",
    "                # Listeye Ã§evir\n",
    "                for embedding in reduced_vector:\n",
    "                    all_embeddings.append(embedding.detach().cpu().tolist())\n",
    "                \n",
    "                print(f\"  ðŸ“Š BGE-M3 Embedding: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "                # GPU memory temizliÄŸi (gerekirse)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ BGE-M3 Embedding hatasÄ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda sÄ±fÄ±r embedding ekle\n",
    "                for _ in batch_texts:\n",
    "                    all_embeddings.append([0.0] * self.config.EMBEDDING_DIM)\n",
    "        \n",
    "        return all_embeddings\n",
    "\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasÄ±nÄ± iÅŸle ve chunk'larÄ± oluÅŸtur\"\"\"\n",
    "        print(f\"ðŸ“„ CSV dosyasÄ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"ðŸ“Š {len(df)} satÄ±r veri yÃ¼klendi\")\n",
    "            print(f\"ðŸ“‹ Mevcut sÃ¼tunlar: {df.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ CSV okuma hatasÄ±: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Ana metin sÃ¼tununu belirle (Ã¶ncelik sÄ±rasÄ±na gÃ¶re)\n",
    "        text_columns = ['rawText', 'chunk_text', 'text', 'content', 'metin']\n",
    "        text_column = None\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"âœ… Ana metin sÃ¼tunu bulundu: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not text_column:\n",
    "            print(f\"âŒ Ana metin sÃ¼tunu bulunamadÄ±. Kontrol edilen sÃ¼tunlar: {text_columns}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"ðŸ”„ Semantic chunking baÅŸlÄ±yor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get(text_column, '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                print(f\"âš ï¸ SatÄ±r {idx}: BoÅŸ metin atlandÄ±\")\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazÄ±rla (CSV yapÄ±nÄ±za gÃ¶re gÃ¼ncellenmiÅŸ)\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress gÃ¶ster\n",
    "            if (idx + 1) % 5 == 0:  # Daha sÄ±k progress gÃ¶ster (az veri olduÄŸu iÃ§in)\n",
    "                print(f\"  âœ… Ä°ÅŸlenen satÄ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"ðŸ§© Toplam {len(all_chunks)} chunk oluÅŸturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'larÄ± Qdrant'a yÃ¼kle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ðŸš€ {len(chunks)} chunk Qdrant'a yÃ¼kleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # BGE-M3 ile embedding'leri oluÅŸtur\n",
    "        print(\"ðŸ”® BGE-M3 embedding'ler oluÅŸturuluyor...\")\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"âŒ Embedding sayÄ±sÄ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazÄ±rla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde yÃ¼kle\n",
    "        batch_size = 256\n",
    "        print(f\"ðŸ“¦ {batch_size} batch size ile yÃ¼kleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  âœ… Batch yÃ¼klendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch yÃ¼kleme hatasÄ±: {e}\")\n",
    "        \n",
    "        print(\"ðŸŽ‰ YÃ¼kleme tamamlandÄ±!\")\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"DÃ¼zeltilmiÅŸ search metodu - 512 dimension uyumlu\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Query embedding Ã§Ä±kar (BGE-M3 dense embedding, 1024 boyut)\n",
    "            query_embedding_result = self.bge_model.encode([query])\n",
    "            \n",
    "            # dense_vecs key kontrolÃ¼\n",
    "            if isinstance(query_embedding_result, dict) and 'dense_vecs' in query_embedding_result:\n",
    "                query_embedding = query_embedding_result['dense_vecs']\n",
    "            else:\n",
    "                query_embedding = query_embedding_result\n",
    "            \n",
    "            # Tensor'a Ã§evir ve GPU'ya taÅŸÄ±\n",
    "            if not isinstance(query_embedding, torch.Tensor):\n",
    "                query_embedding = torch.tensor(query_embedding, device=self.config.DEVICE, dtype=torch.float32)\n",
    "            else:\n",
    "                query_embedding = query_embedding.to(self.config.DEVICE)\n",
    "            \n",
    "            query_embedding = query_embedding.clone().detach()\n",
    "            \n",
    "            # Boyutu 512'ye dÃ¼ÅŸÃ¼r (reducer)\n",
    "            with torch.no_grad():\n",
    "                reduced_query_embedding = self.reducer(query_embedding)\n",
    "            \n",
    "            # CPU'ya taÅŸÄ± ve listeye Ã§evir\n",
    "            query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "            print(f\"ðŸ” Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "            \n",
    "            # Qdrant aramasÄ±\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # SonuÃ§larÄ± formatla\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "            print(f\"ðŸ“Š {len(results)} sonuÃ§ bulundu\")\n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    # def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "    #     \"\"\"BGE-M3 ile semantic arama yap\"\"\"\n",
    "    #     print(f\"ðŸ” Arama: '{query}'\")\n",
    "        \n",
    "    #     try:\n",
    "    #         # Query'yi BGE-M3 ile vektÃ¶rize et\n",
    "    #         query_embeddings = self.bge_model.encode([query]).to('cuda:0')\n",
    "    #         query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "    #         # Qdrant'ta ara (gÃ¼ncel query_points metodu)\n",
    "    #         search_results = self.qdrant_client.query_points(\n",
    "    #             collection_name=self.config.COLLECTION_NAME,\n",
    "    #             query=query_vector,\n",
    "    #             limit=limit,\n",
    "    #             score_threshold=score_threshold\n",
    "    #         )\n",
    "            \n",
    "    #         # SonuÃ§larÄ± formatla\n",
    "    #         results = []\n",
    "    #         for point in search_results.points:#burda muhtemel hata verir search_results olcak verirse\n",
    "    #             results.append({\n",
    "    #                 'score': point.score,\n",
    "    #                 'payload': point.payload\n",
    "    #             })\n",
    "            \n",
    "    #         print(f\"ðŸ“Š {len(results)} sonuÃ§ bulundu\")\n",
    "    #         return results\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"âŒ Arama hatasÄ±: {e}\")\n",
    "    #         return []\n",
    "    \n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"Filtreli arama yap\"\"\"\n",
    "        print(f\"ðŸ” Filtreli arama: '{query}' - Filtreler: {filters}\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vektÃ¶rize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Filter oluÅŸtur\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = []\n",
    "                for key, value in filters.items():\n",
    "                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "                query_filter = Filter(must=conditions)\n",
    "            \n",
    "            # Qdrant'ta filtreli arama yap\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # SonuÃ§larÄ± formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"ðŸ“Š {len(results)} filtreli sonuÃ§ bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Filtreli arama hatasÄ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline SÄ±nÄ±fÄ±\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sÄ±nÄ±fÄ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'Ä± Ã§alÄ±ÅŸtÄ±r\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"ðŸš€ YargÄ±tay BGE-M3 Semantic Pipeline BaÅŸlÄ±yor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon oluÅŸtur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi iÅŸle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"âŒ Ä°ÅŸlenecek chunk bulunamadÄ±\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a yÃ¼kle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri gÃ¶ster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nðŸ“Š Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Ä°nteraktif arama arayÃ¼zÃ¼\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K ARAMA SÄ°STEMÄ°\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nðŸ” Arama SeÃ§enekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana menÃ¼ye dÃ¶n\")\n",
    "            \n",
    "            search_choice = input(\"SeÃ§iminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"âŒ GeÃ§ersiz seÃ§im!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nðŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"ðŸ“Š KaÃ§ sonuÃ§? (varsayÄ±lan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"ðŸŽ¯ Minimum benzerlik skoru? (varsayÄ±lan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\nðŸ”§ Filtre SeÃ§enekleri (boÅŸ bÄ±rakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (Ã¶rn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"âŒ SonuÃ§ bulunamadÄ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nðŸ“‹ {len(results)} sonuÃ§ bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. ðŸ“„ BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   âš–ï¸ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   ðŸ“‹ Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   ðŸ›ï¸ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   ðŸ“… Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   ðŸ”¤ Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   ðŸ“ Metin Ã–nizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# KullanÄ±m Ã¶rneÄŸi ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # KonfigÃ¼rasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100,  # GPU memory'ye gÃ¶re ayarlayÄ±n\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline oluÅŸtur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # MenÃ¼ gÃ¶ster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K CHUNK SÄ°STEMÄ°\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV â†’ Semantic Chunks â†’ BGE-M3 â†’ Qdrant)\")\n",
    "        print(\"2. Ä°nteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini gÃ¶ster\")\n",
    "        print(\"4. Ã‡Ä±kÄ±ÅŸ\")\n",
    "        \n",
    "        choice = input(\"\\nSeÃ§iminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"âœ… BGE-M3 Pipeline baÅŸarÄ±yla tamamlandÄ±!\")\n",
    "            else:\n",
    "                print(\"âŒ Pipeline hatasÄ±!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nðŸ“Š Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"ðŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrolÃ¼\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"âœ… FlagEmbedding kÃ¼tÃ¼phanesi yÃ¼klÃ¼\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ FlagEmbedding kÃ¼tÃ¼phanesi bulunamadÄ±!\")\n",
    "        print(\"Kurulum iÃ§in: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (512 boyut embedding)\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# Embed Reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(1024, 512).to(config.DEVICE)\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"âœ… Model ve sistem hazÄ±r - KullanÄ±lan cihaz: {device_name}\")\n",
    "\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"YargÄ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararÄ±\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            print(f\"âœ… BGE-M3 test baÅŸarÄ±lÄ± - Dense embedding boyutu: {len(dense_embedding)}\")\n",
    "            return len(dense_embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ BGE-M3 baÄŸlantÄ± hatasÄ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(self.config.COLLECTION_NAME)\n",
    "                print(f\"ðŸ—‘ï¸ Eski koleksiyon silindi\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "        if self.config.COLLECTION_NAME not in existing:\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "            )\n",
    "            print(f\"âœ… Koleksiyon oluÅŸturuldu: {self.config.COLLECTION_NAME}\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ Koleksiyon zaten var\")\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text)\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str]) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.config.BATCH_SIZE):\n",
    "            batch = texts[i:i + self.config.BATCH_SIZE]\n",
    "            try:\n",
    "                emb_result = self.bge_model.encode(batch)\n",
    "                dense_embeddings = emb_result['dense_vecs']\n",
    "                tensor_emb = torch.tensor(dense_embeddings, device=self.config.DEVICE, dtype=torch.float32)\n",
    "                reduced = self.reducer(tensor_emb)\n",
    "                all_embeddings.extend([v.detach().cpu().tolist() for v in reduced])\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Batch embedding hatasÄ±: {e}\")\n",
    "                all_embeddings.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch])\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        text_column = next((c for c in ['rawText','chunk_text','text','content','metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"âŒ Ana metin sÃ¼tunu bulunamadÄ±\")\n",
    "            return []\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            all_chunks.extend(self.semantic_chunk_text(str(text), metadata))\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"âŒ YÃ¼klenecek chunk yok\")\n",
    "            return\n",
    "        embeddings = self.create_embeddings_bge([c['text'] for c in chunks])\n",
    "        points = [PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk)\n",
    "                  for chunk, emb in zip(chunks, embeddings)]\n",
    "        for i in range(0, len(points), 256):\n",
    "            self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+256])\n",
    "        print(\"ðŸŽ‰ YÃ¼kleme tamamlandÄ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7):\n",
    "        query_emb_result = self.bge_model.encode([query])\n",
    "        query_tensor = torch.tensor(query_emb_result['dense_vecs'], device=self.config.DEVICE, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            reduced_query = self.reducer(query_tensor)\n",
    "        query_vector = reduced_query[0].cpu().tolist()\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query=query_vector,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        return [{'score': p.score, 'payload': p.payload} for p in results.points]\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6):\n",
    "        query_emb_result = self.bge_model.encode([query])\n",
    "        query_vector = query_emb_result['dense_vecs'][0].tolist()\n",
    "        query_filter = None\n",
    "        if filters:\n",
    "            from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "            conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "            query_filter = Filter(must=conditions)\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query=query_vector,\n",
    "            query_filter=query_filter,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        return [{'score': p.score, 'payload': p.payload} for p in results.points]\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\"collection_name\": self.config.COLLECTION_NAME, \"points_count\": info.points_count,\n",
    "                    \"vectors_count\": info.vectors_count, \"status\": info.status, \"embedding_model\": \"BGE-M3\",\n",
    "                    \"embedding_dim\": self.config.EMBEDDING_DIM}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Pipeline ve main fonksiyon aynÄ± mantÄ±kla entegre edildi\n",
    "# interactive_search ve full_pipeline fonksiyonlarÄ± reducer uyumlu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sÄ±nÄ±fÄ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'Ä± Ã§alÄ±ÅŸtÄ±r\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"ðŸš€ YargÄ±tay BGE-M3 Semantic Pipeline BaÅŸlÄ±yor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon oluÅŸtur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi iÅŸle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"âŒ Ä°ÅŸlenecek chunk bulunamadÄ±\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a yÃ¼kle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri gÃ¶ster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nðŸ“Š Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Ä°nteraktif arama arayÃ¼zÃ¼\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K ARAMA SÄ°STEMÄ°\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nðŸ” Arama SeÃ§enekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana menÃ¼ye dÃ¶n\")\n",
    "            \n",
    "            search_choice = input(\"SeÃ§iminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"âŒ GeÃ§ersiz seÃ§im!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nðŸ” Arama metni (Ã§Ä±kmak iÃ§in 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"ðŸ“Š KaÃ§ sonuÃ§? (varsayÄ±lan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"ðŸŽ¯ Minimum benzerlik skoru? (varsayÄ±lan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\nðŸ”§ Filtre SeÃ§enekleri (boÅŸ bÄ±rakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (Ã¶rn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"âŒ SonuÃ§ bulunamadÄ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nðŸ“‹ {len(results)} sonuÃ§ bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. ðŸ“„ BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   âš–ï¸ Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   ðŸ“‹ Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   ðŸ›ï¸ Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   ðŸ“… Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   ðŸ”¤ Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   ðŸ“ Metin Ã–nizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# KullanÄ±m Ã¶rneÄŸi ve main fonksiyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FlagEmbedding kÃ¼tÃ¼phanesi yÃ¼klÃ¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 102717.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model ve sistem hazÄ±r - KullanÄ±lan cihaz: NVIDIA RTX A6000\n",
      "\n",
      "==================================================\n",
      "ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K CHUNK SÄ°STEMÄ°\n",
      "==================================================\n",
      "1. Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV â†’ Semantic Chunks â†’ BGE-M3 â†’ Qdrant)\n",
      "2. Ä°nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini gÃ¶ster\n",
      "4. Ã‡Ä±kÄ±ÅŸ\n",
      "\n",
      "==================================================\n",
      "ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K ARAMA SÄ°STEMÄ°\n",
      "==================================================\n",
      "\n",
      "ðŸ” Arama SeÃ§enekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menÃ¼ye dÃ¶n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ SonuÃ§ bulunamadÄ±\n",
      "\n",
      "ðŸ” Arama SeÃ§enekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menÃ¼ye dÃ¶n\n",
      "âŒ GeÃ§ersiz seÃ§im!\n",
      "\n",
      "ðŸ” Arama SeÃ§enekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana menÃ¼ye dÃ¶n\n",
      "\n",
      "==================================================\n",
      "ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K CHUNK SÄ°STEMÄ°\n",
      "==================================================\n",
      "1. Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV â†’ Semantic Chunks â†’ BGE-M3 â†’ Qdrant)\n",
      "2. Ä°nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini gÃ¶ster\n",
      "4. Ã‡Ä±kÄ±ÅŸ\n",
      "ðŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!\n"
     ]
    }
   ],
   "source": [
    "# KullanÄ±m Ã¶rneÄŸi ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # KonfigÃ¼rasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100,  # GPU memory'ye gÃ¶re ayarlayÄ±n\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline oluÅŸtur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # MenÃ¼ gÃ¶ster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ðŸ›ï¸ YARGITAY BGE-M3 SEMANTÄ°K CHUNK SÄ°STEMÄ°\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline Ã§alÄ±ÅŸtÄ±r (CSV â†’ Semantic Chunks â†’ BGE-M3 â†’ Qdrant)\")\n",
    "        print(\"2. Ä°nteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini gÃ¶ster\")\n",
    "        print(\"4. Ã‡Ä±kÄ±ÅŸ\")\n",
    "        \n",
    "        choice = input(\"\\nSeÃ§iminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"âœ… BGE-M3 Pipeline baÅŸarÄ±yla tamamlandÄ±!\")\n",
    "            else:\n",
    "                print(\"âŒ Pipeline hatasÄ±!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nðŸ“Š Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"ðŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"âŒ GeÃ§ersiz seÃ§im!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrolÃ¼\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"âœ… FlagEmbedding kÃ¼tÃ¼phanesi yÃ¼klÃ¼\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ FlagEmbedding kÃ¼tÃ¼phanesi bulunamadÄ±!\")\n",
    "        print(\"Kurulum iÃ§in: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
