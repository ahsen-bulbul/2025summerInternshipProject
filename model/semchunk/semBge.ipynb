{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "‚úÖ FlagEmbedding k√ºt√ºphanesi y√ºkl√º\n",
      "üöÄ GPU kullanƒ±lƒ±yor: NVIDIA RTX A6000\n",
      "üîÆ BGE-M3 modeli y√ºkleniyor... (BAAI/bge-m3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 50513.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SemChunk chunker hazƒ±r (Token boyutu: 512)\n",
      "‚úÖ BGE-M3 model hazƒ±r (BAAI/bge-m3)\n",
      "‚úÖ Qdrant client hazƒ±r (http://localhost:6333)\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\n",
      "2. ƒ∞nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini g√∂ster\n",
      "4. √áƒ±kƒ±≈ü\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K ARAMA Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "\n",
      "üîç Arama Se√ßenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana men√ºye d√∂n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query vector boyutu: 512 (hedef: 512)\n",
      "üìä 5 sonu√ß bulundu\n",
      "\n",
      "üìã 5 sonu√ß bulundu:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. üìÑ BGE-M3 Benzerlik Skoru: 0.083\n",
      "   ‚öñÔ∏è Esas No: 2023/576 E.\n",
      "   üìã Karar No: 2024/399 K.\n",
      "   üèõÔ∏è Daire: 6.HukukDairesisi\n",
      "   üìÖ Tarih: 30.01.2024,15.07.2017,24.04.2013,24.01.2023,30.01.2024\n",
      "   üî§ Token: 405\n",
      "   üìù Metin √ñnizleme:\n",
      "      vekili; istinaf dilek√ßesinde ileri s√ºr√ºlen gerek√ßeler ve re'sen dikkate alƒ±nacak nedenlerle B√∂lge Adliye Mahkemesi kararƒ±nƒ±n bozularak ortadan kaldƒ±rƒ±lmasƒ± ve ƒ∞lk Derece Mahkemesi kararƒ±nƒ±n bozulmasƒ± istemi ile temyiz yoluna ba≈üvurmu≈ütur. C. Gerek√ße 1. Uyu≈ümazlƒ±k ve Hukuki Nitelendirme Uyu≈ümazlƒ±k, t...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. üìÑ BGE-M3 Benzerlik Skoru: 0.078\n",
      "   ‚öñÔ∏è Esas No: 2022/3993 E.\n",
      "   üìã Karar No: 2024/775 K.\n",
      "   üèõÔ∏è Daire: 6.HukukDairesisi\n",
      "   üìÖ Tarih: 04.02.2011,07.02.2011,15.07.2012,22.10.2014,16.11.2017,22.09.2020,02.12.2020,17.02.2022,15.07.2012,16.11.2017,12.07.2018,12.07.2018,08.06.2022,28.03.2024\n",
      "   üî§ Token: 311\n",
      "   üìù Metin √ñnizleme:\n",
      "      B√∂lge Adliye Mahkemesince Bozmaya Uyularak Verilen Karar B√∂lge Adliye Mahkemesinin yukarƒ±da tarih ve sayƒ±sƒ± belirtilen kararƒ± ile bozma ilamƒ±ndaki gerek√ßeyle, \"Davacƒ±nƒ±n tapu iptal ve tescil talebinin reddine, davacƒ±nƒ±n tazminat talebinin kabul√ºne,daire bedeli olarak 320.000,00 TL tazminatƒ±n dava ta...\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. üìÑ BGE-M3 Benzerlik Skoru: 0.078\n",
      "   ‚öñÔ∏è Esas No: 2023/576 E.\n",
      "   üìã Karar No: 2024/399 K.\n",
      "   üèõÔ∏è Daire: 6.HukukDairesisi\n",
      "   üìÖ Tarih: 30.01.2024,15.07.2017,24.04.2013,24.01.2023,30.01.2024\n",
      "   üî§ Token: 415\n",
      "   üìù Metin √ñnizleme:\n",
      "      2.Temyizen incelenen karar; taraflarƒ±n kar≈üƒ±lƒ±klƒ± iddia ve savunmalarƒ±na, dayandƒ±klarƒ± belgelere, uyu≈ümazlƒ±ƒüa uygulanmasƒ± gereken hukuk kurallarƒ± ile hukuki ili≈ükinin nitelendirilmesine, dava ≈üartlarƒ±na, yargƒ±lama ve ispat kurallarƒ± ile kararda belirtilen gerek√ßelere ve √∂zellikle, davalƒ±lar arasƒ±nda...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. üìÑ BGE-M3 Benzerlik Skoru: 0.076\n",
      "   ‚öñÔ∏è Esas No: 2023/576 E.\n",
      "   üìã Karar No: 2024/399 K.\n",
      "   üèõÔ∏è Daire: 6.HukukDairesisi\n",
      "   üìÖ Tarih: 30.01.2024,15.07.2017,24.04.2013,24.01.2023,30.01.2024\n",
      "   üî§ Token: 448\n",
      "   üìù Metin √ñnizleme:\n",
      "      vekili cevap dilek√ßesinde √∂zetle; ....A.≈û.'nin diƒüer davalƒ± ≈üirket ile yapmƒ±≈ü olduƒüu s√∂zle≈üme √ßer√ßevesinde davacƒ±ya dava konusu villanƒ±n satƒ±ldƒ±ƒüƒ±nƒ± ve teslim edildiƒüini, dava konusu villanƒ±n tapusunun diƒüer davalƒ± ≈üirket √ºzerinde olup davacƒ±ya tapu devrinin gerektiƒüini savunarak, davanƒ±n reddini is...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. üìÑ BGE-M3 Benzerlik Skoru: 0.073\n",
      "   ‚öñÔ∏è Esas No: 2022/3281 E.\n",
      "   üìã Karar No: 2024/117 K.\n",
      "   üèõÔ∏è Daire: 6.HukukDairesisi\n",
      "   üìÖ Tarih: 30.12.2011,26.06.2009,25.12.2009,04.01.2010,05.01.2010,12.01.2010,04.06.2013,25.12.2009,31.12.2009,04.01.2010,05.01.2010,08.01.2010,13.01.2010,05.01.2010,14.01.2010,05.01.2010,14.01.2010,11.01.2024\n",
      "   üî§ Token: 409\n",
      "   üìù Metin √ñnizleme:\n",
      "      maddesindeki on g√ºnl√ºk s√ºre i√ßerisinde esas hakkƒ±nda dava a√ßmazsa, ihtiyati tedbirin haksƒ±z konulmu≈ü sayƒ±lacaƒüƒ±, haksƒ±z ihtiyati tedbirden dolayƒ± tazminat davasƒ± a√ßan davacƒ±nƒ±n √∂denmesini istediƒüi zararƒ± ile haksƒ±z ihtiyati tedbir arasƒ±nda uygun illiyet (nedensellik) baƒüƒ± (sebep sonu√ß ili≈ükisi) bulu...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîç Arama Se√ßenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana men√ºye d√∂n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3679998/1306147784.py:349: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = self.qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\n",
      "2. ƒ∞nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini g√∂ster\n",
      "4. √áƒ±kƒ±≈ü\n",
      "üëã G√∂r√º≈ü√ºr√ºz!\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon\n",
    "# Yargƒ±tay Kararlarƒ± i√ßin Semantic Chunking Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# Konfig√ºrasyon\n",
    "@dataclass\n",
    "class Config:\n",
    "    # BGE-M3 ayarlarƒ±\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"  # BGE-M3 model\n",
    "    USE_FP16: bool = True  # Hafƒ±za optimizasyonu\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # SemChunk ayarlarƒ±\n",
    "    TOKEN_SIZE: int = 512  # Chunk boyutu (token)\n",
    "    ENCODING_NAME: str = \"cl100k_base\"  # Tiktoken encoding\n",
    "    \n",
    "    # Qdrant ayarlarƒ±\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"  # Lokal Qdrant\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512  # BGE-M3 dense embedding boyutu\n",
    "    \n",
    "    # Dosya ayarlarƒ±\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100  # BGE-M3 i√ßin optimize edilmi≈ü batch size\n",
    "    db_batch=256\n",
    "class YargitaySemanticProcessor:\n",
    "    \"\"\"Yargƒ±tay kararlarƒ± i√ßin semantic chunking ve vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        \n",
    "        # GPU/CPU kontrol√º\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"üöÄ GPU kullanƒ±lƒ±yor: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            print(\"üíª CPU kullanƒ±lƒ±yor\")\n",
    "        \n",
    "        # SemChunk chunker olu≈ütur\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        \n",
    "        # BGE-M3 modelini y√ºkle\n",
    "        print(f\"üîÆ BGE-M3 modeli y√ºkleniyor... ({config.BGE_MODEL_NAME})\")\n",
    "        self.bge_model = BGEM3FlagModel(\n",
    "            config.BGE_MODEL_NAME, \n",
    "            use_fp16=config.USE_FP16,\n",
    "            device=config.DEVICE\n",
    "        )\n",
    "        self.reducer = EmbedReducer(1024, 512).to('cuda:0')\n",
    "        # Qdrant client olu≈ütur\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "        \n",
    "        print(f\"‚úÖ SemChunk chunker hazƒ±r (Token boyutu: {config.TOKEN_SIZE})\")\n",
    "        print(f\"‚úÖ BGE-M3 model hazƒ±r ({config.BGE_MODEL_NAME})\")\n",
    "        print(f\"‚úÖ Qdrant client hazƒ±r ({config.QDRANT_URL})\")\n",
    "    \n",
    "    def test_bge_connection(self):\n",
    "        \"\"\"BGE-M3 modelini test et\"\"\"\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            \n",
    "            # BGE-M3'den dense embedding al\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            embedding_dim = len(dense_embedding)\n",
    "            \n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ± - Dense embedding boyutu: {embedding_dim}\")\n",
    "            print(f\"üîç Sparse embedding mevcut: {'colbert_vecs' in embeddings}\")\n",
    "            return embedding_dim\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        \"\"\"Qdrant koleksiyonu olu≈ütur\"\"\"\n",
    "        collection_name = self.config.COLLECTION_NAME\n",
    "        \n",
    "        # Koleksiyon varsa ve recreate True ise sil\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(collection_name)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi: {collection_name}\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Koleksiyon yoksa olu≈ütur\n",
    "        try:\n",
    "            collections = self.qdrant_client.get_collections().collections\n",
    "            collection_names = [c.name for c in collections]\n",
    "            \n",
    "            if collection_name not in collection_names:\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.config.EMBEDDING_DIM,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                print(f\"‚úÖ Koleksiyon olu≈üturuldu: {collection_name} (Boyut: {self.config.EMBEDDING_DIM})\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è Koleksiyon zaten var: {collection_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Koleksiyon olu≈üturma hatasƒ±: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        \"\"\"Metni semantic olarak chunk'lara b√∂l\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # SemChunk ile metni b√∂l\n",
    "            chunks = self.chunker(text)\n",
    "            \n",
    "            result_chunks = []\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                if chunk_text.strip():  # Bo≈ü chunk'larƒ± atla\n",
    "                    chunk_data = {\n",
    "                        'chunk_id': i,\n",
    "                        'text': chunk_text.strip(),\n",
    "                        'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                        'char_count': len(chunk_text),\n",
    "                    }\n",
    "                    \n",
    "                    # Metadata ekle\n",
    "                    if metadata:\n",
    "                        chunk_data.update(metadata)\n",
    "                    \n",
    "                    result_chunks.append(chunk_data)\n",
    "            \n",
    "            return result_chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Chunking hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embeddings_bge(self, texts: List[str], batch_size: int = 100, target_dim: int = 512) -> List[List[float]]:\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.BATCH_SIZE\n",
    "            \n",
    "        all_embeddings = []\n",
    "        total = len(texts)\n",
    "        print(f\"üîÆ BGE-M3 ile {len(texts)} metin i≈üleniyor...\")\n",
    "        \n",
    "        # BGE-M3 i√ßin batch processing\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # BGE-M3 ile embedding olu≈ütur\n",
    "                embeddings_result = self.bge_model.encode(batch_texts)\n",
    "                \n",
    "                # dense_vecs key kontrol√º\n",
    "                if isinstance(embeddings_result, dict) and 'dense_vecs' in embeddings_result:\n",
    "                    dense_embeddings = embeddings_result['dense_vecs']\n",
    "                else:\n",
    "                    dense_embeddings = embeddings_result  # direkt tensor veya list ise\n",
    "                \n",
    "                # Tensor'a √ßevir ve GPU'ya ta≈üƒ±\n",
    "                if not isinstance(dense_embeddings, torch.Tensor):\n",
    "                    dense_embeddings = torch.tensor(dense_embeddings, device=self.config.DEVICE, dtype=torch.float32)\n",
    "                else:\n",
    "                    dense_embeddings = dense_embeddings.to(self.config.DEVICE)\n",
    "                \n",
    "                # Reducer ile boyut k√º√ß√ºltme\n",
    "                reduced_vector = self.reducer(dense_embeddings)\n",
    "                \n",
    "                # Listeye √ßevir\n",
    "                for embedding in reduced_vector:\n",
    "                    all_embeddings.append(embedding.detach().cpu().tolist())\n",
    "                \n",
    "                print(f\"  üìä BGE-M3 Embedding: {i+len(batch_texts)}/{len(texts)}\")\n",
    "                \n",
    "                # GPU memory temizliƒüi (gerekirse)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå BGE-M3 Embedding hatasƒ± (batch {i//batch_size + 1}): {e}\")\n",
    "                # Hata durumunda sƒ±fƒ±r embedding ekle\n",
    "                for _ in batch_texts:\n",
    "                    all_embeddings.append([0.0] * self.config.EMBEDDING_DIM)\n",
    "        \n",
    "        return all_embeddings\n",
    "\n",
    "    \n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        \"\"\"CSV dosyasƒ±nƒ± i≈üle ve chunk'larƒ± olu≈ütur\"\"\"\n",
    "        print(f\"üìÑ CSV dosyasƒ± okunuyor: {csv_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(f\"üìä {len(df)} satƒ±r veri y√ºklendi\")\n",
    "            print(f\"üìã Mevcut s√ºtunlar: {df.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CSV okuma hatasƒ±: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Ana metin s√ºtununu belirle (√∂ncelik sƒ±rasƒ±na g√∂re)\n",
    "        text_columns = ['rawText', 'chunk_text', 'text', 'content', 'metin']\n",
    "        text_column = None\n",
    "        \n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                text_column = col\n",
    "                print(f\"‚úÖ Ana metin s√ºtunu bulundu: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not text_column:\n",
    "            print(f\"‚ùå Ana metin s√ºtunu bulunamadƒ±. Kontrol edilen s√ºtunlar: {text_columns}\")\n",
    "            return []\n",
    "        \n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"üîÑ Semantic chunking ba≈ülƒ±yor...\")\n",
    "        for idx, row in df.iterrows():\n",
    "            # Ana metni al\n",
    "            text = row.get(text_column, '')\n",
    "            \n",
    "            if not text or pd.isna(text):\n",
    "                print(f\"‚ö†Ô∏è Satƒ±r {idx}: Bo≈ü metin atlandƒ±\")\n",
    "                continue\n",
    "            \n",
    "            # Metadata hazƒ±rla (CSV yapƒ±nƒ±za g√∂re g√ºncellenmi≈ü)\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            \n",
    "            # Semantic chunking yap\n",
    "            chunks = self.semantic_chunk_text(str(text), metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            # Progress g√∂ster\n",
    "            if (idx + 1) % 5 == 0:  # Daha sƒ±k progress g√∂ster (az veri olduƒüu i√ßin)\n",
    "                print(f\"  ‚úÖ ƒ∞≈ülenen satƒ±r: {idx + 1}/{len(df)} (Toplam chunk: {len(all_chunks)})\")\n",
    "        \n",
    "        print(f\"üß© Toplam {len(all_chunks)} chunk olu≈üturuldu\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        \"\"\"Chunk'larƒ± Qdrant'a y√ºkle\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üöÄ {len(chunks)} chunk Qdrant'a y√ºkleniyor...\")\n",
    "        \n",
    "        # Metinleri topla\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # BGE-M3 ile embedding'leri olu≈ütur\n",
    "        print(\"üîÆ BGE-M3 embedding'ler olu≈üturuluyor...\")\n",
    "        embeddings = self.create_embeddings_bge(texts)\n",
    "        \n",
    "        if len(embeddings) != len(chunks):\n",
    "            print(f\"‚ùå Embedding sayƒ±sƒ± uyumsuz: {len(embeddings)} vs {len(chunks)}\")\n",
    "            return\n",
    "        \n",
    "        # Qdrant point'leri hazƒ±rla\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding,\n",
    "                payload=chunk\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Batch halinde y√ºkle\n",
    "        batch_size = 256\n",
    "        print(f\"üì¶ {batch_size} batch size ile y√ºkleniyor...\")\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.config.COLLECTION_NAME,\n",
    "                    points=batch\n",
    "                )\n",
    "                print(f\"  ‚úÖ Batch y√ºklendi: {min(i + batch_size, len(points))}/{len(points)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch y√ºkleme hatasƒ±: {e}\")\n",
    "        \n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = None) -> List[Dict]:\n",
    "        \"\"\"D√ºzeltilmi≈ü search metodu - 512 dimension uyumlu\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Query embedding √ßƒ±kar (BGE-M3 dense embedding, 1024 boyut)\n",
    "            query_embedding_result = self.bge_model.encode([query])\n",
    "            \n",
    "            # dense_vecs key kontrol√º\n",
    "            if isinstance(query_embedding_result, dict) and 'dense_vecs' in query_embedding_result:\n",
    "                query_embedding = query_embedding_result['dense_vecs']\n",
    "            else:\n",
    "                query_embedding = query_embedding_result\n",
    "            \n",
    "            # Tensor'a √ßevir ve GPU'ya ta≈üƒ±\n",
    "            if not isinstance(query_embedding, torch.Tensor):\n",
    "                query_embedding = torch.tensor(query_embedding, device=self.config.DEVICE, dtype=torch.float32)\n",
    "            else:\n",
    "                query_embedding = query_embedding.to(self.config.DEVICE)\n",
    "            \n",
    "            query_embedding = query_embedding.clone().detach()\n",
    "            \n",
    "            # Boyutu 512'ye d√º≈ü√ºr (reducer)\n",
    "            with torch.no_grad():\n",
    "                reduced_query_embedding = self.reducer(query_embedding)\n",
    "            \n",
    "            # CPU'ya ta≈üƒ± ve listeye √ßevir\n",
    "            query_vector = reduced_query_embedding[0].cpu().numpy().tolist()\n",
    "            print(f\"üîç Query vector boyutu: {len(query_vector)} (hedef: {self.config.EMBEDDING_DIM})\")\n",
    "            \n",
    "            # Qdrant aramasƒ±\n",
    "            search_results = self.qdrant_client.search(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query_vector=query_vector,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonu√ßlarƒ± formatla\n",
    "            results = [{'score': p.score, 'payload': p.payload} for p in search_results]\n",
    "            print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "\n",
    "    # def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7) -> List[Dict]:\n",
    "    #     \"\"\"BGE-M3 ile semantic arama yap\"\"\"\n",
    "    #     print(f\"üîç Arama: '{query}'\")\n",
    "        \n",
    "    #     try:\n",
    "    #         # Query'yi BGE-M3 ile vekt√∂rize et\n",
    "    #         query_embeddings = self.bge_model.encode([query]).to('cuda:0')\n",
    "    #         query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "    #         # Qdrant'ta ara (g√ºncel query_points metodu)\n",
    "    #         search_results = self.qdrant_client.query_points(\n",
    "    #             collection_name=self.config.COLLECTION_NAME,\n",
    "    #             query=query_vector,\n",
    "    #             limit=limit,\n",
    "    #             score_threshold=score_threshold\n",
    "    #         )\n",
    "            \n",
    "    #         # Sonu√ßlarƒ± formatla\n",
    "    #         results = []\n",
    "    #         for point in search_results.points:#burda muhtemel hata verir search_results olcak verirse\n",
    "    #             results.append({\n",
    "    #                 'score': point.score,\n",
    "    #                 'payload': point.payload\n",
    "    #             })\n",
    "            \n",
    "    #         print(f\"üìä {len(results)} sonu√ß bulundu\")\n",
    "    #         return results\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"‚ùå Arama hatasƒ±: {e}\")\n",
    "    #         return []\n",
    "    \n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6) -> List[Dict]:\n",
    "        \"\"\"Filtreli arama yap\"\"\"\n",
    "        print(f\"üîç Filtreli arama: '{query}' - Filtreler: {filters}\")\n",
    "        \n",
    "        try:\n",
    "            # Query'yi BGE-M3 ile vekt√∂rize et\n",
    "            query_embeddings = self.bge_model.encode([query])\n",
    "            query_vector = query_embeddings['dense_vecs'][0].tolist()\n",
    "            \n",
    "            # Filter olu≈ütur\n",
    "            query_filter = None\n",
    "            if filters:\n",
    "                from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "                conditions = []\n",
    "                for key, value in filters.items():\n",
    "                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))\n",
    "                query_filter = Filter(must=conditions)\n",
    "            \n",
    "            # Qdrant'ta filtreli arama yap\n",
    "            search_results = self.qdrant_client.query_points(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                query=query_vector,\n",
    "                query_filter=query_filter,\n",
    "                limit=limit,\n",
    "                score_threshold=score_threshold\n",
    "            )\n",
    "            \n",
    "            # Sonu√ßlarƒ± formatla\n",
    "            results = []\n",
    "            for point in search_results.points:\n",
    "                results.append({\n",
    "                    'score': point.score,\n",
    "                    'payload': point.payload\n",
    "                })\n",
    "            \n",
    "            print(f\"üìä {len(results)} filtreli sonu√ß bulundu\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Filtreli arama hatasƒ±: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_collection_info(self) -> dict:\n",
    "        \"\"\"Koleksiyon bilgilerini al\"\"\"\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\n",
    "                \"collection_name\": self.config.COLLECTION_NAME,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"status\": info.status,\n",
    "                \"embedding_model\": \"BGE-M3\",\n",
    "                \"embedding_dim\": self.config.EMBEDDING_DIM\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Ana Pipeline Sƒ±nƒ±fƒ±\n",
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sƒ±nƒ±fƒ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ƒ± √ßalƒ±≈ütƒ±r\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"üöÄ Yargƒ±tay BGE-M3 Semantic Pipeline Ba≈ülƒ±yor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon olu≈ütur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi i≈üle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå ƒ∞≈ülenecek chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a y√ºkle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri g√∂ster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"ƒ∞nteraktif arama aray√ºz√º\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K ARAMA Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nüîç Arama Se√ßenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana men√ºye d√∂n\")\n",
    "            \n",
    "            search_choice = input(\"Se√ßiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nüîç Arama metni (√ßƒ±kmak i√ßin 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"üìä Ka√ß sonu√ß? (varsayƒ±lan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"üéØ Minimum benzerlik skoru? (varsayƒ±lan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\nüîß Filtre Se√ßenekleri (bo≈ü bƒ±rakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (√∂rn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìã {len(results)} sonu√ß bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. üìÑ BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ‚öñÔ∏è Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   üìã Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   üèõÔ∏è Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   üìÖ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   üî§ Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   üìù Metin √ñnizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanƒ±m √∂rneƒüi ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfig√ºrasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100,  # GPU memory'ye g√∂re ayarlayƒ±n\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline olu≈ütur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Men√º g√∂ster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\")\n",
    "        print(\"2. ƒ∞nteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4. √áƒ±kƒ±≈ü\")\n",
    "        \n",
    "        choice = input(\"\\nSe√ßiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"‚úÖ BGE-M3 Pipeline ba≈üarƒ±yla tamamlandƒ±!\")\n",
    "            else:\n",
    "                print(\"‚ùå Pipeline hatasƒ±!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrol√º\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding k√ºt√ºphanesi y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding k√ºt√ºphanesi bulunamadƒ±!\")\n",
    "        print(\"Kurulum i√ßin: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# SemChunk + BGE-M3 + Qdrant Entegrasyon (512 boyut embedding)\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import semchunk\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import numpy as np\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "print(load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\"))\n",
    "\n",
    "# Embed Reducer (1024 -> 512)\n",
    "class EmbedReducer(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    BGE_MODEL_NAME: str = \"BAAI/bge-m3\"\n",
    "    USE_FP16: bool = True\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    TOKEN_SIZE: int = 512\n",
    "    ENCODING_NAME: str = \"cl100k_base\"\n",
    "    QDRANT_URL: str = \"http://localhost:6333\"\n",
    "    COLLECTION_NAME: str = \"yargitay_bge_m3_chunks\"\n",
    "    EMBEDDING_DIM: int = 512\n",
    "    CSV_FILE: str = \"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\"\n",
    "    BATCH_SIZE: int = 100\n",
    "\n",
    "class YargitaySemanticProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.encoding = tiktoken.get_encoding(config.ENCODING_NAME)\n",
    "        self.chunker = semchunk.chunkerify(self.encoding, config.TOKEN_SIZE)\n",
    "        self.bge_model = BGEM3FlagModel(config.BGE_MODEL_NAME, use_fp16=config.USE_FP16, device=config.DEVICE)\n",
    "        self.reducer = EmbedReducer(1024, 512).to(config.DEVICE)\n",
    "        self.qdrant_client = QdrantClient(url=config.QDRANT_URL)\n",
    "\n",
    "        device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úÖ Model ve sistem hazƒ±r - Kullanƒ±lan cihaz: {device_name}\")\n",
    "\n",
    "    def test_bge_connection(self):\n",
    "        try:\n",
    "            test_text = [\"Yargƒ±tay 6. Hukuk Dairesi'nin ihtiyati tedbir kararƒ±\"]\n",
    "            embeddings = self.bge_model.encode(test_text)\n",
    "            dense_embedding = embeddings['dense_vecs'][0]\n",
    "            print(f\"‚úÖ BGE-M3 test ba≈üarƒ±lƒ± - Dense embedding boyutu: {len(dense_embedding)}\")\n",
    "            return len(dense_embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BGE-M3 baƒülantƒ± hatasƒ±: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_qdrant_collection(self, recreate: bool = False):\n",
    "        if recreate:\n",
    "            try:\n",
    "                self.qdrant_client.delete_collection(self.config.COLLECTION_NAME)\n",
    "                print(f\"üóëÔ∏è Eski koleksiyon silindi\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        existing = [c.name for c in self.qdrant_client.get_collections().collections]\n",
    "        if self.config.COLLECTION_NAME not in existing:\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.config.COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=self.config.EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "            )\n",
    "            print(f\"‚úÖ Koleksiyon olu≈üturuldu: {self.config.COLLECTION_NAME}\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Koleksiyon zaten var\")\n",
    "\n",
    "    def semantic_chunk_text(self, text: str, metadata: dict = None) -> List[Dict]:\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        chunks = self.chunker(text)\n",
    "        result_chunks = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            if chunk_text.strip():\n",
    "                chunk_data = {\n",
    "                    'chunk_id': i,\n",
    "                    'text': chunk_text.strip(),\n",
    "                    'token_count': len(self.encoding.encode(chunk_text)),\n",
    "                    'char_count': len(chunk_text)\n",
    "                }\n",
    "                if metadata:\n",
    "                    chunk_data.update(metadata)\n",
    "                result_chunks.append(chunk_data)\n",
    "        return result_chunks\n",
    "\n",
    "    def create_embeddings_bge(self, texts: List[str]) -> List[List[float]]:\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.config.BATCH_SIZE):\n",
    "            batch = texts[i:i + self.config.BATCH_SIZE]\n",
    "            try:\n",
    "                emb_result = self.bge_model.encode(batch)\n",
    "                dense_embeddings = emb_result['dense_vecs']\n",
    "                tensor_emb = torch.tensor(dense_embeddings, device=self.config.DEVICE, dtype=torch.float32)\n",
    "                reduced = self.reducer(tensor_emb)\n",
    "                all_embeddings.extend([v.detach().cpu().tolist() for v in reduced])\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch embedding hatasƒ±: {e}\")\n",
    "                all_embeddings.extend([[0.0]*self.config.EMBEDDING_DIM for _ in batch])\n",
    "        return all_embeddings\n",
    "\n",
    "    def process_csv_file(self, csv_path: str) -> List[Dict]:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        text_column = next((c for c in ['rawText','chunk_text','text','content','metin'] if c in df.columns), None)\n",
    "        if not text_column:\n",
    "            print(\"‚ùå Ana metin s√ºtunu bulunamadƒ±\")\n",
    "            return []\n",
    "        all_chunks = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, '')\n",
    "            if not text or pd.isna(text):\n",
    "                continue\n",
    "            metadata = {\n",
    "                'original_index': idx,\n",
    "                'esas_no': row.get('esasNo', ''),\n",
    "                'karar_no': row.get('kararNo', ''),\n",
    "                'daire': row.get('location', ''),\n",
    "                'tarih': row.get('extractedDates', ''),\n",
    "                'document_id': row.get('_id', ''),\n",
    "            }\n",
    "            all_chunks.extend(self.semantic_chunk_text(str(text), metadata))\n",
    "        return all_chunks\n",
    "\n",
    "    def upload_to_qdrant(self, chunks: List[Dict]):\n",
    "        if not chunks:\n",
    "            print(\"‚ùå Y√ºklenecek chunk yok\")\n",
    "            return\n",
    "        embeddings = self.create_embeddings_bge([c['text'] for c in chunks])\n",
    "        points = [PointStruct(id=str(uuid.uuid4()), vector=emb, payload=chunk)\n",
    "                  for chunk, emb in zip(chunks, embeddings)]\n",
    "        for i in range(0, len(points), 256):\n",
    "            self.qdrant_client.upsert(collection_name=self.config.COLLECTION_NAME, points=points[i:i+256])\n",
    "        print(\"üéâ Y√ºkleme tamamlandƒ±!\")\n",
    "\n",
    "    def search_semantic(self, query: str, limit: int = 10, score_threshold: float = 0.7):\n",
    "        query_emb_result = self.bge_model.encode([query])\n",
    "        query_tensor = torch.tensor(query_emb_result['dense_vecs'], device=self.config.DEVICE, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            reduced_query = self.reducer(query_tensor)\n",
    "        query_vector = reduced_query[0].cpu().tolist()\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query=query_vector,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        return [{'score': p.score, 'payload': p.payload} for p in results.points]\n",
    "\n",
    "    def advanced_search_with_filters(self, query: str, filters: Dict = None, limit: int = 10, score_threshold: float = 0.6):\n",
    "        query_emb_result = self.bge_model.encode([query])\n",
    "        query_vector = query_emb_result['dense_vecs'][0].tolist()\n",
    "        query_filter = None\n",
    "        if filters:\n",
    "            from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "            conditions = [FieldCondition(key=k, match=MatchValue(value=v)) for k, v in filters.items()]\n",
    "            query_filter = Filter(must=conditions)\n",
    "        results = self.qdrant_client.query_points(\n",
    "            collection_name=self.config.COLLECTION_NAME,\n",
    "            query=query_vector,\n",
    "            query_filter=query_filter,\n",
    "            limit=limit,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "        return [{'score': p.score, 'payload': p.payload} for p in results.points]\n",
    "\n",
    "    def get_collection_info(self):\n",
    "        try:\n",
    "            info = self.qdrant_client.get_collection(self.config.COLLECTION_NAME)\n",
    "            return {\"collection_name\": self.config.COLLECTION_NAME, \"points_count\": info.points_count,\n",
    "                    \"vectors_count\": info.vectors_count, \"status\": info.status, \"embedding_model\": \"BGE-M3\",\n",
    "                    \"embedding_dim\": self.config.EMBEDDING_DIM}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Pipeline ve main fonksiyon aynƒ± mantƒ±kla entegre edildi\n",
    "# interactive_search ve full_pipeline fonksiyonlarƒ± reducer uyumlu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YargitayPipeline:\n",
    "    \"\"\"Ana pipeline sƒ±nƒ±fƒ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.processor = YargitaySemanticProcessor(config)\n",
    "        self.config = config\n",
    "    \n",
    "    def full_pipeline(self, csv_path: str = None):\n",
    "        \"\"\"Tam pipeline'ƒ± √ßalƒ±≈ütƒ±r\"\"\"\n",
    "        csv_path = csv_path or self.config.CSV_FILE\n",
    "        \n",
    "        print(\"üöÄ Yargƒ±tay BGE-M3 Semantic Pipeline Ba≈ülƒ±yor\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. BGE-M3 modelini test et\n",
    "        embedding_dim = self.processor.test_bge_connection()\n",
    "        if not embedding_dim:\n",
    "            return False\n",
    "        \n",
    "        # 2. Koleksiyon olu≈ütur\n",
    "        self.processor.create_qdrant_collection(recreate=True)\n",
    "        \n",
    "        # 3. CSV'yi i≈üle\n",
    "        chunks = self.processor.process_csv_file(csv_path)\n",
    "        if not chunks:\n",
    "            print(\"‚ùå ƒ∞≈ülenecek chunk bulunamadƒ±\")\n",
    "            return False\n",
    "        \n",
    "        # 4. Qdrant'a y√ºkle\n",
    "        self.processor.upload_to_qdrant(chunks)\n",
    "        \n",
    "        # 5. Bilgileri g√∂ster\n",
    "        info = self.processor.get_collection_info()\n",
    "        print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "        print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"ƒ∞nteraktif arama aray√ºz√º\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K ARAMA Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nüîç Arama Se√ßenekleri:\")\n",
    "            print(\"1. Basit arama\")\n",
    "            print(\"2. Filtreli arama\")\n",
    "            print(\"3. Ana men√ºye d√∂n\")\n",
    "            \n",
    "            search_choice = input(\"Se√ßiminiz (1-3): \")\n",
    "            \n",
    "            if search_choice == \"3\":\n",
    "                break\n",
    "            elif search_choice not in [\"1\", \"2\"]:\n",
    "                print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "                continue\n",
    "            \n",
    "            query = input(\"\\nüîç Arama metni (√ßƒ±kmak i√ßin 'q'): \")\n",
    "            if query.lower() in ['q', 'quit', 'exit']:\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                limit = int(input(\"üìä Ka√ß sonu√ß? (varsayƒ±lan 5): \") or \"5\")\n",
    "                #threshold = float(input(\"üéØ Minimum benzerlik skoru? (varsayƒ±lan 0.6): \") or \"0.6\")\n",
    "            except:\n",
    "                limit = 5\n",
    "                #threshold = 0.6\n",
    "            \n",
    "            # Arama tipini belirle\n",
    "            if search_choice == \"1\":\n",
    "                results = self.processor.search_semantic(query, limit=limit)\n",
    "            else:\n",
    "                # Filtreli arama\n",
    "                print(\"\\nüîß Filtre Se√ßenekleri (bo≈ü bƒ±rakabilirsiniz):\")\n",
    "                daire_filter = input(\"Daire filtresi (√∂rn: '6. Hukuk Dairesi'): \").strip()\n",
    "                \n",
    "                filters = {}\n",
    "                if daire_filter:\n",
    "                    filters['daire'] = daire_filter\n",
    "                \n",
    "                results = self.processor.advanced_search_with_filters(\n",
    "                    query, filters=filters if filters else None, \n",
    "                    limit=limit\n",
    "                )\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå Sonu√ß bulunamadƒ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìã {len(results)} sonu√ß bulundu:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                payload = result['payload']\n",
    "                print(f\"\\n{i}. üìÑ BGE-M3 Benzerlik Skoru: {result['score']:.3f}\")\n",
    "                print(f\"   ‚öñÔ∏è Esas No: {payload.get('esas_no', 'N/A')}\")\n",
    "                print(f\"   üìã Karar No: {payload.get('karar_no', 'N/A')}\")\n",
    "                print(f\"   üèõÔ∏è Daire: {payload.get('daire', 'N/A')}\")\n",
    "                print(f\"   üìÖ Tarih: {payload.get('tarih', 'N/A')}\")\n",
    "                print(f\"   üî§ Token: {payload.get('token_count', 'N/A')}\")\n",
    "                print(f\"   üìù Metin √ñnizleme:\")\n",
    "                \n",
    "                text = payload.get('text', '')\n",
    "                preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "                print(f\"      {preview}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "# Kullanƒ±m √∂rneƒüi ve main fonksiyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FlagEmbedding k√ºt√ºphanesi y√ºkl√º\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 102717.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model ve sistem hazƒ±r - Kullanƒ±lan cihaz: NVIDIA RTX A6000\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\n",
      "2. ƒ∞nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini g√∂ster\n",
      "4. √áƒ±kƒ±≈ü\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K ARAMA Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "\n",
      "üîç Arama Se√ßenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana men√ºye d√∂n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sonu√ß bulunamadƒ±\n",
      "\n",
      "üîç Arama Se√ßenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana men√ºye d√∂n\n",
      "‚ùå Ge√ßersiz se√ßim!\n",
      "\n",
      "üîç Arama Se√ßenekleri:\n",
      "1. Basit arama\n",
      "2. Filtreli arama\n",
      "3. Ana men√ºye d√∂n\n",
      "\n",
      "==================================================\n",
      "üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\n",
      "==================================================\n",
      "1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\n",
      "2. ƒ∞nteraktif arama yap\n",
      "3. Koleksiyon bilgilerini g√∂ster\n",
      "4. √áƒ±kƒ±≈ü\n",
      "üëã G√∂r√º≈ü√ºr√ºz!\n"
     ]
    }
   ],
   "source": [
    "# Kullanƒ±m √∂rneƒüi ve main fonksiyon\n",
    "def main():\n",
    "    \"\"\"Ana fonksiyon\"\"\"\n",
    "    \n",
    "    # Konfig√ºrasyon\n",
    "    config = Config(\n",
    "        CSV_FILE=\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\",\n",
    "        TOKEN_SIZE=512,  # Chunk boyutu\n",
    "        QDRANT_URL=\"http://localhost:6333\",\n",
    "        COLLECTION_NAME=\"bge_m3_chunks\",\n",
    "        EMBEDDING_DIM=512,\n",
    "        BATCH_SIZE=100,  # GPU memory'ye g√∂re ayarlayƒ±n\n",
    "        USE_FP16=True,\n",
    "        DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Pipeline olu≈ütur\n",
    "    pipeline = YargitayPipeline(config)\n",
    "    \n",
    "    # Men√º g√∂ster\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üèõÔ∏è YARGITAY BGE-M3 SEMANTƒ∞K CHUNK Sƒ∞STEMƒ∞\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. Tam pipeline √ßalƒ±≈ütƒ±r (CSV ‚Üí Semantic Chunks ‚Üí BGE-M3 ‚Üí Qdrant)\")\n",
    "        print(\"2. ƒ∞nteraktif arama yap\")\n",
    "        print(\"3. Koleksiyon bilgilerini g√∂ster\")\n",
    "        print(\"4. √áƒ±kƒ±≈ü\")\n",
    "        \n",
    "        choice = input(\"\\nSe√ßiminiz (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            csv_path = input(f\"CSV dosya yolu (Enter: {config.CSV_FILE}): \").strip()\n",
    "            if not csv_path:\n",
    "                csv_path = config.CSV_FILE\n",
    "            \n",
    "            success = pipeline.full_pipeline(csv_path)\n",
    "            if success:\n",
    "                print(\"‚úÖ BGE-M3 Pipeline ba≈üarƒ±yla tamamlandƒ±!\")\n",
    "            else:\n",
    "                print(\"‚ùå Pipeline hatasƒ±!\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            pipeline.interactive_search()\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            info = pipeline.processor.get_collection_info()\n",
    "            print(\"\\nüìä Koleksiyon Bilgileri:\")\n",
    "            print(json.dumps(info, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã G√∂r√º≈ü√ºr√ºz!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Ge√ßersiz se√ßim!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # BGE-M3 kurulumu kontrol√º\n",
    "    try:\n",
    "        from FlagEmbedding import BGEM3FlagModel\n",
    "        print(\"‚úÖ FlagEmbedding k√ºt√ºphanesi y√ºkl√º\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå FlagEmbedding k√ºt√ºphanesi bulunamadƒ±!\")\n",
    "        print(\"Kurulum i√ßin: pip install FlagEmbedding\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
