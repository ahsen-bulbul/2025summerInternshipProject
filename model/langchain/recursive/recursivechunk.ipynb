{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5334f647-ffc8-443d-986b-d3bb2f337571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV dosyası okunuyor: /home/yapayzeka/ahsen_bulbul/data/10data.csv\n",
      "Toplam karar sayısı: 20\n",
      "Kolonlar: ['_id', 'location', 'rawText', 'extractedDates', 'esasNo', 'kararNo', 'esasNo_num', 'esasNo_tip', 'kararNo_num', 'kararNo_tip']\n",
      "\n",
      "Chunk işlemi başlıyor...\n",
      "Karar 1: 1 chunk oluşturuldu\n",
      "Karar 2: 13 chunk oluşturuldu\n",
      "Karar 3: 10 chunk oluşturuldu\n",
      "Karar 4: 6 chunk oluşturuldu\n",
      "Karar 5: 18 chunk oluşturuldu\n",
      "Karar 6: 1 chunk oluşturuldu\n",
      "Karar 7: 1 chunk oluşturuldu\n",
      "Karar 8: 7 chunk oluşturuldu\n",
      "Karar 9: 1 chunk oluşturuldu\n",
      "Karar 10: 7 chunk oluşturuldu\n",
      "Karar 11: 6 chunk oluşturuldu\n",
      "Karar 12: 12 chunk oluşturuldu\n",
      "Karar 13: 6 chunk oluşturuldu\n",
      "Karar 14: 1 chunk oluşturuldu\n",
      "Karar 15: 11 chunk oluşturuldu\n",
      "Karar 16: 4 chunk oluşturuldu\n",
      "Karar 17: 10 chunk oluşturuldu\n",
      "Karar 18: 2 chunk oluşturuldu\n",
      "Karar 19: 15 chunk oluşturuldu\n",
      "Karar 20: 5 chunk oluşturuldu\n",
      "\n",
      "=== ÖZET ===\n",
      "İşlenen karar sayısı: 20\n",
      "Üretilen chunk sayısı: 137\n",
      "Ortalama chunk/karar: 6.8\n",
      "Chunk uzunlukları - Min: 4, Max: 1200, Ort: 897\n",
      "\n",
      "Sonuç kaydediliyor: yargitay_chunks.csv\n",
      "✅ İşlem tamamlandı!\n",
      "📂 Giriş dosyası: /home/yapayzeka/ahsen_bulbul/data/10data.csv\n",
      "📄 Çıkış dosyası: yargitay_chunks.csv\n",
      "=== CHUNK ANALİZİ ===\n",
      "Toplam chunk sayısı: 137\n",
      "Benzersiz karar sayısı: 20\n",
      "\n",
      "Karar başına chunk dağılımı:\n",
      "Min: 1\n",
      "Max: 18\n",
      "Ortalama: 6.8\n",
      "\n",
      "En çok chunk'a sahip 5 karar:\n",
      "- 2022/3993 E.: 18 chunk\n",
      "- 2022/4331 E.: 15 chunk\n",
      "- 2022/3281 E.: 13 chunk\n",
      "- 2023/1914 E.: 12 chunk\n",
      "- 2022/3536 E.: 11 chunk\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "class YargitayKararlariChunker:\n",
    "    def __init__(self, chunk_size=1200, chunk_overlap=150):\n",
    "        # Hukuki metinler için optimize edilmiş ayırıcılar\n",
    "        self.separators = [\n",
    "            \"\\n\\n\",  # Paragraf arası\n",
    "            \". \",    # Cümle sonu\n",
    "            \", \",    # Virgül\n",
    "            \" \",     # Boşluk\n",
    "            \"\"       # Karakter düzeyi\n",
    "        ]\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=self.separators,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "    \n",
    "    def preprocess_text(self, raw_text):\n",
    "        \"\"\"Metni temizle ve normalize et\"\"\"\n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', raw_text.strip())\n",
    "        \n",
    "        # Özel karakterleri düzelt\n",
    "        text = text.replace('\"\"', '\"')\n",
    "        text = text.replace('...', '[...]')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_metadata_from_text(self, text):\n",
    "        \"\"\"Metinden önemli bilgileri çıkar\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Daire bilgisi\n",
    "        daire_match = re.search(r'(\\d+\\.\\s*\\w+\\s*Dairesi)', text)\n",
    "        if daire_match:\n",
    "            metadata['daire'] = daire_match.group(1)\n",
    "        \n",
    "        # Mahkeme bilgisi\n",
    "        mahkeme_match = re.search(r'MAHKEMESİ\\s*:\\s*([^\\\\n]+)', text)\n",
    "        if mahkeme_match:\n",
    "            metadata['mahkeme'] = mahkeme_match.group(1).strip()\n",
    "        \n",
    "        # Karar türü\n",
    "        if 'reddine karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'RED'\n",
    "        elif 'kabulune karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'KABUL'\n",
    "        else:\n",
    "            metadata['karar_turu'] = 'DİĞER'\n",
    "            \n",
    "        return metadata\n",
    "    \n",
    "    def chunk_single_karar(self, row):\n",
    "        \"\"\"Tek bir kararı chunk'lara böl\"\"\"\n",
    "        # Temel bilgileri hazırla\n",
    "        base_metadata = {\n",
    "            '_id': row['_id'],\n",
    "            'location': row['location'],\n",
    "            'esasNo': row['esasNo'],\n",
    "            'kararNo': row['kararNo'],\n",
    "            'extractedDates': row['extractedDates'],\n",
    "            'esasNo_num': row['esasNo_num'],\n",
    "            'esasNo_tip': row['esasNo_tip'],\n",
    "            'kararNo_num': row['kararNo_num'],\n",
    "            'kararNo_tip': row['kararNo_tip']\n",
    "        }\n",
    "        \n",
    "        # Metni temizle\n",
    "        cleaned_text = self.preprocess_text(row['rawText'])\n",
    "        \n",
    "        # Metinden ek bilgi çıkar\n",
    "        text_metadata = self.extract_metadata_from_text(cleaned_text)\n",
    "        \n",
    "        # Chunk'lara böl\n",
    "        chunks = self.text_splitter.split_text(cleaned_text)\n",
    "        \n",
    "        # Her chunk için metadata ekle\n",
    "        chunked_data = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = base_metadata.copy()\n",
    "            chunk_metadata.update(text_metadata)\n",
    "            chunk_metadata.update({\n",
    "                'chunk_id': f\"{row['_id']}_chunk_{i+1}\",\n",
    "                'chunk_index': i + 1,\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_text': chunk,\n",
    "                'chunk_length': len(chunk)\n",
    "            })\n",
    "            chunked_data.append(chunk_metadata)\n",
    "        \n",
    "        return chunked_data\n",
    "    \n",
    "    def chunk_dataframe(self, df):\n",
    "        \"\"\"Tüm DataFrame'i chunk'lara böl\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                chunks = self.chunk_single_karar(row)\n",
    "                all_chunks.extend(chunks)\n",
    "                print(f\"Karar {index+1}: {len(chunks)} chunk oluşturuldu\")\n",
    "            except Exception as e:\n",
    "                print(f\"Karar {index+1} işlenirken hata: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(all_chunks)\n",
    "\n",
    "# Kullanım örneği\n",
    "# CSV ile çalışma fonksiyonları\n",
    "def process_csv(input_csv_path, output_csv_path, chunk_size=2000, chunk_overlap=100):\n",
    "    \"\"\"CSV dosyasını okur, chunk'lar ve yeni CSV dosyasına kaydeder\"\"\"\n",
    "    \n",
    "    # CSV'yi oku\n",
    "    print(f\"CSV dosyası okunuyor: {input_csv_path}\")\n",
    "    df = pd.read_csv(input_csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Toplam karar sayısı: {len(df)}\")\n",
    "    print(f\"Kolonlar: {list(df.columns)}\")\n",
    "    \n",
    "    # Chunker'ı başlat\n",
    "    chunker = YargitayKararlariChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    # Chunk'lara böl\n",
    "    print(\"\\nChunk işlemi başlıyor...\")\n",
    "    chunked_df = chunker.chunk_dataframe(df)\n",
    "    \n",
    "    # Özet bilgi\n",
    "    print(f\"\\n=== ÖZET ===\")\n",
    "    print(f\"İşlenen karar sayısı: {len(df)}\")\n",
    "    print(f\"Üretilen chunk sayısı: {len(chunked_df)}\")\n",
    "    print(f\"Ortalama chunk/karar: {len(chunked_df)/len(df):.1f}\")\n",
    "    \n",
    "    # Chunk uzunluk istatistikleri\n",
    "    chunk_lengths = chunked_df['chunk_length']\n",
    "    print(f\"Chunk uzunlukları - Min: {chunk_lengths.min()}, Max: {chunk_lengths.max()}, Ort: {chunk_lengths.mean():.0f}\")\n",
    "    \n",
    "    # CSV olarak kaydet\n",
    "    print(f\"\\nSonuç kaydediliyor: {output_csv_path}\")\n",
    "    chunked_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✅ İşlem tamamlandı!\")\n",
    "    print(f\"📂 Giriş dosyası: {input_csv_path}\")\n",
    "    print(f\"📄 Çıkış dosyası: {output_csv_path}\")\n",
    "    return chunked_df\n",
    "\n",
    "def analyze_chunks(csv_path):\n",
    "    \"\"\"Chunk'lanmış CSV'yi analiz eder\"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"=== CHUNK ANALİZİ ===\")\n",
    "    print(f\"Toplam chunk sayısı: {len(df)}\")\n",
    "    print(f\"Benzersiz karar sayısı: {df['_id'].nunique()}\")\n",
    "    \n",
    "    # Karar başına chunk dağılımı\n",
    "    chunks_per_karar = df.groupby('_id')['chunk_index'].max()\n",
    "    print(f\"\\nKarar başına chunk dağılımı:\")\n",
    "    print(f\"Min: {chunks_per_karar.min()}\")\n",
    "    print(f\"Max: {chunks_per_karar.max()}\")\n",
    "    print(f\"Ortalama: {chunks_per_karar.mean():.1f}\")\n",
    "    \n",
    "    # En çok chunk'a sahip kararlar\n",
    "    print(f\"\\nEn çok chunk'a sahip 5 karar:\")\n",
    "    top_chunked = chunks_per_karar.nlargest(5)\n",
    "    for karar_id, chunk_count in top_chunked.items():\n",
    "        esasNo = df[df['_id'] == karar_id]['esasNo'].iloc[0]\n",
    "        print(f\"- {esasNo}: {chunk_count} chunk\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Örnek kullanım - Giriş ve çıkış CSV dosya yollarını belirtin\"\"\"\n",
    "    \n",
    "    # Dosya yollarını belirtin\n",
    "    input_file = \"/home/yapayzeka/ahsen_bulbul/data/10data.csv\"      # Giriş CSV dosyası\n",
    "    output_file = \"yargitay_chunks.csv\"        # Çıkış CSV dosyası\n",
    "    \n",
    "    try:\n",
    "        # CSV'yi işle - Her zaman yeni dosyaya yazar\n",
    "        chunked_data = process_csv(\n",
    "            input_csv_path=input_file,\n",
    "            output_csv_path=output_file,    # Zorunlu parametre\n",
    "            chunk_size=1200,                # İhtiyaca göre ayarlayın\n",
    "            chunk_overlap=100               # İhtiyaca göre ayarlayın\n",
    "        )\n",
    "        \n",
    "        # Analiz yap\n",
    "        analyze_chunks(output_file)\n",
    "        \n",
    "        return chunked_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Dosya bulunamadı: {input_file}\")\n",
    "        print(\"Lütfen doğru dosya yolunu 'input_file' değişkenine girin.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Hata oluştu: {e}\")\n",
    "        return None\n",
    "\n",
    "# Alternatif kullanım fonksiyonları\n",
    "def chunk_csv_simple(input_path, output_path):\n",
    "    \"\"\"Basit kullanım - varsayılan ayarlarla\"\"\"\n",
    "    return process_csv(input_path, output_path)\n",
    "\n",
    "def chunk_csv_custom(input_path, output_path, chunk_size, chunk_overlap):\n",
    "    \"\"\"Özel ayarlarla chunk'lama\"\"\"\n",
    "    return process_csv(input_path, output_path, chunk_size, chunk_overlap)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a91b1e",
   "metadata": {},
   "source": [
    "###DENEME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "class YargitayKararlariChunker:\n",
    "    def __init__(self, chunk_size=2000, chunk_overlap=100, aggressive_mode=False):\n",
    "        # Aggressive mode için daha az ayırıcı kullan\n",
    "        if aggressive_mode:\n",
    "            self.separators = [\n",
    "                \"\\n\\n\",  # Sadece paragraf arası\n",
    "                \". \",    # Cümle sonu\n",
    "                \" \"      # Boşluk\n",
    "            ]\n",
    "        else:\n",
    "            # Hukuki metinler için optimize edilmiş ayırıcılar\n",
    "            self.separators = [\n",
    "                \"\\n\\n\",  # Paragraf arası\n",
    "                \". \",    # Cümle sonu\n",
    "                \", \",    # Virgül\n",
    "                \" \",     # Boşluk\n",
    "                \"\"       # Karakter düzeyi\n",
    "            ]\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=self.separators,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Chunker ayarları: size={chunk_size}, overlap={chunk_overlap}, aggressive={aggressive_mode}\")\n",
    "    \n",
    "    def preprocess_text(self, raw_text):\n",
    "        \"\"\"Metni temizle ve normalize et\"\"\"\n",
    "        # Fazla boşlukları temizle\n",
    "        text = re.sub(r'\\s+', ' ', raw_text.strip())\n",
    "        \n",
    "        # Özel karakterleri düzelt\n",
    "        text = text.replace('\"\"', '\"')\n",
    "        text = text.replace('...', '[...]')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_metadata_from_text(self, text):\n",
    "        \"\"\"Metinden önemli bilgileri çıkar\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Daire bilgisi\n",
    "        daire_match = re.search(r'(\\d+\\.\\s*\\w+\\s*Dairesi)', text)\n",
    "        if daire_match:\n",
    "            metadata['daire'] = daire_match.group(1)\n",
    "        \n",
    "        # Mahkeme bilgisi\n",
    "        mahkeme_match = re.search(r'MAHKEMESİ\\s*:\\s*([^\\\\n]+)', text)\n",
    "        if mahkeme_match:\n",
    "            metadata['mahkeme'] = mahkeme_match.group(1).strip()\n",
    "        \n",
    "        # Karar türü\n",
    "        if 'reddine karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'RED'\n",
    "        elif 'kabulune karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'KABUL'\n",
    "        else:\n",
    "            metadata['karar_turu'] = 'DİĞER'\n",
    "            \n",
    "        return metadata\n",
    "    \n",
    "    def chunk_single_karar(self, row):\n",
    "        \"\"\"Tek bir kararı chunk'lara böl\"\"\"\n",
    "        # Temel bilgileri hazırla\n",
    "        base_metadata = {\n",
    "            '_id': row['_id'],\n",
    "            'location': row['location'],\n",
    "            'esasNo': row['esasNo'],\n",
    "            'kararNo': row['kararNo'],\n",
    "            'extractedDates': row['extractedDates'],\n",
    "            'esasNo_num': row['esasNo_num'],\n",
    "            'esasNo_tip': row['esasNo_tip'],\n",
    "            'kararNo_num': row['kararNo_num'],\n",
    "            'kararNo_tip': row['kararNo_tip']\n",
    "        }\n",
    "        \n",
    "        # Metni temizle\n",
    "        cleaned_text = self.preprocess_text(row['rawText'])\n",
    "        \n",
    "        # Metinden ek bilgi çıkar\n",
    "        text_metadata = self.extract_metadata_from_text(cleaned_text)\n",
    "        \n",
    "        # Chunk'lara böl\n",
    "        chunks = self.text_splitter.split_text(cleaned_text)\n",
    "        \n",
    "        # Her chunk için metadata ekle\n",
    "        chunked_data = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = base_metadata.copy()\n",
    "            chunk_metadata.update(text_metadata)\n",
    "            chunk_metadata.update({\n",
    "                'chunk_id': f\"{row['_id']}_chunk_{i+1}\",\n",
    "                'chunk_index': i + 1,\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_text': chunk,\n",
    "                'chunk_length': len(chunk)\n",
    "            })\n",
    "            chunked_data.append(chunk_metadata)\n",
    "        \n",
    "        return chunked_data\n",
    "    \n",
    "    def chunk_dataframe(self, df):\n",
    "        \"\"\"Tüm DataFrame'i chunk'lara böl\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                chunks = self.chunk_single_karar(row)\n",
    "                all_chunks.extend(chunks)\n",
    "                print(f\"Karar {index+1}: {len(chunks)} chunk oluşturuldu\")\n",
    "            except Exception as e:\n",
    "                print(f\"Karar {index+1} işlenirken hata: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(all_chunks)\n",
    "\n",
    "# Kullanım örneği\n",
    "# CSV ile çalışma fonksiyonları\n",
    "def process_csv(input_csv_path, output_csv_path, chunk_size=2000, chunk_overlap=100, aggressive_mode=False):\n",
    "    \"\"\"CSV dosyasını okur, chunk'lar ve yeni CSV dosyasına kaydeder\"\"\"\n",
    "    \n",
    "    # CSV'yi oku\n",
    "    print(f\"CSV dosyası okunuyor: {input_csv_path}\")\n",
    "    df = pd.read_csv(input_csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Toplam karar sayısı: {len(df)}\")\n",
    "    print(f\"Kolonlar: {list(df.columns)}\")\n",
    "    \n",
    "    # Chunker'ı başlat\n",
    "    chunker = YargitayKararlariChunker(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        aggressive_mode=aggressive_mode\n",
    "    )\n",
    "    \n",
    "    # Chunk'lara böl\n",
    "    print(\"\\nChunk işlemi başlıyor...\")\n",
    "    chunked_df = chunker.chunk_dataframe(df)\n",
    "    \n",
    "    # Özet bilgi\n",
    "    print(f\"\\n=== ÖZET ===\")\n",
    "    print(f\"İşlenen karar sayısı: {len(df)}\")\n",
    "    print(f\"Üretilen chunk sayısı: {len(chunked_df)}\")\n",
    "    print(f\"Ortalama chunk/karar: {len(chunked_df)/len(df):.1f}\")\n",
    "    print(f\"Önceki chunk sayısından azalma: %{((138-len(chunked_df))/138)*100:.1f}\")\n",
    "    \n",
    "    # Chunk uzunluk istatistikleri\n",
    "    chunk_lengths = chunked_df['chunk_length']\n",
    "    print(f\"Chunk uzunlukları - Min: {chunk_lengths.min()}, Max: {chunk_lengths.max()}, Ort: {chunk_lengths.mean():.0f}\")\n",
    "    \n",
    "    # CSV olarak kaydet\n",
    "    print(f\"\\nSonuç kaydediliyor: {output_csv_path}\")\n",
    "    chunked_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✅ İşlem tamamlandı!\")\n",
    "    print(f\"📂 Giriş dosyası: {input_csv_path}\")\n",
    "    print(f\"📄 Çıkış dosyası: {output_csv_path}\")\n",
    "    return chunked_df\n",
    "\n",
    "def analyze_chunks(csv_path):\n",
    "    \"\"\"Chunk'lanmış CSV'yi analiz eder\"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"=== CHUNK ANALİZİ ===\")\n",
    "    print(f\"Toplam chunk sayısı: {len(df)}\")\n",
    "    print(f\"Benzersiz karar sayısı: {df['_id'].nunique()}\")\n",
    "    \n",
    "    # Karar başına chunk dağılımı\n",
    "    chunks_per_karar = df.groupby('_id')['chunk_index'].max()\n",
    "    print(f\"\\nKarar başına chunk dağılımı:\")\n",
    "    print(f\"Min: {chunks_per_karar.min()}\")\n",
    "    print(f\"Max: {chunks_per_karar.max()}\")\n",
    "    print(f\"Ortalama: {chunks_per_karar.mean():.1f}\")\n",
    "    \n",
    "    # En çok chunk'a sahip kararlar\n",
    "    print(f\"\\nEn çok chunk'a sahip 5 karar:\")\n",
    "    top_chunked = chunks_per_karar.nlargest(5)\n",
    "    for karar_id, chunk_count in top_chunked.items():\n",
    "        esasNo = df[df['_id'] == karar_id]['esasNo'].iloc[0]\n",
    "        print(f\"- {esasNo}: {chunk_count} chunk\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Örnek kullanım - Giriş ve çıkış CSV dosya yollarını belirtin\"\"\"\n",
    "    \n",
    "    # Dosya yollarını belirtin\n",
    "    input_file = \"yargitay_kararlari.csv\"      # Giriş CSV dosyası\n",
    "    output_file = \"yargitay_chunks.csv\"        # Çıkış CSV dosyası\n",
    "    \n",
    "    try:\n",
    "        # CSV'yi işle - Az chunk için optimize edilmiş ayarlar\n",
    "        chunked_data = process_csv(\n",
    "            input_csv_path=input_file,\n",
    "            output_csv_path=output_file,\n",
    "            chunk_size=2000,            # Büyük chunk (az chunk için)\n",
    "            chunk_overlap=100,          # Az overlap\n",
    "            aggressive_mode=True        # Agresif mod\n",
    "        )\n",
    "        \n",
    "        # Analiz yap\n",
    "        analyze_chunks(output_file)\n",
    "        \n",
    "        return chunked_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Dosya bulunamadı: {input_file}\")\n",
    "        print(\"Lütfen doğru dosya yolunu 'input_file' değişkenine girin.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Hata oluştu: {e}\")\n",
    "        return None\n",
    "\n",
    "# Chunk sayısını minimize etmek için özel fonksiyonlar\n",
    "def minimize_chunks(input_path, output_path):\n",
    "    \"\"\"Minimum chunk sayısı için optimize edilmiş\"\"\"\n",
    "    return process_csv(\n",
    "        input_path, \n",
    "        output_path, \n",
    "        chunk_size=3000,        # Çok büyük chunk\n",
    "        chunk_overlap=50,       # Minimum overlap\n",
    "        aggressive_mode=True\n",
    "    )\n",
    "\n",
    "def balanced_chunks(input_path, output_path):\n",
    "    \"\"\"Dengeli yaklaşım\"\"\"\n",
    "    return process_csv(\n",
    "        input_path, \n",
    "        output_path, \n",
    "        chunk_size=2000,        # Orta büyüklük\n",
    "        chunk_overlap=100,      # Az overlap\n",
    "        aggressive_mode=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
