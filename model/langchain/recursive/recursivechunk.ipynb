{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5334f647-ffc8-443d-986b-d3bb2f337571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV dosyasÄ± okunuyor: /home/yapayzeka/ahsen_bulbul/data/10data.csv\n",
      "Toplam karar sayÄ±sÄ±: 20\n",
      "Kolonlar: ['_id', 'location', 'rawText', 'extractedDates', 'esasNo', 'kararNo', 'esasNo_num', 'esasNo_tip', 'kararNo_num', 'kararNo_tip']\n",
      "\n",
      "Chunk iÅŸlemi baÅŸlÄ±yor...\n",
      "Karar 1: 1 chunk oluÅŸturuldu\n",
      "Karar 2: 13 chunk oluÅŸturuldu\n",
      "Karar 3: 10 chunk oluÅŸturuldu\n",
      "Karar 4: 6 chunk oluÅŸturuldu\n",
      "Karar 5: 18 chunk oluÅŸturuldu\n",
      "Karar 6: 1 chunk oluÅŸturuldu\n",
      "Karar 7: 1 chunk oluÅŸturuldu\n",
      "Karar 8: 7 chunk oluÅŸturuldu\n",
      "Karar 9: 1 chunk oluÅŸturuldu\n",
      "Karar 10: 7 chunk oluÅŸturuldu\n",
      "Karar 11: 6 chunk oluÅŸturuldu\n",
      "Karar 12: 12 chunk oluÅŸturuldu\n",
      "Karar 13: 6 chunk oluÅŸturuldu\n",
      "Karar 14: 1 chunk oluÅŸturuldu\n",
      "Karar 15: 11 chunk oluÅŸturuldu\n",
      "Karar 16: 4 chunk oluÅŸturuldu\n",
      "Karar 17: 10 chunk oluÅŸturuldu\n",
      "Karar 18: 2 chunk oluÅŸturuldu\n",
      "Karar 19: 15 chunk oluÅŸturuldu\n",
      "Karar 20: 5 chunk oluÅŸturuldu\n",
      "\n",
      "=== Ã–ZET ===\n",
      "Ä°ÅŸlenen karar sayÄ±sÄ±: 20\n",
      "Ãœretilen chunk sayÄ±sÄ±: 137\n",
      "Ortalama chunk/karar: 6.8\n",
      "Chunk uzunluklarÄ± - Min: 4, Max: 1200, Ort: 897\n",
      "\n",
      "SonuÃ§ kaydediliyor: yargitay_chunks.csv\n",
      "âœ… Ä°ÅŸlem tamamlandÄ±!\n",
      "ğŸ“‚ GiriÅŸ dosyasÄ±: /home/yapayzeka/ahsen_bulbul/data/10data.csv\n",
      "ğŸ“„ Ã‡Ä±kÄ±ÅŸ dosyasÄ±: yargitay_chunks.csv\n",
      "=== CHUNK ANALÄ°ZÄ° ===\n",
      "Toplam chunk sayÄ±sÄ±: 137\n",
      "Benzersiz karar sayÄ±sÄ±: 20\n",
      "\n",
      "Karar baÅŸÄ±na chunk daÄŸÄ±lÄ±mÄ±:\n",
      "Min: 1\n",
      "Max: 18\n",
      "Ortalama: 6.8\n",
      "\n",
      "En Ã§ok chunk'a sahip 5 karar:\n",
      "- 2022/3993 E.: 18 chunk\n",
      "- 2022/4331 E.: 15 chunk\n",
      "- 2022/3281 E.: 13 chunk\n",
      "- 2023/1914 E.: 12 chunk\n",
      "- 2022/3536 E.: 11 chunk\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "class YargitayKararlariChunker:\n",
    "    def __init__(self, chunk_size=1200, chunk_overlap=150):\n",
    "        # Hukuki metinler iÃ§in optimize edilmiÅŸ ayÄ±rÄ±cÄ±lar\n",
    "        self.separators = [\n",
    "            \"\\n\\n\",  # Paragraf arasÄ±\n",
    "            \". \",    # CÃ¼mle sonu\n",
    "            \", \",    # VirgÃ¼l\n",
    "            \" \",     # BoÅŸluk\n",
    "            \"\"       # Karakter dÃ¼zeyi\n",
    "        ]\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=self.separators,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "    \n",
    "    def preprocess_text(self, raw_text):\n",
    "        \"\"\"Metni temizle ve normalize et\"\"\"\n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', raw_text.strip())\n",
    "        \n",
    "        # Ã–zel karakterleri dÃ¼zelt\n",
    "        text = text.replace('\"\"', '\"')\n",
    "        text = text.replace('...', '[...]')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_metadata_from_text(self, text):\n",
    "        \"\"\"Metinden Ã¶nemli bilgileri Ã§Ä±kar\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Daire bilgisi\n",
    "        daire_match = re.search(r'(\\d+\\.\\s*\\w+\\s*Dairesi)', text)\n",
    "        if daire_match:\n",
    "            metadata['daire'] = daire_match.group(1)\n",
    "        \n",
    "        # Mahkeme bilgisi\n",
    "        mahkeme_match = re.search(r'MAHKEMESÄ°\\s*:\\s*([^\\\\n]+)', text)\n",
    "        if mahkeme_match:\n",
    "            metadata['mahkeme'] = mahkeme_match.group(1).strip()\n",
    "        \n",
    "        # Karar tÃ¼rÃ¼\n",
    "        if 'reddine karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'RED'\n",
    "        elif 'kabulune karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'KABUL'\n",
    "        else:\n",
    "            metadata['karar_turu'] = 'DÄ°ÄER'\n",
    "            \n",
    "        return metadata\n",
    "    \n",
    "    def chunk_single_karar(self, row):\n",
    "        \"\"\"Tek bir kararÄ± chunk'lara bÃ¶l\"\"\"\n",
    "        # Temel bilgileri hazÄ±rla\n",
    "        base_metadata = {\n",
    "            '_id': row['_id'],\n",
    "            'location': row['location'],\n",
    "            'esasNo': row['esasNo'],\n",
    "            'kararNo': row['kararNo'],\n",
    "            'extractedDates': row['extractedDates'],\n",
    "            'esasNo_num': row['esasNo_num'],\n",
    "            'esasNo_tip': row['esasNo_tip'],\n",
    "            'kararNo_num': row['kararNo_num'],\n",
    "            'kararNo_tip': row['kararNo_tip']\n",
    "        }\n",
    "        \n",
    "        # Metni temizle\n",
    "        cleaned_text = self.preprocess_text(row['rawText'])\n",
    "        \n",
    "        # Metinden ek bilgi Ã§Ä±kar\n",
    "        text_metadata = self.extract_metadata_from_text(cleaned_text)\n",
    "        \n",
    "        # Chunk'lara bÃ¶l\n",
    "        chunks = self.text_splitter.split_text(cleaned_text)\n",
    "        \n",
    "        # Her chunk iÃ§in metadata ekle\n",
    "        chunked_data = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = base_metadata.copy()\n",
    "            chunk_metadata.update(text_metadata)\n",
    "            chunk_metadata.update({\n",
    "                'chunk_id': f\"{row['_id']}_chunk_{i+1}\",\n",
    "                'chunk_index': i + 1,\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_text': chunk,\n",
    "                'chunk_length': len(chunk)\n",
    "            })\n",
    "            chunked_data.append(chunk_metadata)\n",
    "        \n",
    "        return chunked_data\n",
    "    \n",
    "    def chunk_dataframe(self, df):\n",
    "        \"\"\"TÃ¼m DataFrame'i chunk'lara bÃ¶l\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                chunks = self.chunk_single_karar(row)\n",
    "                all_chunks.extend(chunks)\n",
    "                print(f\"Karar {index+1}: {len(chunks)} chunk oluÅŸturuldu\")\n",
    "            except Exception as e:\n",
    "                print(f\"Karar {index+1} iÅŸlenirken hata: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(all_chunks)\n",
    "\n",
    "# KullanÄ±m Ã¶rneÄŸi\n",
    "# CSV ile Ã§alÄ±ÅŸma fonksiyonlarÄ±\n",
    "def process_csv(input_csv_path, output_csv_path, chunk_size=2000, chunk_overlap=100):\n",
    "    \"\"\"CSV dosyasÄ±nÄ± okur, chunk'lar ve yeni CSV dosyasÄ±na kaydeder\"\"\"\n",
    "    \n",
    "    # CSV'yi oku\n",
    "    print(f\"CSV dosyasÄ± okunuyor: {input_csv_path}\")\n",
    "    df = pd.read_csv(input_csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Toplam karar sayÄ±sÄ±: {len(df)}\")\n",
    "    print(f\"Kolonlar: {list(df.columns)}\")\n",
    "    \n",
    "    # Chunker'Ä± baÅŸlat\n",
    "    chunker = YargitayKararlariChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    # Chunk'lara bÃ¶l\n",
    "    print(\"\\nChunk iÅŸlemi baÅŸlÄ±yor...\")\n",
    "    chunked_df = chunker.chunk_dataframe(df)\n",
    "    \n",
    "    # Ã–zet bilgi\n",
    "    print(f\"\\n=== Ã–ZET ===\")\n",
    "    print(f\"Ä°ÅŸlenen karar sayÄ±sÄ±: {len(df)}\")\n",
    "    print(f\"Ãœretilen chunk sayÄ±sÄ±: {len(chunked_df)}\")\n",
    "    print(f\"Ortalama chunk/karar: {len(chunked_df)/len(df):.1f}\")\n",
    "    \n",
    "    # Chunk uzunluk istatistikleri\n",
    "    chunk_lengths = chunked_df['chunk_length']\n",
    "    print(f\"Chunk uzunluklarÄ± - Min: {chunk_lengths.min()}, Max: {chunk_lengths.max()}, Ort: {chunk_lengths.mean():.0f}\")\n",
    "    \n",
    "    # CSV olarak kaydet\n",
    "    print(f\"\\nSonuÃ§ kaydediliyor: {output_csv_path}\")\n",
    "    chunked_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"âœ… Ä°ÅŸlem tamamlandÄ±!\")\n",
    "    print(f\"ğŸ“‚ GiriÅŸ dosyasÄ±: {input_csv_path}\")\n",
    "    print(f\"ğŸ“„ Ã‡Ä±kÄ±ÅŸ dosyasÄ±: {output_csv_path}\")\n",
    "    return chunked_df\n",
    "\n",
    "def analyze_chunks(csv_path):\n",
    "    \"\"\"Chunk'lanmÄ±ÅŸ CSV'yi analiz eder\"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"=== CHUNK ANALÄ°ZÄ° ===\")\n",
    "    print(f\"Toplam chunk sayÄ±sÄ±: {len(df)}\")\n",
    "    print(f\"Benzersiz karar sayÄ±sÄ±: {df['_id'].nunique()}\")\n",
    "    \n",
    "    # Karar baÅŸÄ±na chunk daÄŸÄ±lÄ±mÄ±\n",
    "    chunks_per_karar = df.groupby('_id')['chunk_index'].max()\n",
    "    print(f\"\\nKarar baÅŸÄ±na chunk daÄŸÄ±lÄ±mÄ±:\")\n",
    "    print(f\"Min: {chunks_per_karar.min()}\")\n",
    "    print(f\"Max: {chunks_per_karar.max()}\")\n",
    "    print(f\"Ortalama: {chunks_per_karar.mean():.1f}\")\n",
    "    \n",
    "    # En Ã§ok chunk'a sahip kararlar\n",
    "    print(f\"\\nEn Ã§ok chunk'a sahip 5 karar:\")\n",
    "    top_chunked = chunks_per_karar.nlargest(5)\n",
    "    for karar_id, chunk_count in top_chunked.items():\n",
    "        esasNo = df[df['_id'] == karar_id]['esasNo'].iloc[0]\n",
    "        print(f\"- {esasNo}: {chunk_count} chunk\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Ã–rnek kullanÄ±m - GiriÅŸ ve Ã§Ä±kÄ±ÅŸ CSV dosya yollarÄ±nÄ± belirtin\"\"\"\n",
    "    \n",
    "    # Dosya yollarÄ±nÄ± belirtin\n",
    "    input_file = \"/home/yapayzeka/ahsen_bulbul/data/10data.csv\"      # GiriÅŸ CSV dosyasÄ±\n",
    "    output_file = \"yargitay_chunks.csv\"        # Ã‡Ä±kÄ±ÅŸ CSV dosyasÄ±\n",
    "    \n",
    "    try:\n",
    "        # CSV'yi iÅŸle - Her zaman yeni dosyaya yazar\n",
    "        chunked_data = process_csv(\n",
    "            input_csv_path=input_file,\n",
    "            output_csv_path=output_file,    # Zorunlu parametre\n",
    "            chunk_size=1200,                # Ä°htiyaca gÃ¶re ayarlayÄ±n\n",
    "            chunk_overlap=100               # Ä°htiyaca gÃ¶re ayarlayÄ±n\n",
    "        )\n",
    "        \n",
    "        # Analiz yap\n",
    "        analyze_chunks(output_file)\n",
    "        \n",
    "        return chunked_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Dosya bulunamadÄ±: {input_file}\")\n",
    "        print(\"LÃ¼tfen doÄŸru dosya yolunu 'input_file' deÄŸiÅŸkenine girin.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Hata oluÅŸtu: {e}\")\n",
    "        return None\n",
    "\n",
    "# Alternatif kullanÄ±m fonksiyonlarÄ±\n",
    "def chunk_csv_simple(input_path, output_path):\n",
    "    \"\"\"Basit kullanÄ±m - varsayÄ±lan ayarlarla\"\"\"\n",
    "    return process_csv(input_path, output_path)\n",
    "\n",
    "def chunk_csv_custom(input_path, output_path, chunk_size, chunk_overlap):\n",
    "    \"\"\"Ã–zel ayarlarla chunk'lama\"\"\"\n",
    "    return process_csv(input_path, output_path, chunk_size, chunk_overlap)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a91b1e",
   "metadata": {},
   "source": [
    "###DENEME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "class YargitayKararlariChunker:\n",
    "    def __init__(self, chunk_size=2000, chunk_overlap=100, aggressive_mode=False):\n",
    "        # Aggressive mode iÃ§in daha az ayÄ±rÄ±cÄ± kullan\n",
    "        if aggressive_mode:\n",
    "            self.separators = [\n",
    "                \"\\n\\n\",  # Sadece paragraf arasÄ±\n",
    "                \". \",    # CÃ¼mle sonu\n",
    "                \" \"      # BoÅŸluk\n",
    "            ]\n",
    "        else:\n",
    "            # Hukuki metinler iÃ§in optimize edilmiÅŸ ayÄ±rÄ±cÄ±lar\n",
    "            self.separators = [\n",
    "                \"\\n\\n\",  # Paragraf arasÄ±\n",
    "                \". \",    # CÃ¼mle sonu\n",
    "                \", \",    # VirgÃ¼l\n",
    "                \" \",     # BoÅŸluk\n",
    "                \"\"       # Karakter dÃ¼zeyi\n",
    "            ]\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=self.separators,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Chunker ayarlarÄ±: size={chunk_size}, overlap={chunk_overlap}, aggressive={aggressive_mode}\")\n",
    "    \n",
    "    def preprocess_text(self, raw_text):\n",
    "        \"\"\"Metni temizle ve normalize et\"\"\"\n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = re.sub(r'\\s+', ' ', raw_text.strip())\n",
    "        \n",
    "        # Ã–zel karakterleri dÃ¼zelt\n",
    "        text = text.replace('\"\"', '\"')\n",
    "        text = text.replace('...', '[...]')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_metadata_from_text(self, text):\n",
    "        \"\"\"Metinden Ã¶nemli bilgileri Ã§Ä±kar\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Daire bilgisi\n",
    "        daire_match = re.search(r'(\\d+\\.\\s*\\w+\\s*Dairesi)', text)\n",
    "        if daire_match:\n",
    "            metadata['daire'] = daire_match.group(1)\n",
    "        \n",
    "        # Mahkeme bilgisi\n",
    "        mahkeme_match = re.search(r'MAHKEMESÄ°\\s*:\\s*([^\\\\n]+)', text)\n",
    "        if mahkeme_match:\n",
    "            metadata['mahkeme'] = mahkeme_match.group(1).strip()\n",
    "        \n",
    "        # Karar tÃ¼rÃ¼\n",
    "        if 'reddine karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'RED'\n",
    "        elif 'kabulune karar' in text.lower():\n",
    "            metadata['karar_turu'] = 'KABUL'\n",
    "        else:\n",
    "            metadata['karar_turu'] = 'DÄ°ÄER'\n",
    "            \n",
    "        return metadata\n",
    "    \n",
    "    def chunk_single_karar(self, row):\n",
    "        \"\"\"Tek bir kararÄ± chunk'lara bÃ¶l\"\"\"\n",
    "        # Temel bilgileri hazÄ±rla\n",
    "        base_metadata = {\n",
    "            '_id': row['_id'],\n",
    "            'location': row['location'],\n",
    "            'esasNo': row['esasNo'],\n",
    "            'kararNo': row['kararNo'],\n",
    "            'extractedDates': row['extractedDates'],\n",
    "            'esasNo_num': row['esasNo_num'],\n",
    "            'esasNo_tip': row['esasNo_tip'],\n",
    "            'kararNo_num': row['kararNo_num'],\n",
    "            'kararNo_tip': row['kararNo_tip']\n",
    "        }\n",
    "        \n",
    "        # Metni temizle\n",
    "        cleaned_text = self.preprocess_text(row['rawText'])\n",
    "        \n",
    "        # Metinden ek bilgi Ã§Ä±kar\n",
    "        text_metadata = self.extract_metadata_from_text(cleaned_text)\n",
    "        \n",
    "        # Chunk'lara bÃ¶l\n",
    "        chunks = self.text_splitter.split_text(cleaned_text)\n",
    "        \n",
    "        # Her chunk iÃ§in metadata ekle\n",
    "        chunked_data = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = base_metadata.copy()\n",
    "            chunk_metadata.update(text_metadata)\n",
    "            chunk_metadata.update({\n",
    "                'chunk_id': f\"{row['_id']}_chunk_{i+1}\",\n",
    "                'chunk_index': i + 1,\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_text': chunk,\n",
    "                'chunk_length': len(chunk)\n",
    "            })\n",
    "            chunked_data.append(chunk_metadata)\n",
    "        \n",
    "        return chunked_data\n",
    "    \n",
    "    def chunk_dataframe(self, df):\n",
    "        \"\"\"TÃ¼m DataFrame'i chunk'lara bÃ¶l\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                chunks = self.chunk_single_karar(row)\n",
    "                all_chunks.extend(chunks)\n",
    "                print(f\"Karar {index+1}: {len(chunks)} chunk oluÅŸturuldu\")\n",
    "            except Exception as e:\n",
    "                print(f\"Karar {index+1} iÅŸlenirken hata: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(all_chunks)\n",
    "\n",
    "# KullanÄ±m Ã¶rneÄŸi\n",
    "# CSV ile Ã§alÄ±ÅŸma fonksiyonlarÄ±\n",
    "def process_csv(input_csv_path, output_csv_path, chunk_size=2000, chunk_overlap=100, aggressive_mode=False):\n",
    "    \"\"\"CSV dosyasÄ±nÄ± okur, chunk'lar ve yeni CSV dosyasÄ±na kaydeder\"\"\"\n",
    "    \n",
    "    # CSV'yi oku\n",
    "    print(f\"CSV dosyasÄ± okunuyor: {input_csv_path}\")\n",
    "    df = pd.read_csv(input_csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Toplam karar sayÄ±sÄ±: {len(df)}\")\n",
    "    print(f\"Kolonlar: {list(df.columns)}\")\n",
    "    \n",
    "    # Chunker'Ä± baÅŸlat\n",
    "    chunker = YargitayKararlariChunker(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        aggressive_mode=aggressive_mode\n",
    "    )\n",
    "    \n",
    "    # Chunk'lara bÃ¶l\n",
    "    print(\"\\nChunk iÅŸlemi baÅŸlÄ±yor...\")\n",
    "    chunked_df = chunker.chunk_dataframe(df)\n",
    "    \n",
    "    # Ã–zet bilgi\n",
    "    print(f\"\\n=== Ã–ZET ===\")\n",
    "    print(f\"Ä°ÅŸlenen karar sayÄ±sÄ±: {len(df)}\")\n",
    "    print(f\"Ãœretilen chunk sayÄ±sÄ±: {len(chunked_df)}\")\n",
    "    print(f\"Ortalama chunk/karar: {len(chunked_df)/len(df):.1f}\")\n",
    "    print(f\"Ã–nceki chunk sayÄ±sÄ±ndan azalma: %{((138-len(chunked_df))/138)*100:.1f}\")\n",
    "    \n",
    "    # Chunk uzunluk istatistikleri\n",
    "    chunk_lengths = chunked_df['chunk_length']\n",
    "    print(f\"Chunk uzunluklarÄ± - Min: {chunk_lengths.min()}, Max: {chunk_lengths.max()}, Ort: {chunk_lengths.mean():.0f}\")\n",
    "    \n",
    "    # CSV olarak kaydet\n",
    "    print(f\"\\nSonuÃ§ kaydediliyor: {output_csv_path}\")\n",
    "    chunked_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"âœ… Ä°ÅŸlem tamamlandÄ±!\")\n",
    "    print(f\"ğŸ“‚ GiriÅŸ dosyasÄ±: {input_csv_path}\")\n",
    "    print(f\"ğŸ“„ Ã‡Ä±kÄ±ÅŸ dosyasÄ±: {output_csv_path}\")\n",
    "    return chunked_df\n",
    "\n",
    "def analyze_chunks(csv_path):\n",
    "    \"\"\"Chunk'lanmÄ±ÅŸ CSV'yi analiz eder\"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"=== CHUNK ANALÄ°ZÄ° ===\")\n",
    "    print(f\"Toplam chunk sayÄ±sÄ±: {len(df)}\")\n",
    "    print(f\"Benzersiz karar sayÄ±sÄ±: {df['_id'].nunique()}\")\n",
    "    \n",
    "    # Karar baÅŸÄ±na chunk daÄŸÄ±lÄ±mÄ±\n",
    "    chunks_per_karar = df.groupby('_id')['chunk_index'].max()\n",
    "    print(f\"\\nKarar baÅŸÄ±na chunk daÄŸÄ±lÄ±mÄ±:\")\n",
    "    print(f\"Min: {chunks_per_karar.min()}\")\n",
    "    print(f\"Max: {chunks_per_karar.max()}\")\n",
    "    print(f\"Ortalama: {chunks_per_karar.mean():.1f}\")\n",
    "    \n",
    "    # En Ã§ok chunk'a sahip kararlar\n",
    "    print(f\"\\nEn Ã§ok chunk'a sahip 5 karar:\")\n",
    "    top_chunked = chunks_per_karar.nlargest(5)\n",
    "    for karar_id, chunk_count in top_chunked.items():\n",
    "        esasNo = df[df['_id'] == karar_id]['esasNo'].iloc[0]\n",
    "        print(f\"- {esasNo}: {chunk_count} chunk\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Ã–rnek kullanÄ±m - GiriÅŸ ve Ã§Ä±kÄ±ÅŸ CSV dosya yollarÄ±nÄ± belirtin\"\"\"\n",
    "    \n",
    "    # Dosya yollarÄ±nÄ± belirtin\n",
    "    input_file = \"yargitay_kararlari.csv\"      # GiriÅŸ CSV dosyasÄ±\n",
    "    output_file = \"yargitay_chunks.csv\"        # Ã‡Ä±kÄ±ÅŸ CSV dosyasÄ±\n",
    "    \n",
    "    try:\n",
    "        # CSV'yi iÅŸle - Az chunk iÃ§in optimize edilmiÅŸ ayarlar\n",
    "        chunked_data = process_csv(\n",
    "            input_csv_path=input_file,\n",
    "            output_csv_path=output_file,\n",
    "            chunk_size=2000,            # BÃ¼yÃ¼k chunk (az chunk iÃ§in)\n",
    "            chunk_overlap=100,          # Az overlap\n",
    "            aggressive_mode=True        # Agresif mod\n",
    "        )\n",
    "        \n",
    "        # Analiz yap\n",
    "        analyze_chunks(output_file)\n",
    "        \n",
    "        return chunked_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Dosya bulunamadÄ±: {input_file}\")\n",
    "        print(\"LÃ¼tfen doÄŸru dosya yolunu 'input_file' deÄŸiÅŸkenine girin.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Hata oluÅŸtu: {e}\")\n",
    "        return None\n",
    "\n",
    "# Chunk sayÄ±sÄ±nÄ± minimize etmek iÃ§in Ã¶zel fonksiyonlar\n",
    "def minimize_chunks(input_path, output_path):\n",
    "    \"\"\"Minimum chunk sayÄ±sÄ± iÃ§in optimize edilmiÅŸ\"\"\"\n",
    "    return process_csv(\n",
    "        input_path, \n",
    "        output_path, \n",
    "        chunk_size=3000,        # Ã‡ok bÃ¼yÃ¼k chunk\n",
    "        chunk_overlap=50,       # Minimum overlap\n",
    "        aggressive_mode=True\n",
    "    )\n",
    "\n",
    "def balanced_chunks(input_path, output_path):\n",
    "    \"\"\"Dengeli yaklaÅŸÄ±m\"\"\"\n",
    "    return process_csv(\n",
    "        input_path, \n",
    "        output_path, \n",
    "        chunk_size=2000,        # Orta bÃ¼yÃ¼klÃ¼k\n",
    "        chunk_overlap=100,      # Az overlap\n",
    "        aggressive_mode=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
