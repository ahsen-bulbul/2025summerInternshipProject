{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760cbe7-9c32-4a8a-bc12-8eff3ce4890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1ed61",
   "metadata": {},
   "source": [
    "### bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model=\"BAAI/bge-m3\",\n",
    "    threshold=0.9,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=64,                              # Maximum tokens per chunk\n",
    "    min_sentences=3                              # Initial sentences per chunk\n",
    ")\n",
    "\n",
    "semantic_chunks = semantic_chunker.chunk(text)\n",
    "\n",
    "for chunk in semantic_chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Number of sentences: {len(chunk.sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1123cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chonkie import SemanticChunker\n",
    "\n",
    "# CSV yükle\n",
    "df = pd.read_csv(\"/home/yapayzeka/ahsen_bulbul/data/cleaned10chunk.csv\")\n",
    "texts = df['rawText'].astype(str).tolist()\n",
    "\n",
    "# Tüm metni birleştir\n",
    "full_text = \"\\n\".join(texts)\n",
    "\n",
    "# Chunker\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model=\"BAAI/bge-m3\",\n",
    "    threshold=0.5,\n",
    "    chunk_size=128,\n",
    "    min_sentences=3\n",
    ")\n",
    "\n",
    "# Chunk’ları çıkar\n",
    "semantic_chunks = semantic_chunker.chunk(full_text)\n",
    "\n",
    "# Chunk bilgilerini DataFrame’e aktar\n",
    "chunk_data = []\n",
    "for i, chunk in enumerate(semantic_chunks, 1):\n",
    "    chunk_data.append({\n",
    "        \"chunk_id\": i,\n",
    "        \"chunk_text\": chunk.text,\n",
    "        \"token_count\": chunk.token_count,\n",
    "        \"num_sentences\": len(chunk.sentences)\n",
    "    })\n",
    "\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "\n",
    "# CSV olarak kaydet\n",
    "chunk_df.to_csv(\"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/chunks.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Chunks CSV olarak kaydedildi.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480f65e",
   "metadata": {},
   "source": [
    "### LAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chonkie import SemanticChunker\n",
    "\n",
    "# CSV yükle\n",
    "df = pd.read_csv(\"/home/yapayzeka/ahsen_bulbul/data/10data.csv\")\n",
    "\n",
    "# Chunker\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model=\"BAAI/bge-m3\",\n",
    "    threshold=0.8,\n",
    "    chunk_size=300,\n",
    "    min_sentences=5\n",
    ")\n",
    "\n",
    "chunk_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = str(row['rawText'])\n",
    "    chunks = semantic_chunker.chunk(text)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        metadata = {\n",
    "            \"_id\": row[\"_id\"],\n",
    "            \"location\": row[\"location\"],\n",
    "            \"extractedDates\": row[\"extractedDates\"],\n",
    "            \"esasNo\": row[\"esasNo\"],\n",
    "            \"kararNo\": row[\"kararNo\"],\n",
    "            \"esasNo_num\": row[\"esasNo_num\"],\n",
    "            \"esasNo_tip\": row[\"esasNo_tip\"],\n",
    "            \"kararNo_num\": row[\"kararNo_num\"],\n",
    "            \"kararNo_tip\": row[\"kararNo_tip\"]\n",
    "        }\n",
    "        chunk_data.append({\n",
    "            \"chunk_id\": f\"{row['_id']}_{i}\",   # Örn: _id_chunkIndex\n",
    "            \"chunk_text\": chunk.text,\n",
    "            \"token_count\": chunk.token_count,\n",
    "            \"num_sentences\": len(chunk.sentences),\n",
    "            **metadata\n",
    "        })\n",
    "\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "\n",
    "chunk_df.to_csv(\n",
    "    \"/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/chunks_with_metadata.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(\"Chunks + metadata CSV olarak kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chonkie import SemanticChunker\n",
    "\n",
    "# 1. CSV dosyasını yükle\n",
    "df = pd.read_csv('/home/yapayzeka/ahsen_bulbul/data/10data.csv')\n",
    "\n",
    "# 2. Chunker oluştur\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"BAAI/bge-m3\",\n",
    "    threshold=0.8,      # benzerlik eşiği\n",
    "    chunk_size=512,     # her chunk max token\n",
    "    min_sentences=5     # en az cümle sayısı\n",
    ")\n",
    "\n",
    "# 3. Chunk sonuçlarını tutmak için liste\n",
    "chunked_data = []\n",
    "\n",
    "# 4. Her satır için chunk işle\n",
    "for index, row in df.iterrows():\n",
    "    text_to_chunk = row['rawText']\n",
    "    \n",
    "    if pd.notna(text_to_chunk):\n",
    "        # ⚠️ chunk alma → bazı versiyonlarda chunker(text), bazılarında chunker.chunk(text) gerekiyor\n",
    "        try:\n",
    "            chunks = chunker.chunk(text_to_chunk)   # önce bu denenir\n",
    "        except:\n",
    "            chunks = chunker(text_to_chunk)        # eğer olmazsa bu çalışır\n",
    "        \n",
    "        # 5. Her chunk için metadata ekle\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # chunk.text varsa onu al, yoksa direk chunk kullan\n",
    "            chunk_text = getattr(chunk, \"text\", chunk)\n",
    "            \n",
    "            chunked_data.append({\n",
    "                \"original_id\": row[\"_id\"],\n",
    "                \"original_location\": row[\"location\"],\n",
    "                \"extractedDates\": row[\"extractedDates\"],\n",
    "                \"esasNo\": row[\"esasNo\"],\n",
    "                \"kararNo\": row[\"kararNo\"],\n",
    "                \"esasNo_num\": row[\"esasNo_num\"],\n",
    "                \"esasNo_tip\": row[\"esasNo_tip\"],\n",
    "                \"kararNo_num\": row[\"kararNo_num\"],\n",
    "                \"kararNo_tip\": row[\"kararNo_tip\"],\n",
    "                \"chunk_id\": f\"{row['_id']}-sem_{i+1}\",\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"token_count\": getattr(chunk, \"token_count\", None),\n",
    "                \"num_sentences\": len(getattr(chunk, \"sentences\", [])) if hasattr(chunk, \"sentences\") else None\n",
    "            })\n",
    "\n",
    "# 6. DataFrame oluştur\n",
    "chunked_df = pd.DataFrame(chunked_data)\n",
    "\n",
    "# 7. CSV olarak kaydet\n",
    "output_file = '/home/yapayzeka/ahsen_bulbul/model/chonkie/chunked_data_semantic_with_metadata.csv'\n",
    "chunked_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Metinler chunk’landı ve metadata ile birlikte '{output_file}' dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chonkie import SemanticChunker\n",
    "import re\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# 1. CSV dosyasını yükle\n",
    "df = pd.read_csv('/home/yapayzeka/ahsen_bulbul/data/10data.csv')\n",
    "\n",
    "# 2. Chunker oluştur\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"embed-multilingual-v3.0\",\n",
    "    threshold=0.6,\n",
    "    chunk_size=512,\n",
    "    min_sentences=4,\n",
    "    cohere_api_key=cohere_api_key\n",
    ")\n",
    "\n",
    "# 3. Chunk sonuçlarını tutmak için liste\n",
    "chunked_data = []\n",
    "\n",
    "# 4. Her satır için chunk işle\n",
    "for index, row in df.iterrows():\n",
    "    text_to_chunk = row['rawText']\n",
    "    \n",
    "    if pd.notna(text_to_chunk):\n",
    "        try:\n",
    "            chunks = chunker.chunk(text_to_chunk)\n",
    "        except:\n",
    "            chunks = chunker(text_to_chunk)\n",
    "        \n",
    "        # 4a. Küçük chunkları birleştirme\n",
    "        merged_chunks = []\n",
    "        token_threshold = 100  # 100 token’dan küçük chunk birleştirilecek\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_text = getattr(chunk, \"text\", chunk)\n",
    "            token_count = getattr(chunk, \"token_count\", None)\n",
    "            num_sentences = len(getattr(chunk, \"sentences\", [])) if hasattr(chunk, \"sentences\") else None\n",
    "\n",
    "            if merged_chunks and token_count is not None and token_count < token_threshold:\n",
    "                # Önceki chunk ile birleştir\n",
    "                prev = merged_chunks[-1]\n",
    "                prev[\"chunk_text\"] += \" \" + chunk_text\n",
    "                prev[\"token_count\"] += token_count\n",
    "                if num_sentences is not None:\n",
    "                    prev[\"num_sentences\"] += num_sentences\n",
    "            else:\n",
    "                merged_chunks.append({\n",
    "                    \"chunk_text\": chunk_text,\n",
    "                    \"token_count\": token_count,\n",
    "                    \"num_sentences\": num_sentences\n",
    "                })\n",
    "\n",
    "        # 5. Metadata ekleyip chunked_data’ya aktar\n",
    "        for i, mc in enumerate(merged_chunks, 1):\n",
    "            chunked_data.append({\n",
    "                \"_id\": row[\"_id\"],\n",
    "                \"location\": re.sub(r\"(\\d+)\\. ?HukukDairesi\", r\"\\1. Hukuk Dairesi\", str(row[\"location\"])),\n",
    "                \"extractedDates\": row[\"extractedDates\"],\n",
    "                \"esasNo\": row[\"esasNo\"],\n",
    "                \"kararNo\": row[\"kararNo\"],\n",
    "                \"esasNo_num\": row[\"esasNo_num\"],\n",
    "                \"esasNo_tip\": row[\"esasNo_tip\"],\n",
    "                \"kararNo_num\": row[\"kararNo_num\"],\n",
    "                \"kararNo_tip\": row[\"kararNo_tip\"],\n",
    "                \"chunk_id\": f\"{row['_id']}-sem_{i}\",\n",
    "                \"chunk_text\": mc[\"chunk_text\"],\n",
    "                \"token_count\": mc[\"token_count\"],\n",
    "                \"num_sentences\": mc[\"num_sentences\"]\n",
    "            })\n",
    "\n",
    "# 6. DataFrame oluştur\n",
    "chunked_df = pd.DataFrame(chunked_data)\n",
    "\n",
    "\n",
    "# 7. CSV olarak kaydet\n",
    "output_file = '/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/lastSemantic.csv'\n",
    "chunked_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Metinler chunk’landı, küçük chunklar merge edildi ve CSV kaydedildi: '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f895f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/chonkie/embeddings/cohere.py:173: UserWarning: Text has 715 tokens which exceeds the model's context length of 512.Generation may not be optimal.\n",
      "  warnings.warn(\n",
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/chonkie/embeddings/cohere.py:173: UserWarning: Text has 913 tokens which exceeds the model's context length of 512.Generation may not be optimal.\n",
      "  warnings.warn(\n",
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/chonkie/embeddings/cohere.py:173: UserWarning: Text has 675 tokens which exceeds the model's context length of 512.Generation may not be optimal.\n",
      "  warnings.warn(\n",
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/chonkie/embeddings/cohere.py:173: UserWarning: Text has 657 tokens which exceeds the model's context length of 512.Generation may not be optimal.\n",
      "  warnings.warn(\n",
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/chonkie/embeddings/cohere.py:173: UserWarning: Text has 613 tokens which exceeds the model's context length of 512.Generation may not be optimal.\n",
      "  warnings.warn(\n",
      "/home/yapayzeka/ahsen_bulbul/ahsen/lib/python3.10/site-packages/chonkie/embeddings/cohere.py:173: UserWarning: Text has 842 tokens which exceeds the model's context length of 512.Generation may not be optimal.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metinler chunk’landı, küçük chunklar merge edildi ve CSV kaydedildi: '/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/lastSemantic.csv'\n"
     ]
    }
   ],
   "source": [
    "from chonkie.embeddings.cohere import CohereEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from chonkie import SemanticChunker\n",
    "import re\n",
    "\n",
    "load_dotenv(\"/home/yapayzeka/ahsen_bulbul/qdrant/.env\")  # .env dosyasını yükle\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "if not cohere_api_key:\n",
    "    raise ValueError(\"COHERE_API_KEY ortam değişkeni bulunamadı!\")\n",
    "\n",
    "embeddings = CohereEmbeddings(\n",
    "    model=\"embed-multilingual-v3.0\",\n",
    "    api_key=cohere_api_key  # Burada doğrudan veriyoruz\n",
    ")\n",
    "\n",
    "df = pd.read_csv('/home/yapayzeka/ahsen_bulbul/data/10data.csv')\n",
    "\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=embeddings,  # CohereEmbeddings nesnesi\n",
    "    threshold=0.6,\n",
    "    chunk_size=512,\n",
    "    min_sentences=4\n",
    ")\n",
    "\n",
    "# 3. Chunk sonuçlarını tutmak için liste\n",
    "chunked_data = []\n",
    "\n",
    "# 4. Her satır için chunk işle\n",
    "for index, row in df.iterrows():\n",
    "    text_to_chunk = row['rawText']\n",
    "    \n",
    "    if pd.notna(text_to_chunk):\n",
    "        try:\n",
    "            chunks = chunker.chunk(text_to_chunk)\n",
    "        except:\n",
    "            chunks = chunker(text_to_chunk)\n",
    "        \n",
    "        # 4a. Küçük chunkları birleştirme\n",
    "        merged_chunks = []\n",
    "        token_threshold = 100  # 100 token’dan küçük chunk birleştirilecek\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_text = getattr(chunk, \"text\", chunk)\n",
    "            token_count = getattr(chunk, \"token_count\", None)\n",
    "            num_sentences = len(getattr(chunk, \"sentences\", [])) if hasattr(chunk, \"sentences\") else None\n",
    "\n",
    "            if merged_chunks and token_count is not None and token_count < token_threshold:\n",
    "                # Önceki chunk ile birleştir\n",
    "                prev = merged_chunks[-1]\n",
    "                prev[\"chunk_text\"] += \" \" + chunk_text\n",
    "                prev[\"token_count\"] += token_count\n",
    "                if num_sentences is not None:\n",
    "                    prev[\"num_sentences\"] += num_sentences\n",
    "            else:\n",
    "                merged_chunks.append({\n",
    "                    \"chunk_text\": chunk_text,\n",
    "                    \"token_count\": token_count,\n",
    "                    \"num_sentences\": num_sentences\n",
    "                })\n",
    "\n",
    "        # 5. Metadata ekleyip chunked_data’ya aktar\n",
    "        for i, mc in enumerate(merged_chunks, 1):\n",
    "            chunked_data.append({\n",
    "                \"_id\": row[\"_id\"],\n",
    "                \"location\": re.sub(r\"(\\d+)\\. ?HukukDairesi\", r\"\\1. Hukuk Dairesi\", str(row[\"location\"])),\n",
    "                \"extractedDates\": row[\"extractedDates\"],\n",
    "                \"esasNo\": row[\"esasNo\"],\n",
    "                \"kararNo\": row[\"kararNo\"],\n",
    "                \"esasNo_num\": row[\"esasNo_num\"],\n",
    "                \"esasNo_tip\": row[\"esasNo_tip\"],\n",
    "                \"kararNo_num\": row[\"kararNo_num\"],\n",
    "                \"kararNo_tip\": row[\"kararNo_tip\"],\n",
    "                \"chunk_id\": f\"{row['_id']}-sem_{i}\",\n",
    "                \"chunk_text\": mc[\"chunk_text\"],\n",
    "                \"token_count\": mc[\"token_count\"],\n",
    "                \"num_sentences\": mc[\"num_sentences\"]\n",
    "            })\n",
    "\n",
    "# 6. DataFrame oluştur\n",
    "chunked_df = pd.DataFrame(chunked_data)\n",
    "\n",
    "\n",
    "# 7. CSV olarak kaydet\n",
    "output_file = '/home/yapayzeka/ahsen_bulbul/model/chonkie/semantic/lastSemantic.csv'\n",
    "chunked_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Metinler chunk’landı, küçük chunklar merge edildi ve CSV kaydedildi: '{output_file}'\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahsen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
